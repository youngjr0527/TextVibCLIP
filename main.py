"""
TextVibCLIP ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
Joint Training + Continual Learning ì „ì²´ íŒŒì´í”„ë¼ì¸
"""

import argparse
import logging
import os
import time
import torch
from datetime import datetime

# ëª¨ë“ˆ import
from src.continual_trainer import ContinualTrainer
from src.data_loader import create_domain_dataloaders, create_combined_dataloader, create_first_domain_dataloader
from src.textvib_model import create_textvib_model
from src.utils import set_seed
from configs.model_config import TRAINING_CONFIG, DATA_CONFIG

# ë¡œê¹… ì„¤ì •
def setup_logging(log_dir: str = 'logs'):
    """ë¡œê¹… ì„¤ì •"""
    os.makedirs(log_dir, exist_ok=True)
    
    log_filename = f"textvibclip_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    log_path = os.path.join(log_dir, log_filename)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¡œê¹… ì„¤ì • ì™„ë£Œ: {log_path}")
    return logger


def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='TextVibCLIP Continual Learning Experiment')
    
    # ì‹¤í—˜ ì„¤ì •
    parser.add_argument('--experiment_name', type=str, default='textvibclip_experiment',
                       help='ì‹¤í—˜ ì´ë¦„')
    parser.add_argument('--save_dir', type=str, default='results',
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument('--embedding_dim', type=int, default=512,
                       help='ì„ë² ë”© ì°¨ì›')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                       help='í•™ìŠµë¥ ')
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument('--first_domain_epochs', type=int, default=50,
                       help='First domain training ì—í¬í¬ ìˆ˜')
    parser.add_argument('--remaining_domains_epochs', type=int, default=30,
                       help='Remaining domains training ì—í¬í¬ ìˆ˜')
    parser.add_argument('--replay_buffer_size', type=int, default=500,
                       help='Replay buffer í¬ê¸°')
    parser.add_argument('--replay_ratio', type=float, default=0.3,
                       help='Replay ë°ì´í„° ë¹„ìœ¨')
    
    # ì‹¤í—˜ ëª¨ë“œ
    parser.add_argument('--mode', type=str, choices=['full', 'first_domain_only', 'remaining_domains_only'],
                       default='full', help='ì‹¤í—˜ ëª¨ë“œ')
    parser.add_argument('--load_first_domain_checkpoint', type=str, default=None,
                       help='First domain training ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (remaining_domains_only ëª¨ë“œìš©)')
    
    # í•˜ë“œì›¨ì–´ ì„¤ì •
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cpu', 'cuda'], help='í•™ìŠµ ë””ë°”ì´ìŠ¤')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='DataLoader ì›Œì»¤ ìˆ˜')
    
    # í‰ê°€ ì„¤ì •
    parser.add_argument('--eval_interval', type=int, default=5,
                       help='í‰ê°€ ê°„ê²© (ì—í¬í¬)')
    parser.add_argument('--save_plots', action='store_true',
                       help='ê²°ê³¼ í”Œë¡¯ ì €ì¥ ì—¬ë¶€')
    
    # ì¬í˜„ì„± ì„¤ì •
    parser.add_argument('--seed', type=int, default=42,
                       help='ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œê°’')
    parser.add_argument('--no_amp', action='store_true',
                       help='AMP ë¹„í™œì„±í™”')
    parser.add_argument('--max_grad_norm', type=float, default=1.0,
                       help='Gradient clipping ìµœëŒ€ norm')
    
    return parser.parse_args()


def setup_device(device_arg: str) -> torch.device:
    """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
    if device_arg == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device_arg)
    
    if device.type == 'cuda':
        print(f"GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("CPU ì‚¬ìš©")
    
    return device


def create_experiment_directory(base_dir: str, experiment_name: str) -> str:
    """ì‹¤í—˜ ë””ë ‰í† ë¦¬ ìƒì„±"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    exp_dir = os.path.join(base_dir, f"{experiment_name}_{timestamp}")
    
    # í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
    subdirs = ['checkpoints', 'logs', 'plots', 'results']
    for subdir in subdirs:
        os.makedirs(os.path.join(exp_dir, subdir), exist_ok=True)
    
    return exp_dir


def run_first_domain_training(trainer: ContinualTrainer, 
                             args: argparse.Namespace,
                             logger: logging.Logger) -> dict:
    """First Domain Training ì‹¤í–‰ (600 RPM)"""
    logger.info("ğŸš€ First Domain Training ì‹œì‘ (600 RPM)")
    
    # ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œì˜ ë°ì´í„°ë¡œë” ìƒì„±
    first_domain_loader = create_first_domain_dataloader(
        subset='train', 
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )
    
    first_domain_rpm = DATA_CONFIG['domain_order'][0]
    logger.info(f"First Domain Training ë°ì´í„°: Domain {first_domain_rpm} RPM, "
               f"{len(first_domain_loader.dataset)}ê°œ ìƒ˜í”Œ")
    
    # First domain training ì‹¤í–‰
    start_time = time.time()
    first_domain_results = trainer.train_first_domain(
        first_domain_dataloader=first_domain_loader,
        num_epochs=args.first_domain_epochs
    )
    elapsed_time = time.time() - start_time
    
    logger.info(f"âœ… First Domain Training ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed_time/60:.1f}ë¶„)")
    logger.info(f"ìµœì¢… Loss: {first_domain_results['final_loss']:.4f}")
    
    # ë„ë©”ì¸ë³„ ì„±ëŠ¥ ì¶œë ¥
    for domain, metrics in first_domain_results['domain_performances'].items():
        logger.info(f"Domain {domain} ì„±ëŠ¥: {metrics['accuracy']:.4f}")
    
    return first_domain_results


def run_remaining_domains_training(trainer: ContinualTrainer,
                                  args: argparse.Namespace, 
                                  logger: logging.Logger) -> dict:
    """Remaining Domains Training ì‹¤í–‰ (800~1600 RPM)"""
    logger.info("ğŸ”„ Remaining Domains Training ì‹œì‘ (800~1600 RPM)")
    
    # ë„ë©”ì¸ë³„ ë°ì´í„°ë¡œë” ìƒì„±
    domain_loaders = create_domain_dataloaders(
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )
    
    remaining_domains = DATA_CONFIG['domain_order'][1:]  # ì²« ë²ˆì§¸ ì œì™¸
    logger.info(f"Remaining Domains: {remaining_domains}")
    
    # Replay buffer ì„¤ì • ì—…ë°ì´íŠ¸
    trainer.replay_buffer.buffer_size_per_domain = args.replay_buffer_size
    trainer.replay_ratio = args.replay_ratio
    
    # Remaining domains training ì‹¤í–‰
    start_time = time.time()
    remaining_domains_results = trainer.train_remaining_domains(domain_loaders)
    elapsed_time = time.time() - start_time
    
    logger.info(f"âœ… Remaining Domains Training ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed_time/60:.1f}ë¶„)")
    
    # ìµœì¢… ë©”íŠ¸ë¦­ ì¶œë ¥
    final_metrics = remaining_domains_results['final_metrics']
    logger.info(f"í‰ê·  ì •í™•ë„: {final_metrics['average_accuracy']:.4f}")
    logger.info(f"í‰ê·  ë§ê°ë„: {final_metrics['average_forgetting']:.4f}")
    
    return remaining_domains_results


def save_experiment_results(trainer: ContinualTrainer,
                          first_domain_results: dict,
                          remaining_domains_results: dict,
                          exp_dir: str,
                          args: argparse.Namespace,
                          logger: logging.Logger):
    """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
    logger.info("ğŸ’¾ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # 1. í•™ìŠµ ì´ë ¥ ì €ì¥
    history_path = os.path.join(exp_dir, 'results', 'training_history.pth')
    trainer.save_training_history(history_path)
    
    # 2. ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = os.path.join(exp_dir, 'checkpoints', 'final_model.pth')
    trainer.model.save_checkpoint(final_model_path, 0)
    
    # 3. Replay buffer ì €ì¥
    buffer_path = os.path.join(exp_dir, 'results', 'replay_buffer.pth')
    trainer.replay_buffer.save_buffer(buffer_path)
    
    # 4. ì‹¤í—˜ ì„¤ì • ì €ì¥
    experiment_config = {
        'args': vars(args),
        'first_domain_results': first_domain_results,
        'remaining_domains_results': remaining_domains_results,
        'model_config': {
            'embedding_dim': args.embedding_dim,
            'trainable_params': trainer.model.get_trainable_parameters()
        },
        'data_config': {
            'domain_order': DATA_CONFIG['domain_order'],
            'window_size': DATA_CONFIG['window_size']
        }
    }
    
    config_path = os.path.join(exp_dir, 'results', 'experiment_config.pth')
    torch.save(experiment_config, config_path)
    
    # 5. í”Œë¡¯ ì €ì¥ (ì˜µì…˜)
    if args.save_plots:
        plot_path = os.path.join(exp_dir, 'plots', 'continual_learning_curves.png')
        trainer.plot_continual_learning_curves(plot_path)
    
    logger.info(f"âœ… ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {exp_dir}")


def print_experiment_summary(first_domain_results: dict, 
                           remaining_domains_results: dict,
                           logger: logging.Logger):
    """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    logger.info("\n" + "="*50)
    logger.info("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    logger.info("="*50)
    
    # First Domain Training ìš”ì•½
    first_domain_rpm = DATA_CONFIG['domain_order'][0]
    logger.info(f"ğŸ¯ First Domain Training (Domain {first_domain_rpm} RPM):")
    logger.info(f"  - ìµœì¢… Loss: {first_domain_results['final_loss']:.4f}")
    logger.info(f"  - í‰ê·  Loss: {first_domain_results['avg_loss']:.4f}")
    
    # Remaining Domains Training ìš”ì•½
    final_metrics = remaining_domains_results['final_metrics']
    logger.info(f"ğŸ”„ Remaining Domains Training:")
    logger.info(f"  - í‰ê·  ì •í™•ë„: {final_metrics['average_accuracy']:.4f}")
    logger.info(f"  - í‰ê·  ë§ê°ë„: {final_metrics['average_forgetting']:.4f}")
    logger.info(f"  - í•™ìŠµ ë„ë©”ì¸ ìˆ˜: {final_metrics['num_domains']}")
    
    # ë„ë©”ì¸ë³„ ìµœì¢… ì„±ëŠ¥
    logger.info(f"ğŸ“ˆ ë„ë©”ì¸ë³„ ìµœì¢… ì„±ëŠ¥:")
    for i, acc in enumerate(final_metrics['final_accuracies']):
        domain = DATA_CONFIG['domain_order'][i]
        logger.info(f"  - Domain {domain} RPM: {acc:.4f}")
    
    logger.info("="*50)


def main():
    """ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì¸ìˆ˜ íŒŒì‹±
    args = parse_arguments()
    
    # ì¬í˜„ì„± ì„¤ì • (ê°€ì¥ ë¨¼ì €)
    set_seed(args.seed)
    
    # ì‹¤í—˜ ë””ë ‰í† ë¦¬ ìƒì„±
    exp_dir = create_experiment_directory(args.save_dir, args.experiment_name)
    
    # ë¡œê¹… ì„¤ì •
    logger = setup_logging(os.path.join(exp_dir, 'logs'))
    logger.info(f"ğŸ¯ ì‹¤í—˜ ì‹œì‘: {args.experiment_name}")
    logger.info(f"ğŸ“ ì‹¤í—˜ ë””ë ‰í† ë¦¬: {exp_dir}")
    logger.info(f"ğŸŒ± ì‹œë“œ: {args.seed}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = setup_device(args.device)
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    
    # ëª¨ë¸ ë° Trainer ìƒì„±
    logger.info("ğŸ—ï¸ ëª¨ë¸ ë° Trainer ì´ˆê¸°í™”...")
    
    if args.mode == 'remaining_domains_only' and args.load_first_domain_checkpoint:
        # First domain checkpointì—ì„œ ëª¨ë¸ ë¡œë”©
        model = create_textvib_model('joint')
        checkpoint = torch.load(args.load_first_domain_checkpoint, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        trainer = ContinualTrainer(
            model=model, 
            device=device, 
            save_dir=os.path.join(exp_dir, 'checkpoints'),
            use_amp=not args.no_amp,
            max_grad_norm=args.max_grad_norm
        )
    else:
        trainer = ContinualTrainer(
            device=device, 
            save_dir=os.path.join(exp_dir, 'checkpoints'),
            use_amp=not args.no_amp,
            max_grad_norm=args.max_grad_norm
        )
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
    trainer.batch_size = args.batch_size
    trainer.learning_rate = args.learning_rate
    trainer.num_epochs = args.first_domain_epochs  # First domain training ê¸°ë³¸ê°’
    
    param_info = trainer.model.get_trainable_parameters()
    logger.info(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: Total={param_info['total']:,}, "
               f"Text={param_info['text_total']:,}, Vib={param_info['vibration']:,}")
    
    # ì‹¤í—˜ ì‹¤í–‰
    first_domain_results = {}
    remaining_domains_results = {}
    
    try:
        if args.mode in ['full', 'first_domain_only']:
            # First Domain Training ì‹¤í–‰
            first_domain_results = run_first_domain_training(trainer, args, logger)
        
        if args.mode in ['full', 'remaining_domains_only']:
            # Remaining Domains Training ì‹¤í–‰
            trainer.num_epochs = args.remaining_domains_epochs  # Remaining domainsìš© ì—í¬í¬ ì„¤ì •
            remaining_domains_results = run_remaining_domains_training(trainer, args, logger)
        
        # ê²°ê³¼ ì €ì¥
        save_experiment_results(trainer, first_domain_results, remaining_domains_results, exp_dir, args, logger)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if first_domain_results and remaining_domains_results:
            print_experiment_summary(first_domain_results, remaining_domains_results, logger)
        
        logger.info("ğŸ‰ ì‹¤í—˜ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.exception("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
        raise e
    
    finally:
        # ì •ë¦¬ ì‘ì—…
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info(f"ğŸ“ ì‹¤í—˜ ê²°ê³¼ ê²½ë¡œ: {exp_dir}")


if __name__ == "__main__":
    main()
