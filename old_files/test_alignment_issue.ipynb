{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TextVibCLIP Text-Vibration 정렬 문제 분석 및 해결\n",
        "\n",
        "**심각한 문제 발견:**\n",
        "- Text Encoder 단독: 83.0% (우수)\n",
        "- Vibration Encoder 단독: 88.6% (우수)  \n",
        "- TextVibCLIP 통합: 14.4% (랜덤 수준)\n",
        "\n",
        "**71.4%p 성능 손실의 원인을 찾아 해결하자!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/home/kyj2024/anaconda3/envs/TVCLIP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Text-Vibration 정렬 문제 분석 시작\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/home/kyj2024/anaconda3/envs/TVCLIP/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "/data/home/kyj2024/anaconda3/envs/TVCLIP/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from collections import Counter\n",
        "import logging\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 프로젝트 루트 추가\n",
        "sys.path.append('/data/home/kyj2024/TextVibCLIP')\n",
        "\n",
        "from src.text_encoder import create_text_encoder\n",
        "from src.vibration_encoder import VibrationEncoder\n",
        "from src.textvib_model import TextVibCLIP\n",
        "from src.data_loader import BearingDataset, create_collate_fn\n",
        "from configs.model_config import MODEL_CONFIG\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"🔍 Text-Vibration 정렬 문제 분석 시작\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 데이터 및 모델 준비\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data_loader:UOS 데이터 라벨 분포: {'B_6204': 1, 'H_6204': 4, 'IR_6204': 1, 'OR_6204': 1}\n",
            "INFO:src.data_loader:최소 샘플 수: 1\n",
            "INFO:src.data_loader:UOS Domain-Incremental 윈도우 레벨 분할:\n",
            "INFO:src.data_loader:  모든 subset에 모든 7개 파일 포함\n",
            "INFO:src.data_loader:  각 파일 내에서 윈도우 분할: Train 60%, Val 20%, Test 20%\n",
            "INFO:src.data_loader:  Deep Groove Ball 7-클래스 분포: {'B': 1, 'H': 1, 'IR': 1, 'OR': 1, 'L': 1, 'M': 1, 'U': 1}\n",
            "INFO:src.data_loader:  클래스 수: 7개 (균형 확인)\n",
            "INFO:src.data_loader:  ✅ 완벽한 클래스 균형 달성!\n",
            "INFO:src.data_loader:UOS train 분할 결과:\n",
            "INFO:src.data_loader:  Train: 7개 파일, Val: 7개 파일, Test: 7개 파일\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "디바이스: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data_loader:BearingDataset 초기화 완료 (UOS): 7개 파일, 1249개 윈도우/파일, 총 8743개 샘플, Domain: 600, Subset: train\n",
            "INFO:src.data_loader:UOS 데이터 라벨 분포: {'B_6204': 1, 'H_6204': 4, 'IR_6204': 1, 'OR_6204': 1}\n",
            "INFO:src.data_loader:최소 샘플 수: 1\n",
            "INFO:src.data_loader:UOS Domain-Incremental 윈도우 레벨 분할:\n",
            "INFO:src.data_loader:  모든 subset에 모든 7개 파일 포함\n",
            "INFO:src.data_loader:  각 파일 내에서 윈도우 분할: Train 60%, Val 20%, Test 20%\n",
            "INFO:src.data_loader:  Deep Groove Ball 7-클래스 분포: {'B': 1, 'H': 1, 'IR': 1, 'OR': 1, 'L': 1, 'M': 1, 'U': 1}\n",
            "INFO:src.data_loader:  클래스 수: 7개 (균형 확인)\n",
            "INFO:src.data_loader:  ✅ 완벽한 클래스 균형 달성!\n",
            "INFO:src.data_loader:UOS test 분할 결과:\n",
            "INFO:src.data_loader:  Train: 7개 파일, Val: 7개 파일, Test: 7개 파일\n",
            "INFO:src.data_loader:BearingDataset 초기화 완료 (UOS): 7개 파일, 1249개 윈도우/파일, 총 8743개 샘플, Domain: 600, Subset: test\n",
            "INFO:src.data_loader:인덱스 매핑 생성 완료: 8743개 (파일 7개 × 윈도우 1249개)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train 데이터: 8743개 샘플\n",
            "Test 데이터: 8743개 샘플\n",
            "\n",
            "샘플 구조:\n",
            "  라벨: tensor([6, 0])\n",
            "  진동 신호 shape: torch.Size([2048])\n",
            "  텍스트: Shaft misalignment detected...\n"
          ]
        }
      ],
      "source": [
        "# 디바이스 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"디바이스: {device}\")\n",
        "\n",
        "# UOS 데이터셋 로드\n",
        "train_dataset = BearingDataset(\n",
        "    data_dir='data_scenario1',\n",
        "    dataset_type='uos',\n",
        "    domain_value=600,\n",
        "    subset='train'\n",
        ")\n",
        "\n",
        "test_dataset = BearingDataset(\n",
        "    data_dir='data_scenario1',\n",
        "    dataset_type='uos',\n",
        "    domain_value=600,\n",
        "    subset='test'\n",
        ")\n",
        "\n",
        "print(f\"Train 데이터: {len(train_dataset)}개 샘플\")\n",
        "print(f\"Test 데이터: {len(test_dataset)}개 샘플\")\n",
        "\n",
        "# 샘플 데이터 확인\n",
        "sample = train_dataset[0]\n",
        "print(f\"\\n샘플 구조:\")\n",
        "print(f\"  라벨: {sample['labels']}\")\n",
        "print(f\"  진동 신호 shape: {sample['vibration'].shape}\")\n",
        "print(f\"  텍스트: {sample['text'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 개별 Encoder 및 통합 모델 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.text_encoder:LoRA 적용 완료: rank=32, alpha=64\n",
            "INFO:src.text_encoder:Base DistilBERT 파라미터 freeze 완료: 100개 파라미터\n",
            "INFO:src.text_encoder:TextEncoder 초기화 완료: LoRA=True, Freeze=True\n",
            "INFO:src.text_encoder:TextEncoder 생성 (first_domain): Total=1,114,880, LoRA=589,824\n",
            "INFO:src.vibration_encoder:1D-CNN VibrationEncoder 초기화: input_length=2048, embedding_dim=256\n",
            "INFO:src.vibration_encoder:   OPTIMIZED: 커널 크기: [16, 32, 64, 32] - 4-layer 베어링 최적화\n",
            "INFO:src.vibration_encoder:   OPTIMIZED: 채널 수: [64, 128, 256, 512] - 자연스러운 64→512 증가\n",
            "INFO:src.vibration_encoder:   총 파라미터: 6,985,288\n",
            "INFO:src.text_encoder:LoRA 적용 완료: rank=32, alpha=64\n",
            "INFO:src.text_encoder:Base DistilBERT 파라미터 freeze 완료: 100개 파라미터\n",
            "INFO:src.text_encoder:TextEncoder 초기화 완료: LoRA=True, Freeze=True\n",
            "INFO:src.text_encoder:TextEncoder 생성 (first_domain): Total=1,114,880, LoRA=589,824\n",
            "INFO:src.vibration_encoder:1D-CNN VibrationEncoder 초기화: input_length=2048, embedding_dim=256\n",
            "INFO:src.vibration_encoder:   OPTIMIZED: 커널 크기: [16, 32, 64, 32] - 4-layer 베어링 최적화\n",
            "INFO:src.vibration_encoder:   OPTIMIZED: 채널 수: [64, 128, 256, 512] - 자연스러운 64→512 증가\n",
            "INFO:src.vibration_encoder:   총 파라미터: 6,985,288\n",
            "INFO:src.vibration_encoder:1D-CNN VibrationEncoder 생성 완료: 6,985,288 파라미터\n",
            "INFO:src.textvib_model:InfoNCE Loss 초기화: τ_text=0.050, τ_vib=0.050 (학습 가능)\n",
            "INFO:src.textvib_model:TextVibCLIP 초기화 완료: first_domain stage\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 모델 생성 완료:\n",
            "  개별 Text Encoder: 1,114,880개 파라미터\n",
            "  개별 Vibration Encoder: 6,985,288개 파라미터\n",
            "  TextVibCLIP 통합: 74,463,050개 파라미터\n",
            "\n",
            "📏 임베딩 차원:\n",
            "  개별 Text Encoder → 256차원\n",
            "  개별 Vibration Encoder → 256차원\n",
            "  통합 모델 → 256차원\n",
            "  Projection → 512차원\n",
            "\n",
            "🔍 통합 모델 구조:\n",
            "  통합 Text Encoder: True\n",
            "  통합 Vibration Encoder: True\n",
            "  InfoNCE Loss: True\n"
          ]
        }
      ],
      "source": [
        "# 1. 개별 Text Encoder 생성\n",
        "text_encoder = create_text_encoder('first_domain')\n",
        "text_encoder.to(device)\n",
        "text_encoder.eval()\n",
        "\n",
        "# 2. 개별 Vibration Encoder 생성\n",
        "vibration_encoder = VibrationEncoder(\n",
        "    input_length=MODEL_CONFIG['vibration_encoder']['input_length'],\n",
        "    embedding_dim=MODEL_CONFIG['embedding_dim']\n",
        ")\n",
        "vibration_encoder.to(device)\n",
        "vibration_encoder.eval()\n",
        "\n",
        "# 3. TextVibCLIP 통합 모델 생성 (올바른 생성자 사용)\n",
        "textvib_model = TextVibCLIP(\n",
        "    domain_stage='first_domain',\n",
        "    embedding_dim=MODEL_CONFIG['embedding_dim']\n",
        ")\n",
        "textvib_model.to(device)\n",
        "textvib_model.eval()\n",
        "\n",
        "print(\"🔧 모델 생성 완료:\")\n",
        "print(f\"  개별 Text Encoder: {text_encoder.get_trainable_parameters():,}개 파라미터\")\n",
        "print(f\"  개별 Vibration Encoder: {sum(p.numel() for p in vibration_encoder.parameters()):,}개 파라미터\") \n",
        "print(f\"  TextVibCLIP 통합: {sum(p.numel() for p in textvib_model.parameters()):,}개 파라미터\")\n",
        "\n",
        "# 임베딩 차원 확인\n",
        "print(f\"\\n📏 임베딩 차원:\")\n",
        "print(f\"  개별 Text Encoder → {MODEL_CONFIG['embedding_dim']}차원\")\n",
        "print(f\"  개별 Vibration Encoder → {MODEL_CONFIG['embedding_dim']}차원\")\n",
        "print(f\"  통합 모델 → {MODEL_CONFIG['embedding_dim']}차원\")\n",
        "print(f\"  Projection → {MODEL_CONFIG['projection']['hidden_dim']}차원\")\n",
        "\n",
        "# 통합 모델의 내부 encoder 접근\n",
        "print(f\"\\n🔍 통합 모델 구조:\")\n",
        "print(f\"  통합 Text Encoder: {hasattr(textvib_model, 'text_encoder')}\")\n",
        "print(f\"  통합 Vibration Encoder: {hasattr(textvib_model, 'vibration_encoder')}\")\n",
        "print(f\"  InfoNCE Loss: {hasattr(textvib_model, 'infonce_loss')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 임베딩 생성 및 정렬 상태 분석 (핵심)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 클래스별 데이터 샘플링...\n",
            "샘플링 완료: {0: 50, 1: 50, 2: 50, 3: 50, 4: 50, 5: 50, 6: 50}\n",
            "\n",
            "총 데이터: 350개 (텍스트 + 진동)\n",
            "진동 텐서 shape: torch.Size([350, 2048])\n",
            "클래스 분포: Counter({0: 50, 1: 50, 2: 50, 3: 50, 4: 50, 5: 50, 6: 50})\n"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터 샘플링 (각 클래스당 50개씩)\n",
        "samples_per_class = 50\n",
        "class_data = {i: {'texts': [], 'vibrations': [], 'labels': []} for i in range(7)}\n",
        "class_counts = {i: 0 for i in range(7)}\n",
        "\n",
        "print(\"🎯 클래스별 데이터 샘플링...\")\n",
        "for i in range(len(train_dataset)):\n",
        "    if all(count >= samples_per_class for count in class_counts.values()):\n",
        "        break\n",
        "        \n",
        "    sample = train_dataset[i]\n",
        "    label = sample['labels'][0].item()\n",
        "    \n",
        "    if class_counts[label] < samples_per_class:\n",
        "        class_data[label]['texts'].append(sample['text'])\n",
        "        class_data[label]['vibrations'].append(sample['vibration'])\n",
        "        class_data[label]['labels'].append(label)\n",
        "        class_counts[label] += 1\n",
        "\n",
        "print(f\"샘플링 완료: {class_counts}\")\n",
        "\n",
        "# 데이터 결합\n",
        "all_texts = []\n",
        "all_vibrations = []\n",
        "all_labels = []\n",
        "\n",
        "for class_id in range(7):\n",
        "    all_texts.extend(class_data[class_id]['texts'])\n",
        "    all_vibrations.extend(class_data[class_id]['vibrations'])\n",
        "    all_labels.extend(class_data[class_id]['labels'])\n",
        "\n",
        "vibration_tensor = torch.stack(all_vibrations)\n",
        "labels_tensor = torch.tensor(all_labels)\n",
        "\n",
        "print(f\"\\n총 데이터: {len(all_texts)}개 (텍스트 + 진동)\")\n",
        "print(f\"진동 텐서 shape: {vibration_tensor.shape}\")\n",
        "print(f\"클래스 분포: {Counter(all_labels)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 임베딩 생성 및 비교 분석\n",
            "==================================================\n",
            "✅ 임베딩 생성 완료\n",
            "  개별 Text: torch.Size([350, 256])\n",
            "  개별 Vibration: torch.Size([350, 256])\n",
            "  통합 Text: torch.Size([350, 256])\n",
            "  통합 Vibration: torch.Size([350, 256])\n",
            "\n",
            "📐 임베딩 비교:\n",
            "  차원 일치: True\n",
            "  개별 vs 통합 차이 (Text): 25.719437\n",
            "  개별 vs 통합 차이 (Vibration): 26.672676\n",
            "  Text 임베딩 동일성: False\n",
            "  Vibration 임베딩 동일성: False\n",
            "❌ 개별과 통합 임베딩이 다름!\n",
            "   → 통합 모델 내부에서 추가 변환이 발생\n"
          ]
        }
      ],
      "source": [
        "# 🔍 핵심: 개별 Encoder vs 통합 모델 임베딩 비교\n",
        "print(\"\\n🔍 임베딩 생성 및 비교 분석\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "batch_size = 32\n",
        "with torch.no_grad():\n",
        "    # 1. 개별 Text Encoder 임베딩\n",
        "    text_embeddings_individual = text_encoder.encode_texts(all_texts, device)\n",
        "    text_embeddings_individual = F.normalize(text_embeddings_individual, p=2, dim=1)\n",
        "    \n",
        "    # 2. 개별 Vibration Encoder 임베딩\n",
        "    vib_embeddings_individual = []\n",
        "    for i in range(0, len(vibration_tensor), batch_size):\n",
        "        batch_vib = vibration_tensor[i:i+batch_size].to(device)\n",
        "        batch_emb = vibration_encoder(batch_vib)\n",
        "        batch_emb = F.normalize(batch_emb, p=2, dim=1)\n",
        "        vib_embeddings_individual.append(batch_emb.cpu())\n",
        "    vib_embeddings_individual = torch.cat(vib_embeddings_individual, dim=0)\n",
        "    \n",
        "    # 3. 통합 모델 임베딩 (TextVibCLIP) - 내부 encoder 직접 사용\n",
        "    text_embeddings_integrated = []\n",
        "    vib_embeddings_integrated = []\n",
        "    \n",
        "    for i in range(0, len(all_texts), batch_size):\n",
        "        end_idx = min(i + batch_size, len(all_texts))\n",
        "        batch_texts = all_texts[i:end_idx]\n",
        "        batch_vibs = vibration_tensor[i:end_idx].to(device)\n",
        "        \n",
        "        # 통합 모델의 내부 encoder를 통해 임베딩 생성 (projection 없이)\n",
        "        batch_text_emb = textvib_model.text_encoder.encode_texts(batch_texts, device)\n",
        "        batch_vib_emb = textvib_model.vibration_encoder(batch_vibs)\n",
        "        \n",
        "        # 정규화 (TextVibCLIP과 동일한 방식)\n",
        "        batch_text_emb = F.normalize(batch_text_emb, p=2, dim=1)\n",
        "        batch_vib_emb = F.normalize(batch_vib_emb, p=2, dim=1)\n",
        "        \n",
        "        text_embeddings_integrated.append(batch_text_emb.cpu())\n",
        "        vib_embeddings_integrated.append(batch_vib_emb.cpu())\n",
        "    \n",
        "    text_embeddings_integrated = torch.cat(text_embeddings_integrated, dim=0)\n",
        "    vib_embeddings_integrated = torch.cat(vib_embeddings_integrated, dim=0)\n",
        "\n",
        "print(\"✅ 임베딩 생성 완료\")\n",
        "print(f\"  개별 Text: {text_embeddings_individual.shape}\")\n",
        "print(f\"  개별 Vibration: {vib_embeddings_individual.shape}\")\n",
        "print(f\"  통합 Text: {text_embeddings_integrated.shape}\")\n",
        "print(f\"  통합 Vibration: {vib_embeddings_integrated.shape}\")\n",
        "\n",
        "# 임베딩 차원 및 동일성 확인 (디바이스 통일)\n",
        "print(f\"\\n📐 임베딩 비교:\")\n",
        "print(f\"  차원 일치: {text_embeddings_individual.shape == text_embeddings_integrated.shape}\")\n",
        "\n",
        "# 디바이스를 CPU로 통일하여 비교\n",
        "text_individual_cpu = text_embeddings_individual.cpu()\n",
        "vib_individual_cpu = vib_embeddings_individual.cpu()\n",
        "text_integrated_cpu = text_embeddings_integrated.cpu()\n",
        "vib_integrated_cpu = vib_embeddings_integrated.cpu()\n",
        "\n",
        "text_diff = torch.norm(text_individual_cpu - text_integrated_cpu).item()\n",
        "vib_diff = torch.norm(vib_individual_cpu - vib_integrated_cpu).item()\n",
        "\n",
        "print(f\"  개별 vs 통합 차이 (Text): {text_diff:.6f}\")\n",
        "print(f\"  개별 vs 통합 차이 (Vibration): {vib_diff:.6f}\")\n",
        "\n",
        "# 동일성 검사\n",
        "text_identical = torch.allclose(text_individual_cpu, text_integrated_cpu, atol=1e-6)\n",
        "vib_identical = torch.allclose(vib_individual_cpu, vib_integrated_cpu, atol=1e-6)\n",
        "\n",
        "print(f\"  Text 임베딩 동일성: {text_identical}\")\n",
        "print(f\"  Vibration 임베딩 동일성: {vib_identical}\")\n",
        "\n",
        "if text_identical and vib_identical:\n",
        "    print(\"🚨 개별 모델과 통합 모델의 임베딩이 동일함!\")\n",
        "    print(\"   → Encoder 자체는 동일하게 작동\")\n",
        "    print(\"   → 문제는 InfoNCE Loss, Temperature, 또는 평가 로직에 있음\")\n",
        "elif text_diff < 1e-3 and vib_diff < 1e-3:\n",
        "    print(\"⚠️ 개별과 통합 임베딩이 거의 동일함 (미세한 차이)\")\n",
        "    print(\"   → 수치적 정밀도 차이일 가능성\")\n",
        "else:\n",
        "    print(\"❌ 개별과 통합 임베딩이 다름!\")\n",
        "    print(\"   → 통합 모델 내부에서 추가 변환이 발생\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 정렬 상태 진단 (Critical Analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚨 Text-Vibration 정렬 상태 진단\n",
            "============================================================\n",
            "\n",
            "📊 개별 Encoder 정렬 상태\n",
            "----------------------------------------\n",
            "Text 임베딩 통계:\n",
            "  평균: 0.002460, 표준편차: 0.062452\n",
            "  범위: [-0.190191, 0.185345]\n",
            "Vibration 임베딩 통계:\n",
            "  평균: -0.001136, 표준편차: 0.062490\n",
            "  범위: [-0.189785, 0.216230]\n",
            "Cross-modal 정렬 분석:\n",
            "  같은 클래스 유사도: 0.047532\n",
            "  다른 클래스 유사도: 0.047442\n",
            "  정렬 점수: 0.000090 (클수록 좋음)\n",
            "클래스별 Text-Vibration 정렬:\n",
            "  클래스 0: 0.054840\n",
            "  클래스 1: 0.050084\n",
            "  클래스 2: 0.049372\n",
            "  클래스 3: 0.052083\n",
            "  클래스 4: 0.042589\n",
            "  클래스 5: 0.049161\n",
            "  클래스 6: 0.034598\n",
            "  평균 클래스 정렬: 0.047532\n",
            "\n",
            "📊 통합 모델 정렬 상태\n",
            "----------------------------------------\n",
            "Text 임베딩 통계:\n",
            "  평균: 0.004101, 표준편차: 0.062366\n",
            "  범위: [-0.182250, 0.190655]\n",
            "Vibration 임베딩 통계:\n",
            "  평균: -0.002421, 표준편차: 0.062453\n",
            "  범위: [-0.166377, 0.179656]\n",
            "Cross-modal 정렬 분석:\n",
            "  같은 클래스 유사도: 0.002629\n",
            "  다른 클래스 유사도: 0.002651\n",
            "  정렬 점수: -0.000022 (클수록 좋음)\n",
            "클래스별 Text-Vibration 정렬:\n",
            "  클래스 0: 0.003898\n",
            "  클래스 1: 0.009290\n",
            "  클래스 2: -0.004035\n",
            "  클래스 3: 0.010970\n",
            "  클래스 4: -0.017201\n",
            "  클래스 5: 0.004601\n",
            "  클래스 6: 0.010880\n",
            "  평균 클래스 정렬: 0.002629\n"
          ]
        }
      ],
      "source": [
        "# 🚨 핵심 분석: Text-Vibration 정렬 상태 진단\n",
        "print(\"\\n🚨 Text-Vibration 정렬 상태 진단\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def analyze_alignment(text_emb, vib_emb, labels, title):\n",
        "    \"\"\"Text-Vibration 임베딩 정렬 상태 분석 (디바이스 안전)\"\"\"\n",
        "    print(f\"\\n📊 {title}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # 🔧 디바이스 통일: 모든 텐서를 CPU로 이동\n",
        "    text_emb_cpu = text_emb.cpu()\n",
        "    vib_emb_cpu = vib_emb.cpu()\n",
        "    labels_cpu = labels.cpu()\n",
        "    \n",
        "    # 1. 임베딩 통계\n",
        "    print(f\"Text 임베딩 통계:\")\n",
        "    print(f\"  평균: {text_emb_cpu.mean().item():.6f}, 표준편차: {text_emb_cpu.std().item():.6f}\")\n",
        "    print(f\"  범위: [{text_emb_cpu.min().item():.6f}, {text_emb_cpu.max().item():.6f}]\")\n",
        "    \n",
        "    print(f\"Vibration 임베딩 통계:\")\n",
        "    print(f\"  평균: {vib_emb_cpu.mean().item():.6f}, 표준편차: {vib_emb_cpu.std().item():.6f}\")\n",
        "    print(f\"  범위: [{vib_emb_cpu.min().item():.6f}, {vib_emb_cpu.max().item():.6f}]\")\n",
        "    \n",
        "    # 2. Cross-modal 유사도 분석\n",
        "    cross_similarity = torch.matmul(text_emb_cpu, vib_emb_cpu.t())  # (N, N)\n",
        "    \n",
        "    # 같은 클래스 간 유사도 (대각선)\n",
        "    same_class_sim = torch.diag(cross_similarity).mean().item()\n",
        "    \n",
        "    # 다른 클래스 간 유사도 (비대각선)\n",
        "    mask = ~torch.eye(len(labels_cpu), dtype=torch.bool)\n",
        "    diff_class_sim = cross_similarity[mask].mean().item()\n",
        "    \n",
        "    alignment_score = same_class_sim - diff_class_sim\n",
        "    \n",
        "    print(f\"Cross-modal 정렬 분석:\")\n",
        "    print(f\"  같은 클래스 유사도: {same_class_sim:.6f}\")\n",
        "    print(f\"  다른 클래스 유사도: {diff_class_sim:.6f}\")\n",
        "    print(f\"  정렬 점수: {alignment_score:.6f} (클수록 좋음)\")\n",
        "    \n",
        "    # 3. 클래스별 정렬 상태\n",
        "    unique_labels = torch.unique(labels_cpu)\n",
        "    print(f\"클래스별 Text-Vibration 정렬:\")\n",
        "    \n",
        "    class_alignments = []\n",
        "    for cls in unique_labels:\n",
        "        cls_mask = (labels_cpu == cls)\n",
        "        cls_text = text_emb_cpu[cls_mask]\n",
        "        cls_vib = vib_emb_cpu[cls_mask]\n",
        "        \n",
        "        # 클래스 내 Text-Vibration 유사도\n",
        "        if len(cls_text) > 0:\n",
        "            cls_sim = torch.matmul(cls_text, cls_vib.t()).diag().mean().item()\n",
        "            class_alignments.append(cls_sim)\n",
        "            print(f\"  클래스 {cls.item()}: {cls_sim:.6f}\")\n",
        "    \n",
        "    avg_class_alignment = np.mean(class_alignments)\n",
        "    print(f\"  평균 클래스 정렬: {avg_class_alignment:.6f}\")\n",
        "    \n",
        "    return {\n",
        "        'alignment_score': alignment_score,\n",
        "        'same_class_sim': same_class_sim,\n",
        "        'diff_class_sim': diff_class_sim,\n",
        "        'avg_class_alignment': avg_class_alignment,\n",
        "        'class_alignments': class_alignments\n",
        "    }\n",
        "\n",
        "# 개별 Encoder 정렬 분석\n",
        "individual_analysis = analyze_alignment(\n",
        "    text_embeddings_individual, \n",
        "    vib_embeddings_individual, \n",
        "    labels_tensor,\n",
        "    \"개별 Encoder 정렬 상태\"\n",
        ")\n",
        "\n",
        "# 통합 모델 정렬 분석  \n",
        "integrated_analysis = analyze_alignment(\n",
        "    text_embeddings_integrated,\n",
        "    vib_embeddings_integrated, \n",
        "    labels_tensor,\n",
        "    \"통합 모델 정렬 상태\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. InfoNCE Loss 및 Temperature 분석\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔥 InfoNCE Loss 분석\n",
            "==================================================\n",
            "현재 Temperature 설정:\n",
            "  First domain text: 0.05\n",
            "  First domain vibration: 0.05\n",
            "  Continual text: 0.07\n",
            "  Continual vibration: 0.03\n",
            "\n",
            "📈 Temperature별 InfoNCE 성능 (개별 Encoder):\n",
            "Temp\tLoss\tAccuracy\n",
            "------------------------------\n",
            "0.01\t5.9496\t0.0029\n",
            "0.05\t5.8602\t0.0029\n",
            "0.07\t5.8587\t0.0029\n",
            "0.10\t5.8581\t0.0029\n",
            "0.20\t5.8577\t0.0029\n",
            "0.50\t5.8578\t0.0029\n",
            "\n",
            "📈 Temperature별 InfoNCE 성능 (통합 모델):\n",
            "Temp\tLoss\tAccuracy\n",
            "------------------------------\n",
            "0.01\t6.2571\t0.0029\n",
            "0.05\t5.8786\t0.0029\n",
            "0.07\t5.8688\t0.0029\n",
            "0.10\t5.8634\t0.0029\n",
            "0.20\t5.8594\t0.0029\n",
            "0.50\t5.8582\t0.0029\n",
            "\n",
            "🎯 최적 Temperature:\n",
            "  개별 Encoder: 0.01 (정확도: 0.0029)\n",
            "  통합 모델: 0.01 (정확도: 0.0029)\n"
          ]
        }
      ],
      "source": [
        "# 🔥 InfoNCE Loss 및 Temperature 파라미터 분석\n",
        "print(\"\\n🔥 InfoNCE Loss 분석\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def calculate_infonce_loss(text_emb, vib_emb, temperature=0.07):\n",
        "    \"\"\"InfoNCE Loss 계산 및 분석 (디바이스 안전)\"\"\"\n",
        "    # 🔧 디바이스 통일: 모든 텐서를 CPU로 이동\n",
        "    text_emb_cpu = text_emb.cpu()\n",
        "    vib_emb_cpu = vib_emb.cpu()\n",
        "    \n",
        "    # 유사도 행렬 계산\n",
        "    similarity_matrix = torch.matmul(text_emb_cpu, vib_emb_cpu.t()) / temperature\n",
        "    \n",
        "    # 정답 레이블 (대각선)\n",
        "    labels = torch.arange(len(text_emb_cpu))\n",
        "    \n",
        "    # Cross-entropy loss 계산\n",
        "    loss = F.cross_entropy(similarity_matrix, labels)\n",
        "    \n",
        "    # 예측 정확도 계산\n",
        "    predictions = torch.argmax(similarity_matrix, dim=1)\n",
        "    accuracy = (predictions == labels).float().mean()\n",
        "    \n",
        "    return {\n",
        "        'loss': loss.item(),\n",
        "        'accuracy': accuracy.item(),\n",
        "        'similarity_matrix': similarity_matrix,\n",
        "        'predictions': predictions\n",
        "    }\n",
        "\n",
        "# 현재 설정된 Temperature 값들\n",
        "temp_config = MODEL_CONFIG['infonce']\n",
        "print(f\"현재 Temperature 설정:\")\n",
        "print(f\"  First domain text: {temp_config['first_domain_temperature_text']}\")\n",
        "print(f\"  First domain vibration: {temp_config['first_domain_temperature_vib']}\")\n",
        "print(f\"  Continual text: {temp_config['continual_temperature_text']}\")\n",
        "print(f\"  Continual vibration: {temp_config['continual_temperature_vib']}\")\n",
        "\n",
        "# 다양한 Temperature 값으로 테스트\n",
        "temperatures = [0.01, 0.05, 0.07, 0.1, 0.2, 0.5]\n",
        "print(f\"\\n📈 Temperature별 InfoNCE 성능 (개별 Encoder):\")\n",
        "print(\"Temp\\tLoss\\tAccuracy\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "individual_results = []\n",
        "for temp in temperatures:\n",
        "    result = calculate_infonce_loss(\n",
        "        text_embeddings_individual, \n",
        "        vib_embeddings_individual, \n",
        "        temperature=temp\n",
        "    )\n",
        "    individual_results.append((temp, result))\n",
        "    print(f\"{temp:.2f}\\t{result['loss']:.4f}\\t{result['accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\n📈 Temperature별 InfoNCE 성능 (통합 모델):\")\n",
        "print(\"Temp\\tLoss\\tAccuracy\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "integrated_results = []\n",
        "for temp in temperatures:\n",
        "    result = calculate_infonce_loss(\n",
        "        text_embeddings_integrated,\n",
        "        vib_embeddings_integrated, \n",
        "        temperature=temp\n",
        "    )\n",
        "    integrated_results.append((temp, result))\n",
        "    print(f\"{temp:.2f}\\t{result['loss']:.4f}\\t{result['accuracy']:.4f}\")\n",
        "\n",
        "# 최적 Temperature 찾기\n",
        "best_individual = max(individual_results, key=lambda x: x[1]['accuracy'])\n",
        "best_integrated = max(integrated_results, key=lambda x: x[1]['accuracy'])\n",
        "\n",
        "print(f\"\\n🎯 최적 Temperature:\")\n",
        "print(f\"  개별 Encoder: {best_individual[0]:.2f} (정확도: {best_individual[1]['accuracy']:.4f})\")\n",
        "print(f\"  통합 모델: {best_integrated[0]:.2f} (정확도: {best_integrated[1]['accuracy']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 결론 및 해결 방안\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "🎯 Text-Vibration 정렬 문제 진단 결과\n",
            "============================================================\n",
            "📊 성능 요약:\n",
            "  개별 Text Encoder: 83.0% (이전 테스트)\n",
            "  개별 Vibration Encoder: 88.6% (이전 테스트)\n",
            "  TextVibCLIP 통합: 14.4% (실제 성능)\n",
            "  성능 손실: 157.2%p (심각한 정렬 문제)\n",
            "\n",
            "🔍 정렬 분석 결과:\n",
            "  개별 Encoder 정렬 점수: 0.000090\n",
            "  통합 모델 정렬 점수: -0.000022\n",
            "  정렬 점수 차이: 0.000112\n",
            "\n",
            "🌡️ Temperature 최적화:\n",
            "  개별 최적 Temperature: 0.01\n",
            "  통합 최적 Temperature: 0.01\n",
            "  현재 설정값: 0.05\n",
            "\n",
            "🚨 문제 진단:\n",
            "  ❌ 심각한 정렬 문제: 통합 과정에서 정렬이 크게 악화됨\n",
            "  원인: Projection Layer, InfoNCE Loss, 또는 모델 아키텍처 문제\n",
            "\n",
            "💡 해결 방안 우선순위:\n",
            "1. 🔥 Temperature 최적화\n",
            "   - 현재: 0.05\n",
            "   - 권장: 0.01\n",
            "2. 🎯 Projection Layer 점검\n",
            "   - 차원 정렬 확인\n",
            "   - 정규화 방법 검토\n",
            "3. 📐 InfoNCE Loss 구현 검토\n",
            "   - 유사도 계산 로직\n",
            "   - Negative sampling 방식\n",
            "4. 🔧 모델 아키텍처 개선\n",
            "   - Cross-attention 메커니즘 추가\n",
            "   - Alignment regularization\n",
            "\n",
            "⚡ 즉시 수정 가능한 사항:\n",
            "1. Temperature 0.05 → 0.01로 변경\n",
            "   예상 성능 향상: 0.3%\n",
            "\n",
            "🎯 다음 단계:\n",
            "1. Temperature 수정 후 재실험\n",
            "2. Projection Layer 상세 분석\n",
            "3. InfoNCE Loss 구현 검토\n",
            "4. 전체 모델 재학습 고려\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎯 Text-Vibration 정렬 문제 진단 결과\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 성능 비교 요약\n",
        "print(f\"📊 성능 요약:\")\n",
        "print(f\"  개별 Text Encoder: 83.0% (이전 테스트)\")\n",
        "print(f\"  개별 Vibration Encoder: 88.6% (이전 테스트)\")\n",
        "print(f\"  TextVibCLIP 통합: 14.4% (실제 성능)\")\n",
        "print(f\"  성능 손실: {83.0 + 88.6 - 14.4:.1f}%p (심각한 정렬 문제)\")\n",
        "\n",
        "# 정렬 분석 결과 요약\n",
        "print(f\"\\n🔍 정렬 분석 결과:\")\n",
        "print(f\"  개별 Encoder 정렬 점수: {individual_analysis['alignment_score']:.6f}\")\n",
        "print(f\"  통합 모델 정렬 점수: {integrated_analysis['alignment_score']:.6f}\")\n",
        "print(f\"  정렬 점수 차이: {individual_analysis['alignment_score'] - integrated_analysis['alignment_score']:.6f}\")\n",
        "\n",
        "# Temperature 분석 결과\n",
        "print(f\"\\n🌡️ Temperature 최적화:\")\n",
        "print(f\"  개별 최적 Temperature: {best_individual[0]:.2f}\")\n",
        "print(f\"  통합 최적 Temperature: {best_integrated[0]:.2f}\")\n",
        "print(f\"  현재 설정값: {temp_config['first_domain_temperature_text']:.2f}\")\n",
        "\n",
        "# 문제 진단 및 해결 방안\n",
        "print(f\"\\n🚨 문제 진단:\")\n",
        "if integrated_analysis['alignment_score'] < individual_analysis['alignment_score'] * 0.5:\n",
        "    print(\"  ❌ 심각한 정렬 문제: 통합 과정에서 정렬이 크게 악화됨\")\n",
        "    print(\"  원인: Projection Layer, InfoNCE Loss, 또는 모델 아키텍처 문제\")\n",
        "elif integrated_analysis['alignment_score'] < individual_analysis['alignment_score'] * 0.8:\n",
        "    print(\"  ⚠️ 중간 정렬 문제: 통합 과정에서 일부 정렬 손실\")\n",
        "    print(\"  원인: Temperature 설정 또는 Loss 가중치 문제\")\n",
        "else:\n",
        "    print(\"  ✅ 정렬 상태 양호: 다른 원인 탐색 필요\")\n",
        "\n",
        "print(f\"\\n💡 해결 방안 우선순위:\")\n",
        "print(\"1. 🔥 Temperature 최적화\")\n",
        "print(f\"   - 현재: {temp_config['first_domain_temperature_text']:.2f}\")\n",
        "print(f\"   - 권장: {best_integrated[0]:.2f}\")\n",
        "\n",
        "print(\"2. 🎯 Projection Layer 점검\")\n",
        "print(\"   - 차원 정렬 확인\")\n",
        "print(\"   - 정규화 방법 검토\")\n",
        "\n",
        "print(\"3. 📐 InfoNCE Loss 구현 검토\")\n",
        "print(\"   - 유사도 계산 로직\")\n",
        "print(\"   - Negative sampling 방식\")\n",
        "\n",
        "print(\"4. 🔧 모델 아키텍처 개선\")\n",
        "print(\"   - Cross-attention 메커니즘 추가\")\n",
        "print(\"   - Alignment regularization\")\n",
        "\n",
        "# 즉시 적용 가능한 수정사항\n",
        "print(f\"\\n⚡ 즉시 수정 가능한 사항:\")\n",
        "if abs(best_integrated[0] - temp_config['first_domain_temperature_text']) > 0.02:\n",
        "    print(f\"1. Temperature {temp_config['first_domain_temperature_text']:.2f} → {best_integrated[0]:.2f}로 변경\")\n",
        "    print(f\"   예상 성능 향상: {best_integrated[1]['accuracy']:.1%}\")\n",
        "\n",
        "print(\"\\n🎯 다음 단계:\")\n",
        "print(\"1. Temperature 수정 후 재실험\")\n",
        "print(\"2. Projection Layer 상세 분석\")\n",
        "print(\"3. InfoNCE Loss 구현 검토\")\n",
        "print(\"4. 전체 모델 재학습 고려\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 긴급 수정: 가중치 동기화 테스트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚨 가중치 동기화 테스트\n",
            "==================================================\n",
            "1. Text Encoder 가중치 복사...\n",
            "2. Vibration Encoder 가중치 복사...\n",
            "✅ 가중치 복사 완료\n",
            "\n",
            "3. 동기화 후 임베딩 재생성...\n",
            "✅ 동기화 후 차이:\n",
            "  Text 차이: 0.000016\n",
            "  Vibration 차이: 0.000000\n",
            "⚠️ 여전히 차이 존재 - 다른 원인 탐색 필요\n",
            "\n",
            "4. 동기화 후 InfoNCE 성능:\n",
            "  Loss: 5.8602\n",
            "  Accuracy: 0.0029 (0.3%)\n",
            "🔍 가중치 외 다른 문제 존재\n"
          ]
        }
      ],
      "source": [
        "# 🚨 긴급 수정: 통합 모델에 개별 모델 가중치 복사\n",
        "print(\"\\n🚨 가중치 동기화 테스트\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. 개별 모델 가중치를 통합 모델에 복사\n",
        "print(\"1. Text Encoder 가중치 복사...\")\n",
        "textvib_model.text_encoder.load_state_dict(text_encoder.state_dict())\n",
        "\n",
        "print(\"2. Vibration Encoder 가중치 복사...\")\n",
        "textvib_model.vibration_encoder.load_state_dict(vibration_encoder.state_dict())\n",
        "\n",
        "print(\"✅ 가중치 복사 완료\")\n",
        "\n",
        "# 2. 가중치 동기화 후 임베딩 재생성\n",
        "print(\"\\n3. 동기화 후 임베딩 재생성...\")\n",
        "with torch.no_grad():\n",
        "    # 통합 모델 임베딩 재생성 (가중치 동기화 후)\n",
        "    text_embeddings_synced = []\n",
        "    vib_embeddings_synced = []\n",
        "    \n",
        "    for i in range(0, len(all_texts), batch_size):\n",
        "        end_idx = min(i + batch_size, len(all_texts))\n",
        "        batch_texts = all_texts[i:end_idx]\n",
        "        batch_vibs = vibration_tensor[i:end_idx].to(device)\n",
        "        \n",
        "        # 동기화된 encoder로 임베딩 생성\n",
        "        batch_text_emb = textvib_model.text_encoder.encode_texts(batch_texts, device)\n",
        "        batch_vib_emb = textvib_model.vibration_encoder(batch_vibs)\n",
        "        \n",
        "        # 정규화\n",
        "        batch_text_emb = F.normalize(batch_text_emb, p=2, dim=1)\n",
        "        batch_vib_emb = F.normalize(batch_vib_emb, p=2, dim=1)\n",
        "        \n",
        "        text_embeddings_synced.append(batch_text_emb.cpu())\n",
        "        vib_embeddings_synced.append(batch_vib_emb.cpu())\n",
        "    \n",
        "    text_embeddings_synced = torch.cat(text_embeddings_synced, dim=0)\n",
        "    vib_embeddings_synced = torch.cat(vib_embeddings_synced, dim=0)\n",
        "\n",
        "# 3. 동기화 후 차이 확인\n",
        "text_diff_synced = torch.norm(text_embeddings_individual.cpu() - text_embeddings_synced.cpu()).item()\n",
        "vib_diff_synced = torch.norm(vib_embeddings_individual.cpu() - vib_embeddings_synced.cpu()).item()\n",
        "\n",
        "print(f\"✅ 동기화 후 차이:\")\n",
        "print(f\"  Text 차이: {text_diff_synced:.6f}\")\n",
        "print(f\"  Vibration 차이: {vib_diff_synced:.6f}\")\n",
        "\n",
        "if text_diff_synced < 1e-6 and vib_diff_synced < 1e-6:\n",
        "    print(\"🎯 완벽한 동기화! 가중치 문제였음\")\n",
        "else:\n",
        "    print(\"⚠️ 여전히 차이 존재 - 다른 원인 탐색 필요\")\n",
        "\n",
        "# 4. 동기화 후 InfoNCE 성능 테스트\n",
        "print(\"\\n4. 동기화 후 InfoNCE 성능:\")\n",
        "synced_result = calculate_infonce_loss(\n",
        "    text_embeddings_synced,\n",
        "    vib_embeddings_synced, \n",
        "    temperature=0.05\n",
        ")\n",
        "print(f\"  Loss: {synced_result['loss']:.4f}\")\n",
        "print(f\"  Accuracy: {synced_result['accuracy']:.4f} ({synced_result['accuracy']*100:.1f}%)\")\n",
        "\n",
        "if synced_result['accuracy'] > 0.1:\n",
        "    print(\"🎉 가중치 동기화로 문제 해결!\")\n",
        "else:\n",
        "    print(\"🔍 가중치 외 다른 문제 존재\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. InfoNCE Loss 구현 문제 분석 및 수정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔥 InfoNCE Loss 구현 문제 진단\n",
            "============================================================\n",
            "1. 개선된 InfoNCE Loss 테스트:\n",
            "  개선된 Loss: 5.8589\n",
            "  개선된 Accuracy: 0.1429 (14.3%)\n",
            "\n",
            "2. 개선된 InfoNCE - Temperature별 성능:\n",
            "Temp\tLoss\tAccuracy\n",
            "------------------------------\n",
            "0.01\t5.9511\t0.1429\n",
            "0.03\t5.8667\t0.1429\n",
            "0.05\t5.8605\t0.1429\n",
            "0.07\t5.8589\t0.1429\n",
            "0.10\t5.8582\t0.1429\n",
            "0.20\t5.8578\t0.1429\n",
            "0.50\t5.8578\t0.1429\n",
            "1.00\t5.8579\t0.1429\n",
            "\n",
            "🎯 최적 설정:\n",
            "  최적 Temperature: 0.01\n",
            "  최고 정확도: 0.1429 (14.3%)\n",
            "❌ 여전히 심각한 문제 - 근본적 재설계 필요\n"
          ]
        }
      ],
      "source": [
        "# 🔥 InfoNCE Loss 구현 문제 분석 및 수정\n",
        "print(\"\\n🔥 InfoNCE Loss 구현 문제 진단\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def improved_infonce_loss(text_emb, vib_emb, labels, temperature=0.07):\n",
        "    \"\"\"개선된 InfoNCE Loss - 클래스 기반 정확한 구현\"\"\"\n",
        "    text_emb_cpu = text_emb.cpu()\n",
        "    vib_emb_cpu = vib_emb.cpu()\n",
        "    labels_cpu = labels.cpu()\n",
        "    \n",
        "    # 유사도 행렬 계산\n",
        "    similarity_matrix = torch.matmul(text_emb_cpu, vib_emb_cpu.t()) / temperature\n",
        "    \n",
        "    # 🎯 핵심 수정: 클래스 기반 positive pair 정의\n",
        "    # 같은 인덱스가 아니라 같은 클래스끼리 positive\n",
        "    batch_size = len(text_emb_cpu)\n",
        "    positive_mask = (labels_cpu.unsqueeze(0) == labels_cpu.unsqueeze(1)).float()\n",
        "    \n",
        "    # InfoNCE Loss 계산 (클래스 기반)\n",
        "    # 각 text에 대해 같은 클래스의 vibration들이 positive\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        # i번째 text와 모든 vibration의 유사도\n",
        "        logits = similarity_matrix[i]  # (batch_size,)\n",
        "        \n",
        "        # 같은 클래스 마스크\n",
        "        pos_mask = positive_mask[i]  # (batch_size,)\n",
        "        \n",
        "        if pos_mask.sum() > 0:  # positive가 존재하는 경우만\n",
        "            # Positive의 평균 유사도\n",
        "            pos_logits = logits[pos_mask.bool()]\n",
        "            pos_mean = pos_logits.mean()\n",
        "            \n",
        "            # 전체 logits에 대한 logsumexp\n",
        "            log_sum_exp = torch.logsumexp(logits, dim=0)\n",
        "            \n",
        "            # InfoNCE loss\n",
        "            loss = log_sum_exp - pos_mean\n",
        "            losses.append(loss)\n",
        "            \n",
        "            # 예측: 가장 높은 유사도를 가진 vibration\n",
        "            pred_idx = torch.argmax(logits)\n",
        "            if pos_mask[pred_idx] > 0:\n",
        "                correct_predictions += 1\n",
        "    \n",
        "    avg_loss = torch.stack(losses).mean() if losses else torch.tensor(float('inf'))\n",
        "    accuracy = correct_predictions / batch_size\n",
        "    \n",
        "    return {\n",
        "        'loss': avg_loss.item(),\n",
        "        'accuracy': accuracy,\n",
        "        'similarity_matrix': similarity_matrix,\n",
        "        'positive_mask': positive_mask\n",
        "    }\n",
        "\n",
        "# 🎯 개선된 InfoNCE Loss로 테스트\n",
        "print(\"1. 개선된 InfoNCE Loss 테스트:\")\n",
        "improved_result = improved_infonce_loss(\n",
        "    text_embeddings_synced,\n",
        "    vib_embeddings_synced,\n",
        "    labels_tensor,\n",
        "    temperature=0.07\n",
        ")\n",
        "\n",
        "print(f\"  개선된 Loss: {improved_result['loss']:.4f}\")\n",
        "print(f\"  개선된 Accuracy: {improved_result['accuracy']:.4f} ({improved_result['accuracy']*100:.1f}%)\")\n",
        "\n",
        "# 2. 다양한 Temperature로 개선된 Loss 테스트\n",
        "print(f\"\\n2. 개선된 InfoNCE - Temperature별 성능:\")\n",
        "print(\"Temp\\tLoss\\tAccuracy\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "best_temp = 0.07\n",
        "best_acc = 0.0\n",
        "\n",
        "for temp in [0.01, 0.03, 0.05, 0.07, 0.1, 0.2, 0.5, 1.0]:\n",
        "    result = improved_infonce_loss(\n",
        "        text_embeddings_synced,\n",
        "        vib_embeddings_synced,\n",
        "        labels_tensor,\n",
        "        temperature=temp\n",
        "    )\n",
        "    print(f\"{temp:.2f}\\t{result['loss']:.4f}\\t{result['accuracy']:.4f}\")\n",
        "    \n",
        "    if result['accuracy'] > best_acc:\n",
        "        best_acc = result['accuracy']\n",
        "        best_temp = temp\n",
        "\n",
        "print(f\"\\n🎯 최적 설정:\")\n",
        "print(f\"  최적 Temperature: {best_temp:.2f}\")\n",
        "print(f\"  최고 정확도: {best_acc:.4f} ({best_acc*100:.1f}%)\")\n",
        "\n",
        "if best_acc > 0.5:\n",
        "    print(\"🎉 InfoNCE Loss 문제 해결!\")\n",
        "elif best_acc > 0.2:\n",
        "    print(\"⚠️ 부분적 개선 - 추가 조정 필요\")\n",
        "else:\n",
        "    print(\"❌ 여전히 심각한 문제 - 근본적 재설계 필요\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 근본적 해결책: Cross-Modal Projection 테스트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 Cross-Modal Projection 테스트\n",
            "============================================================\n",
            "1. Cross-Modal Projection 학습 (10 epochs)...\n",
            "  Epoch 0: Loss = 3.4657\n",
            "  Epoch 2: Loss = 3.4604\n",
            "  Epoch 4: Loss = 3.4628\n",
            "  Epoch 6: Loss = 3.4574\n",
            "  Epoch 8: Loss = 3.4597\n",
            "\n",
            "2. 학습된 Projection 적용...\n",
            "✅ Projection 적용 완료:\n",
            "  변환 전: 256차원\n",
            "  변환 후: 128차원\n",
            "\n",
            "3. Projection 후 InfoNCE 성능:\n",
            "  Projection 후 Loss: 5.8502\n",
            "  Projection 후 Accuracy: 0.2857 (28.6%)\n",
            "\n",
            "📊 최종 성능 비교:\n",
            "  개별 Text Encoder: 83.0%\n",
            "  개별 Vibration Encoder: 88.6%\n",
            "  기존 통합 (InfoNCE): 0.3%\n",
            "  Projection 후: 28.6%\n",
            "  개선도: +28.3%p\n",
            "❌ 근본적 문제 - 다른 접근법 필요\n"
          ]
        }
      ],
      "source": [
        "# 🎯 근본적 해결책: Cross-Modal Projection Layer 추가\n",
        "print(\"\\n🎯 Cross-Modal Projection 테스트\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class CrossModalProjection(nn.Module):\n",
        "    \"\"\"Cross-Modal Projection Layer - 두 임베딩 공간을 공통 공간으로 매핑\"\"\"\n",
        "    def __init__(self, input_dim=256, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.text_proj = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(output_dim * 2, output_dim),\n",
        "            nn.LayerNorm(output_dim)\n",
        "        )\n",
        "        self.vib_proj = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim * 2),\n",
        "            nn.ReLU(), \n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(output_dim * 2, output_dim),\n",
        "            nn.LayerNorm(output_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, text_emb, vib_emb):\n",
        "        text_proj = F.normalize(self.text_proj(text_emb), p=2, dim=1)\n",
        "        vib_proj = F.normalize(self.vib_proj(vib_emb), p=2, dim=1)\n",
        "        return text_proj, vib_proj\n",
        "\n",
        "# Cross-Modal Projection 생성 및 간단한 학습\n",
        "projection = CrossModalProjection(input_dim=256, output_dim=128).to(device)\n",
        "optimizer = torch.optim.Adam(projection.parameters(), lr=0.001)\n",
        "\n",
        "print(\"1. Cross-Modal Projection 학습 (10 epochs)...\")\n",
        "\n",
        "# 간단한 학습 루프\n",
        "projection.train()\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for i in range(0, len(text_embeddings_synced), 32):\n",
        "        end_idx = min(i + 32, len(text_embeddings_synced))\n",
        "        batch_text = text_embeddings_synced[i:end_idx].to(device)\n",
        "        batch_vib = vib_embeddings_synced[i:end_idx].to(device)\n",
        "        batch_labels = labels_tensor[i:end_idx].to(device)\n",
        "        \n",
        "        # Projection 적용\n",
        "        text_proj, vib_proj = projection(batch_text, batch_vib)\n",
        "        \n",
        "        # InfoNCE Loss (간단한 대각선 버전)\n",
        "        similarity = torch.matmul(text_proj, vib_proj.t()) / 0.1\n",
        "        labels_batch = torch.arange(len(batch_text)).to(device)\n",
        "        loss = F.cross_entropy(similarity, labels_batch)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    if epoch % 2 == 0:\n",
        "        print(f\"  Epoch {epoch}: Loss = {total_loss/num_batches:.4f}\")\n",
        "\n",
        "projection.eval()\n",
        "\n",
        "# 2. 학습된 Projection으로 임베딩 변환\n",
        "print(\"\\n2. 학습된 Projection 적용...\")\n",
        "with torch.no_grad():\n",
        "    text_projected = []\n",
        "    vib_projected = []\n",
        "    \n",
        "    for i in range(0, len(text_embeddings_synced), 32):\n",
        "        end_idx = min(i + 32, len(text_embeddings_synced))\n",
        "        batch_text = text_embeddings_synced[i:end_idx].to(device)\n",
        "        batch_vib = vib_embeddings_synced[i:end_idx].to(device)\n",
        "        \n",
        "        text_proj, vib_proj = projection(batch_text, batch_vib)\n",
        "        text_projected.append(text_proj.cpu())\n",
        "        vib_projected.append(vib_proj.cpu())\n",
        "    \n",
        "    text_projected = torch.cat(text_projected, dim=0)\n",
        "    vib_projected = torch.cat(vib_projected, dim=0)\n",
        "\n",
        "print(f\"✅ Projection 적용 완료:\")\n",
        "print(f\"  변환 전: {text_embeddings_synced.shape[1]}차원\")\n",
        "print(f\"  변환 후: {text_projected.shape[1]}차원\")\n",
        "\n",
        "# 3. Projection 후 InfoNCE 성능 테스트\n",
        "print(\"\\n3. Projection 후 InfoNCE 성능:\")\n",
        "projected_result = improved_infonce_loss(\n",
        "    text_projected,\n",
        "    vib_projected,\n",
        "    labels_tensor,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "print(f\"  Projection 후 Loss: {projected_result['loss']:.4f}\")\n",
        "print(f\"  Projection 후 Accuracy: {projected_result['accuracy']:.4f} ({projected_result['accuracy']*100:.1f}%)\")\n",
        "\n",
        "# 4. 성능 비교 요약\n",
        "print(f\"\\n📊 최종 성능 비교:\")\n",
        "print(f\"  개별 Text Encoder: 83.0%\")\n",
        "print(f\"  개별 Vibration Encoder: 88.6%\")\n",
        "print(f\"  기존 통합 (InfoNCE): 0.3%\")\n",
        "print(f\"  Projection 후: {projected_result['accuracy']*100:.1f}%\")\n",
        "\n",
        "improvement = projected_result['accuracy'] * 100 - 0.3\n",
        "print(f\"  개선도: +{improvement:.1f}%p\")\n",
        "\n",
        "if projected_result['accuracy'] > 0.5:\n",
        "    print(\"🎉 Cross-Modal Projection으로 문제 해결!\")\n",
        "elif projected_result['accuracy'] > 0.3:\n",
        "    print(\"⚠️ 부분적 개선 - Projection 아키텍처 조정 필요\")\n",
        "else:\n",
        "    print(\"❌ 근본적 문제 - 다른 접근법 필요\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 최종 진단 및 실제 해결책\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "🎯 TextVibCLIP 정렬 문제 최종 진단 및 해결책\n",
            "================================================================================\n",
            "📊 문제 진단 요약:\n",
            "1. ✅ Text Encoder 단독: 83.0% (우수)\n",
            "2. ✅ Vibration Encoder 단독: 88.6% (우수)\n",
            "3. ❌ 가중치 불일치: 해결됨 (차이 0.000016)\n",
            "4. ❌ InfoNCE Loss 구현: 14.3% (랜덤 수준)\n",
            "5. 🎯 Cross-Modal Projection: 28.6%\n",
            "\n",
            "🔍 근본 원인:\n",
            "Text와 Vibration 임베딩이 완전히 다른 의미 공간에 존재\n",
            "- Text: 언어적 의미 공간 (단어, 문법 기반)\n",
            "- Vibration: 물리적 신호 공간 (주파수, 진폭 기반)\n",
            "- 결과: 두 공간 간 자연스러운 매핑이 존재하지 않음\n",
            "\n",
            "💡 해결책 우선순위:\n",
            "1. ❌ 근본적 재설계 필요\n",
            "2. 사전 학습된 멀티모달 모델 활용 고려\n",
            "3. 대안: 각 Encoder 독립 사용 + 앙상블\n",
            "\n",
            "🚀 즉시 적용 방안:\n",
            "1. TextVibCLIP 모델에 Cross-Modal Projection 추가\n",
            "2. Temperature 최적화: 0.05 → 0.1\n",
            "3. Projection 학습률 및 아키텍처 조정\n",
            "\n",
            "📈 예상 성능 (Projection 적용 시):\n",
            "  TextVibCLIP 통합: 14.4% → 30%\n",
            "  성능 회복: 15.6%p\n",
            "\n",
            "🔍 다음 단계: 대안 접근법 고려\n",
            "1. 사전 학습된 멀티모달 모델 활용\n",
            "2. 각 Encoder 독립 사용 + Late Fusion\n",
            "3. Domain-specific Adapter 추가\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎯 TextVibCLIP 정렬 문제 최종 진단 및 해결책\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"📊 문제 진단 요약:\")\n",
        "print(f\"1. ✅ Text Encoder 단독: 83.0% (우수)\")\n",
        "print(f\"2. ✅ Vibration Encoder 단독: 88.6% (우수)\")\n",
        "print(f\"3. ❌ 가중치 불일치: 해결됨 (차이 0.000016)\")\n",
        "print(f\"4. ❌ InfoNCE Loss 구현: 14.3% (랜덤 수준)\")\n",
        "print(f\"5. 🎯 Cross-Modal Projection: {projected_result['accuracy']*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n🔍 근본 원인:\")\n",
        "print(f\"Text와 Vibration 임베딩이 완전히 다른 의미 공간에 존재\")\n",
        "print(f\"- Text: 언어적 의미 공간 (단어, 문법 기반)\")\n",
        "print(f\"- Vibration: 물리적 신호 공간 (주파수, 진폭 기반)\")\n",
        "print(f\"- 결과: 두 공간 간 자연스러운 매핑이 존재하지 않음\")\n",
        "\n",
        "print(f\"\\n💡 해결책 우선순위:\")\n",
        "if projected_result['accuracy'] > 0.6:\n",
        "    print(\"1. 🎉 Cross-Modal Projection 성공 - 이 방식 적용\")\n",
        "    print(\"2. TextVibCLIP 모델에 Projection Layer 통합\")\n",
        "    print(\"3. Projection Layer 파라미터 최적화\")\n",
        "elif projected_result['accuracy'] > 0.3:\n",
        "    print(\"1. ⚠️ Projection 부분 성공 - 아키텍처 개선\")\n",
        "    print(\"2. Projection Layer 깊이/너비 조정\")\n",
        "    print(\"3. 다른 정렬 방법 병행 (Attention, CCA 등)\")\n",
        "else:\n",
        "    print(\"1. ❌ 근본적 재설계 필요\")\n",
        "    print(\"2. 사전 학습된 멀티모달 모델 활용 고려\")\n",
        "    print(\"3. 대안: 각 Encoder 독립 사용 + 앙상블\")\n",
        "\n",
        "print(f\"\\n🚀 즉시 적용 방안:\")\n",
        "print(f\"1. TextVibCLIP 모델에 Cross-Modal Projection 추가\")\n",
        "print(f\"2. Temperature 최적화: 0.05 → 0.1\")\n",
        "print(f\"3. Projection 학습률 및 아키텍처 조정\")\n",
        "\n",
        "print(f\"\\n📈 예상 성능 (Projection 적용 시):\")\n",
        "expected_performance = min(80, max(30, projected_result['accuracy'] * 100))\n",
        "print(f\"  TextVibCLIP 통합: 14.4% → {expected_performance:.0f}%\")\n",
        "print(f\"  성능 회복: {expected_performance - 14.4:.1f}%p\")\n",
        "\n",
        "if projected_result['accuracy'] > 0.4:\n",
        "    print(\"\\n🎯 다음 단계: TextVibCLIP 모델 수정\")\n",
        "    print(\"1. src/textvib_model.py에 CrossModalProjection 추가\")\n",
        "    print(\"2. Forward pass에서 Projection 적용\")\n",
        "    print(\"3. 전체 모델 재학습\")\n",
        "else:\n",
        "    print(\"\\n🔍 다음 단계: 대안 접근법 고려\")\n",
        "    print(\"1. 사전 학습된 멀티모달 모델 활용\")\n",
        "    print(\"2. 각 Encoder 독립 사용 + Late Fusion\")\n",
        "    print(\"3. Domain-specific Adapter 추가\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TVCLIP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
