{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TextVibCLIP Vibration Encoder ë‹¨ë… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "UOS ë°ì´í„°ì…‹ì—ì„œ Vibration Encoderê°€ ì§„ë™ ì‹ í˜¸ë¥¼ ì œëŒ€ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/home/kyj2024/anaconda3/envs/TVCLIP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Vibration Encoder ë‹¨ë… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/home/kyj2024/anaconda3/envs/TVCLIP/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "/data/home/kyj2024/anaconda3/envs/TVCLIP/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from collections import Counter\n",
        "import logging\n",
        "import seaborn as sns\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€\n",
        "sys.path.append('/data/home/kyj2024/TextVibCLIP')\n",
        "\n",
        "from src.vibration_encoder import VibrationEncoder\n",
        "from src.data_loader import BearingDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from src.data_loader import create_collate_fn\n",
        "from configs.model_config import MODEL_CONFIG\n",
        "\n",
        "# ë¡œê¹… ì„¤ì •\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"ğŸ”§ Vibration Encoder ë‹¨ë… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ë°ì´í„° ì¤€ë¹„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data_loader:UOS ë°ì´í„° ë¼ë²¨ ë¶„í¬: {'B_6204': 1, 'H_6204': 4, 'IR_6204': 1, 'OR_6204': 1}\n",
            "INFO:src.data_loader:ìµœì†Œ ìƒ˜í”Œ ìˆ˜: 1\n",
            "INFO:src.data_loader:UOS Domain-Incremental ìœˆë„ìš° ë ˆë²¨ ë¶„í• :\n",
            "INFO:src.data_loader:  ëª¨ë“  subsetì— ëª¨ë“  7ê°œ íŒŒì¼ í¬í•¨\n",
            "INFO:src.data_loader:  ê° íŒŒì¼ ë‚´ì—ì„œ ìœˆë„ìš° ë¶„í• : Train 60%, Val 20%, Test 20%\n",
            "INFO:src.data_loader:  Deep Groove Ball 7-í´ë˜ìŠ¤ ë¶„í¬: {'B': 1, 'H': 1, 'IR': 1, 'OR': 1, 'L': 1, 'M': 1, 'U': 1}\n",
            "INFO:src.data_loader:  í´ë˜ìŠ¤ ìˆ˜: 7ê°œ (ê· í˜• í™•ì¸)\n",
            "INFO:src.data_loader:  âœ… ì™„ë²½í•œ í´ë˜ìŠ¤ ê· í˜• ë‹¬ì„±!\n",
            "INFO:src.data_loader:UOS train ë¶„í•  ê²°ê³¼:\n",
            "INFO:src.data_loader:  Train: 7ê°œ íŒŒì¼, Val: 7ê°œ íŒŒì¼, Test: 7ê°œ íŒŒì¼\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë””ë°”ì´ìŠ¤: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data_loader:BearingDataset ì´ˆê¸°í™” ì™„ë£Œ (UOS): 7ê°œ íŒŒì¼, 1249ê°œ ìœˆë„ìš°/íŒŒì¼, ì´ 8743ê°œ ìƒ˜í”Œ, Domain: 600, Subset: train\n",
            "INFO:src.data_loader:UOS ë°ì´í„° ë¼ë²¨ ë¶„í¬: {'B_6204': 1, 'H_6204': 4, 'IR_6204': 1, 'OR_6204': 1}\n",
            "INFO:src.data_loader:ìµœì†Œ ìƒ˜í”Œ ìˆ˜: 1\n",
            "INFO:src.data_loader:UOS Domain-Incremental ìœˆë„ìš° ë ˆë²¨ ë¶„í• :\n",
            "INFO:src.data_loader:  ëª¨ë“  subsetì— ëª¨ë“  7ê°œ íŒŒì¼ í¬í•¨\n",
            "INFO:src.data_loader:  ê° íŒŒì¼ ë‚´ì—ì„œ ìœˆë„ìš° ë¶„í• : Train 60%, Val 20%, Test 20%\n",
            "INFO:src.data_loader:  Deep Groove Ball 7-í´ë˜ìŠ¤ ë¶„í¬: {'B': 1, 'H': 1, 'IR': 1, 'OR': 1, 'L': 1, 'M': 1, 'U': 1}\n",
            "INFO:src.data_loader:  í´ë˜ìŠ¤ ìˆ˜: 7ê°œ (ê· í˜• í™•ì¸)\n",
            "INFO:src.data_loader:  âœ… ì™„ë²½í•œ í´ë˜ìŠ¤ ê· í˜• ë‹¬ì„±!\n",
            "INFO:src.data_loader:UOS test ë¶„í•  ê²°ê³¼:\n",
            "INFO:src.data_loader:  Train: 7ê°œ íŒŒì¼, Val: 7ê°œ íŒŒì¼, Test: 7ê°œ íŒŒì¼\n",
            "INFO:src.data_loader:BearingDataset ì´ˆê¸°í™” ì™„ë£Œ (UOS): 7ê°œ íŒŒì¼, 1249ê°œ ìœˆë„ìš°/íŒŒì¼, ì´ 8743ê°œ ìƒ˜í”Œ, Domain: 600, Subset: test\n",
            "INFO:src.data_loader:ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„± ì™„ë£Œ: 8743ê°œ (íŒŒì¼ 7ê°œ Ã— ìœˆë„ìš° 1249ê°œ)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train ë°ì´í„°: 8743ê°œ ìƒ˜í”Œ\n",
            "Test ë°ì´í„°: 8743ê°œ ìƒ˜í”Œ\n",
            "\n",
            "ìƒ˜í”Œ êµ¬ì¡°:\n",
            "  ë¼ë²¨: tensor([6, 0])\n",
            "  ì§„ë™ ì‹ í˜¸ shape: torch.Size([2048])\n",
            "  ì§„ë™ ì‹ í˜¸ ë²”ìœ„: [-1.5622, 1.7830]\n"
          ]
        }
      ],
      "source": [
        "# UOS ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "# Train ë°ì´í„°\n",
        "train_dataset = BearingDataset(\n",
        "    data_dir='data_scenario1',\n",
        "    dataset_type='uos',\n",
        "    domain_value=600,\n",
        "    subset='train'\n",
        ")\n",
        "\n",
        "# Test ë°ì´í„°\n",
        "test_dataset = BearingDataset(\n",
        "    data_dir='data_scenario1',\n",
        "    dataset_type='uos',\n",
        "    domain_value=600,\n",
        "    subset='test'\n",
        ")\n",
        "\n",
        "print(f\"Train ë°ì´í„°: {len(train_dataset)}ê°œ ìƒ˜í”Œ\")\n",
        "print(f\"Test ë°ì´í„°: {len(test_dataset)}ê°œ ìƒ˜í”Œ\")\n",
        "\n",
        "# ìƒ˜í”Œ í™•ì¸\n",
        "sample = train_dataset[0]\n",
        "print(f\"\\nìƒ˜í”Œ êµ¬ì¡°:\")\n",
        "print(f\"  ë¼ë²¨: {sample['labels']}\")\n",
        "print(f\"  ì§„ë™ ì‹ í˜¸ shape: {sample['vibration'].shape}\")\n",
        "print(f\"  ì§„ë™ ì‹ í˜¸ ë²”ìœ„: [{sample['vibration'].min():.4f}, {sample['vibration'].max():.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Vibration Encoder ìƒì„± ë° ì§„ë™ ì„ë² ë”©\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.vibration_encoder:1D-CNN VibrationEncoder ì´ˆê¸°í™”: input_length=2048, embedding_dim=256\n",
            "INFO:src.vibration_encoder:   OPTIMIZED: ì»¤ë„ í¬ê¸°: [16, 32, 64, 32] - 4-layer ë² ì–´ë§ ìµœì í™”\n",
            "INFO:src.vibration_encoder:   OPTIMIZED: ì±„ë„ ìˆ˜: [64, 128, 256, 512] - ìì—°ìŠ¤ëŸ¬ìš´ 64â†’512 ì¦ê°€\n",
            "INFO:src.vibration_encoder:   ì´ íŒŒë¼ë¯¸í„°: 6,985,288\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vibration Encoder ì •ë³´:\n",
            "  ì…ë ¥ ê¸¸ì´: 2048\n",
            "  ì¶œë ¥ ì°¨ì›: 256\n",
            "  ì´ íŒŒë¼ë¯¸í„°: 6,985,288ê°œ\n",
            "  í•™ìŠµ íŒŒë¼ë¯¸í„°: 6,985,288ê°œ\n",
            "  ì•„í‚¤í…ì²˜: [16, 32, 64, 32] kernels, [64, 128, 256, 512] channels\n",
            "  ì‹¤ì œ ì¶œë ¥ shape: torch.Size([1, 256])\n",
            "  ì •ê·œí™” í›„ ë²”ìœ„: [-0.0825, 0.0676]\n"
          ]
        }
      ],
      "source": [
        "# Vibration Encoder ìƒì„± (ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)\n",
        "vibration_config = MODEL_CONFIG['vibration_encoder']\n",
        "embedding_dim = MODEL_CONFIG['embedding_dim']\n",
        "\n",
        "vibration_encoder = VibrationEncoder(\n",
        "    input_length=vibration_config['input_length'],\n",
        "    embedding_dim=embedding_dim\n",
        ")\n",
        "\n",
        "vibration_encoder.to(device)\n",
        "vibration_encoder.eval()\n",
        "\n",
        "# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n",
        "total_params = sum(p.numel() for p in vibration_encoder.parameters())\n",
        "trainable_params = sum(p.numel() for p in vibration_encoder.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Vibration Encoder ì •ë³´:\")\n",
        "print(f\"  ì…ë ¥ ê¸¸ì´: {vibration_config['input_length']}\")\n",
        "print(f\"  ì¶œë ¥ ì°¨ì›: {embedding_dim}\")\n",
        "print(f\"  ì´ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ\")\n",
        "print(f\"  í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable_params:,}ê°œ\")\n",
        "print(f\"  ì•„í‚¤í…ì²˜: {vibration_config['kernel_sizes']} kernels, {vibration_config['channels']} channels\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ ì¶œë ¥ ì°¨ì› í™•ì¸\n",
        "with torch.no_grad():\n",
        "    test_input = torch.randn(1, vibration_config['input_length']).to(device)\n",
        "    test_output = vibration_encoder(test_input)\n",
        "    print(f\"  ì‹¤ì œ ì¶œë ¥ shape: {test_output.shape}\")\n",
        "    print(f\"  ì •ê·œí™” í›„ ë²”ìœ„: [{test_output.min().item():.4f}, {test_output.max().item():.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ì§„ë™ ë°ì´í„° ìƒ˜í”Œë§ ë° ì„ë² ë”© ìƒì„±\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í´ë˜ìŠ¤ë³„ ì§„ë™ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...\n",
            "\n",
            "ìˆ˜ì§‘ëœ í´ë˜ìŠ¤ë³„ ì§„ë™ ë°ì´í„° ìˆ˜: {0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100}\n",
            "ì „ì²´ ì§„ë™ ë°ì´í„°: torch.Size([700, 2048])\n",
            "í´ë˜ìŠ¤ ë¶„í¬: Counter({0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100})\n",
            "ì§„ë™ ì‹ í˜¸ í†µê³„: mean=-0.0004, std=1.0121\n"
          ]
        }
      ],
      "source": [
        "# í´ë˜ìŠ¤ë³„ ì§„ë™ ë°ì´í„° ìˆ˜ì§‘ (ê° í´ë˜ìŠ¤ë‹¹ 100ê°œì”©)\n",
        "class_vibrations = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: []}  # 7ê°œ í´ë˜ìŠ¤\n",
        "class_labels = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: []}\n",
        "samples_per_class = 100\n",
        "class_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
        "\n",
        "print(\"í´ë˜ìŠ¤ë³„ ì§„ë™ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...\")\n",
        "for i in range(len(train_dataset)):\n",
        "    if all(count >= samples_per_class for count in class_counts.values()):\n",
        "        break\n",
        "        \n",
        "    sample = train_dataset[i]\n",
        "    label = sample['labels'][0].item()  # ì£¼ ë¶„ë¥˜\n",
        "    vibration = sample['vibration']\n",
        "    \n",
        "    if class_counts[label] < samples_per_class:\n",
        "        class_vibrations[label].append(vibration)\n",
        "        class_labels[label].append(label)\n",
        "        class_counts[label] += 1\n",
        "\n",
        "print(f\"\\nìˆ˜ì§‘ëœ í´ë˜ìŠ¤ë³„ ì§„ë™ ë°ì´í„° ìˆ˜: {class_counts}\")\n",
        "\n",
        "# ëª¨ë“  ì§„ë™ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í…ì„œë¡œ ê²°í•©\n",
        "all_vibrations = []\n",
        "all_labels = []\n",
        "\n",
        "for class_id, vibrations in class_vibrations.items():\n",
        "    all_vibrations.extend(vibrations)\n",
        "    all_labels.extend(class_labels[class_id])\n",
        "\n",
        "# í…ì„œë¡œ ë³€í™˜\n",
        "vibration_tensor = torch.stack(all_vibrations)  # (700, 2048)\n",
        "labels_tensor = torch.tensor(all_labels)\n",
        "\n",
        "print(f\"ì „ì²´ ì§„ë™ ë°ì´í„°: {vibration_tensor.shape}\")\n",
        "print(f\"í´ë˜ìŠ¤ ë¶„í¬: {Counter(all_labels)}\")\n",
        "print(f\"ì§„ë™ ì‹ í˜¸ í†µê³„: mean={vibration_tensor.mean():.4f}, std={vibration_tensor.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ì§„ë™ ì„ë² ë”© ìƒì„± ë° ë¶„ì„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì§„ë™ ì„ë² ë”© ìƒì„± ì¤‘...\n",
            "Vibration embeddings shape: torch.Size([700, 256])\n",
            "ì„ë² ë”© í†µê³„: mean=-0.0034, std=0.0624\n",
            "ì„ë² ë”© ë²”ìœ„: min=-0.1840, max=0.1650\n",
            "ê³ ìœ  í´ë˜ìŠ¤: [0, 1, 2, 3, 4, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "# ì§„ë™ ì„ë² ë”© ìƒì„±\n",
        "print(\"ì§„ë™ ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
        "vibration_embeddings_list = []\n",
        "batch_size = 32\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(vibration_tensor), batch_size):\n",
        "        batch_vibrations = vibration_tensor[i:i+batch_size].to(device)\n",
        "        batch_embeddings = vibration_encoder(batch_vibrations)\n",
        "        batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
        "        vibration_embeddings_list.append(batch_embeddings.cpu())\n",
        "\n",
        "vibration_embeddings = torch.cat(vibration_embeddings_list, dim=0)\n",
        "\n",
        "print(f\"Vibration embeddings shape: {vibration_embeddings.shape}\")\n",
        "print(f\"ì„ë² ë”© í†µê³„: mean={vibration_embeddings.mean().item():.4f}, std={vibration_embeddings.std().item():.4f}\")\n",
        "print(f\"ì„ë² ë”© ë²”ìœ„: min={vibration_embeddings.min().item():.4f}, max={vibration_embeddings.max().item():.4f}\")\n",
        "\n",
        "unique_classes = torch.unique(labels_tensor)\n",
        "print(f\"ê³ ìœ  í´ë˜ìŠ¤: {unique_classes.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ì§„ë™ ì‹ í˜¸ ë¶„ë¥˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (í•µì‹¬)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í´ë˜ìŠ¤ë³„ ì§„ë™ prototype ìƒì„±:\n",
            "  í´ë˜ìŠ¤ 0: 100ê°œ, ë‚´ë¶€ ìœ ì‚¬ë„ 0.9985Â±0.0004\n",
            "  í´ë˜ìŠ¤ 1: 100ê°œ, ë‚´ë¶€ ìœ ì‚¬ë„ 0.9977Â±0.0007\n",
            "  í´ë˜ìŠ¤ 2: 100ê°œ, ë‚´ë¶€ ìœ ì‚¬ë„ 0.9909Â±0.0031\n",
            "  í´ë˜ìŠ¤ 3: 100ê°œ, ë‚´ë¶€ ìœ ì‚¬ë„ 0.9968Â±0.0008\n",
            "  í´ë˜ìŠ¤ 4: 100ê°œ, ë‚´ë¶€ ìœ ì‚¬ë„ 0.9936Â±0.0025\n",
            "  í´ë˜ìŠ¤ 5: 100ê°œ, ë‚´ë¶€ ìœ ì‚¬ë„ 0.9943Â±0.0022\n",
            "  í´ë˜ìŠ¤ 6: 100ê°œ, ë‚´ë¶€ ìœ ì‚¬ë„ 0.9975Â±0.0010\n",
            "\n",
            "Prototype matrix shape: torch.Size([7, 256])\n",
            "Prototype labels: [0, 1, 2, 3, 4, 5, 6]\n",
            "\n",
            "ğŸ”§ Vibration Encoder ë¶„ë¥˜ ì •í™•ë„: 0.8857 (88.6%)\n",
            "ì´ë¡ ì  ëœë¤ ë² ì´ìŠ¤ë¼ì¸: 0.1429 (14.3%)\n",
            "ëœë¤ ëŒ€ë¹„ í–¥ìƒ: 6.20ë°°\n",
            "\n",
            "í´ë˜ìŠ¤ë³„ ì •í™•ë„:\n",
            "  í´ë˜ìŠ¤ 0 (H): 100/100 = 1.0000 (100.0%)\n",
            "  í´ë˜ìŠ¤ 1 (B): 98/100 = 0.9800 (98.0%)\n",
            "  í´ë˜ìŠ¤ 2 (IR): 66/100 = 0.6600 (66.0%)\n",
            "  í´ë˜ìŠ¤ 3 (OR): 100/100 = 1.0000 (100.0%)\n",
            "  í´ë˜ìŠ¤ 4 (L): 91/100 = 0.9100 (91.0%)\n",
            "  í´ë˜ìŠ¤ 5 (U): 95/100 = 0.9500 (95.0%)\n",
            "  í´ë˜ìŠ¤ 6 (M): 70/100 = 0.7000 (70.0%)\n",
            "\n",
            "í˜¼ë™ í–‰ë ¬:\n",
            "[[100   0   0   0   0   0   0]\n",
            " [  2  98   0   0   0   0   0]\n",
            " [  0   0  66   5  29   0   0]\n",
            " [  0   0   0 100   0   0   0]\n",
            " [  0   0   1   8  91   0   0]\n",
            " [  0   5   0   0   0  95   0]\n",
            " [ 30   0   0   0   0   0  70]]\n"
          ]
        }
      ],
      "source": [
        "# í´ë˜ìŠ¤ë³„ prototype ê³„ì‚°\n",
        "class_prototypes = []\n",
        "prototype_labels = []\n",
        "\n",
        "print(\"í´ë˜ìŠ¤ë³„ ì§„ë™ prototype ìƒì„±:\")\n",
        "for cls in unique_classes:\n",
        "    cls_mask = (labels_tensor == cls)\n",
        "    cls_embeddings = vibration_embeddings[cls_mask]\n",
        "    cls_prototype = cls_embeddings.mean(dim=0, keepdim=True)\n",
        "    \n",
        "    class_prototypes.append(cls_prototype)\n",
        "    prototype_labels.append(cls)\n",
        "    \n",
        "    # í´ë˜ìŠ¤ ë‚´ ìœ ì‚¬ë„ ë¶„ì„\n",
        "    if len(cls_embeddings) > 1:\n",
        "        intra_sim = torch.matmul(cls_embeddings, cls_embeddings.t())\n",
        "        # ëŒ€ê°ì„  ì œì™¸í•œ í‰ê·  (ìê¸° ìì‹  ì œì™¸)\n",
        "        mask = ~torch.eye(len(cls_embeddings), dtype=torch.bool)\n",
        "        intra_mean = intra_sim[mask].mean().item()\n",
        "        intra_std = intra_sim[mask].std().item()\n",
        "        print(f\"  í´ë˜ìŠ¤ {cls.item()}: {len(cls_embeddings)}ê°œ, ë‚´ë¶€ ìœ ì‚¬ë„ {intra_mean:.4f}Â±{intra_std:.4f}\")\n",
        "\n",
        "# Prototype ê²°í•©\n",
        "prototype_matrix = torch.cat(class_prototypes, dim=0)  # (7, 256)\n",
        "prototype_labels = torch.stack(prototype_labels)  # (7,)\n",
        "\n",
        "print(f\"\\nPrototype matrix shape: {prototype_matrix.shape}\")\n",
        "print(f\"Prototype labels: {prototype_labels.tolist()}\")\n",
        "\n",
        "# ğŸ¯ í•µì‹¬: ì§„ë™ ì‹ í˜¸ ë¶„ë¥˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
        "similarities = torch.matmul(vibration_embeddings, prototype_matrix.t())  # (700, 7)\n",
        "predicted_indices = torch.argmax(similarities, dim=1)  # (700,)\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì¼ì¹˜ í™•ì¸\n",
        "if predicted_indices.device != prototype_labels.device:\n",
        "    predicted_indices = predicted_indices.to(prototype_labels.device)\n",
        "\n",
        "predicted_classes = prototype_labels[predicted_indices]  # (700,)\n",
        "\n",
        "# ì •í™•ë„ ê³„ì‚°\n",
        "accuracy = (predicted_classes == labels_tensor).float().mean().item()\n",
        "print(f\"\\nğŸ”§ Vibration Encoder ë¶„ë¥˜ ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
        "\n",
        "# ì´ë¡ ì  ëœë¤ ë² ì´ìŠ¤ë¼ì¸\n",
        "random_baseline = 1.0 / len(unique_classes)\n",
        "print(f\"ì´ë¡ ì  ëœë¤ ë² ì´ìŠ¤ë¼ì¸: {random_baseline:.4f} ({random_baseline*100:.1f}%)\")\n",
        "print(f\"ëœë¤ ëŒ€ë¹„ í–¥ìƒ: {accuracy/random_baseline:.2f}ë°°\")\n",
        "\n",
        "# í´ë˜ìŠ¤ë³„ ì •í™•ë„\n",
        "print(\"\\ní´ë˜ìŠ¤ë³„ ì •í™•ë„:\")\n",
        "class_names = ['H', 'B', 'IR', 'OR', 'L', 'U', 'M']\n",
        "class_accuracies = []\n",
        "for cls in unique_classes:\n",
        "    cls_mask = (labels_tensor == cls)\n",
        "    cls_correct = (predicted_classes[cls_mask] == labels_tensor[cls_mask]).sum().item()\n",
        "    cls_total = cls_mask.sum().item()\n",
        "    cls_acc = cls_correct / cls_total if cls_total > 0 else 0\n",
        "    class_accuracies.append(cls_acc)\n",
        "    print(f\"  í´ë˜ìŠ¤ {cls.item()} ({class_names[cls.item()]}): {cls_correct}/{cls_total} = {cls_acc:.4f} ({cls_acc*100:.1f}%)\")\n",
        "\n",
        "# í˜¼ë™ í–‰ë ¬\n",
        "cm = confusion_matrix(labels_tensor.cpu(), predicted_classes.cpu())\n",
        "print(f\"\\ní˜¼ë™ í–‰ë ¬:\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ê²°ë¡  ë° ì§„ë‹¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ”§ Vibration Encoder ì„±ëŠ¥ ì§„ë‹¨ ê²°ê³¼\n",
            "============================================================\n",
            "1. ë¶„ë¥˜ ì •í™•ë„: 88.6% (ëœë¤: 14.3%)\n",
            "2. í´ë˜ìŠ¤ êµ¬ë¶„ë„: 0.0073\n",
            "3. ì„ë² ë”© í’ˆì§ˆ: std=0.0624\n",
            "4. ìµœê³  í´ë˜ìŠ¤ ì •í™•ë„: 100.0%\n",
            "5. ìµœì € í´ë˜ìŠ¤ ì •í™•ë„: 66.0%\n",
            "\n",
            "ğŸ” ì¢…í•© ì§„ë‹¨:\n",
            "âœ… Vibration Encoder ì„±ëŠ¥: ìš°ìˆ˜ (88.6%)\n",
            "ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„: Text-Vibration ì •ë ¬ ë¬¸ì œ ì§‘ì¤‘ ë¶„ì„\n",
            "\n",
            "ğŸ¯ ì„¸ë¶€ ê¶Œì¥ì‚¬í•­:\n",
            "- Vibration EncoderëŠ” ì •ìƒ ì‘ë™\n",
            "- Text-Vibration ì •ë ¬ ë¬¸ì œì— ì§‘ì¤‘\n",
            "\n",
            "ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ:\n",
            "  Text Encoder ë‹¨ë…: 83.0%\n",
            "  Vibration Encoder ë‹¨ë…: 88.6%\n",
            "  TextVibCLIP ì „ì²´: 14.4%\n",
            "ğŸš¨ ë‘ Encoder ëª¨ë‘ ì •ìƒ - ì •ë ¬/í†µí•© ê³¼ì •ì— ì‹¬ê°í•œ ë¬¸ì œ\n",
            "\n",
            "ğŸ”§ ì•„í‚¤í…ì²˜ ì •ë³´:\n",
            "  ì»¤ë„ í¬ê¸°: [16, 32, 64, 32]\n",
            "  ì±„ë„ ìˆ˜: [64, 128, 256, 512]\n",
            "  ë“œë¡­ì•„ì›ƒ: 0.1\n",
            "  ì…ë ¥â†’ì¶œë ¥: 2048 â†’ 256\n",
            "  ì´ íŒŒë¼ë¯¸í„°: 6,985,288ê°œ\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ”§ Vibration Encoder ì„±ëŠ¥ ì§„ë‹¨ ê²°ê³¼\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# í´ë˜ìŠ¤ êµ¬ë¶„ë„ ê³„ì‚°\n",
        "inter_class_sim = torch.matmul(prototype_matrix, prototype_matrix.t())\n",
        "diag_sim = torch.diag(inter_class_sim).mean().item()\n",
        "off_diag_mask = ~torch.eye(len(unique_classes), dtype=torch.bool)\n",
        "off_diag_sim = inter_class_sim[off_diag_mask].mean().item()\n",
        "separation_score = diag_sim - off_diag_sim\n",
        "\n",
        "print(f\"1. ë¶„ë¥˜ ì •í™•ë„: {accuracy*100:.1f}% (ëœë¤: {random_baseline*100:.1f}%)\")\n",
        "print(f\"2. í´ë˜ìŠ¤ êµ¬ë¶„ë„: {separation_score:.4f}\")\n",
        "print(f\"3. ì„ë² ë”© í’ˆì§ˆ: std={vibration_embeddings.std().item():.4f}\")\n",
        "print(f\"4. ìµœê³  í´ë˜ìŠ¤ ì •í™•ë„: {max(class_accuracies)*100:.1f}%\")\n",
        "print(f\"5. ìµœì € í´ë˜ìŠ¤ ì •í™•ë„: {min(class_accuracies)*100:.1f}%\")\n",
        "\n",
        "# ì¢…í•© ì§„ë‹¨\n",
        "print(f\"\\nğŸ” ì¢…í•© ì§„ë‹¨:\")\n",
        "if accuracy > 0.8:\n",
        "    diagnosis = \"ìš°ìˆ˜\"\n",
        "    next_step = \"Text-Vibration ì •ë ¬ ë¬¸ì œ ì§‘ì¤‘ ë¶„ì„\"\n",
        "    color = \"âœ…\"\n",
        "elif accuracy > 0.6:\n",
        "    diagnosis = \"ì–‘í˜¸\"\n",
        "    next_step = \"Vibration Encoder ì•„í‚¤í…ì²˜ ê°œì„ \"\n",
        "    color = \"âš ï¸\"\n",
        "elif accuracy > 0.4:\n",
        "    diagnosis = \"ë³´í†µ\"\n",
        "    next_step = \"ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ê·œí™” ê°œì„ \"\n",
        "    color = \"âš ï¸\"\n",
        "else:\n",
        "    diagnosis = \"ë¶ˆëŸ‰\"\n",
        "    next_step = \"Vibration Encoder ì•„í‚¤í…ì²˜ ì¬ì„¤ê³„\"\n",
        "    color = \"âŒ\"\n",
        "\n",
        "print(f\"{color} Vibration Encoder ì„±ëŠ¥: {diagnosis} ({accuracy*100:.1f}%)\")\n",
        "print(f\"ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„: {next_step}\")\n",
        "\n",
        "# ì„¸ë¶€ ê¶Œì¥ì‚¬í•­\n",
        "print(f\"\\nğŸ¯ ì„¸ë¶€ ê¶Œì¥ì‚¬í•­:\")\n",
        "if accuracy < 0.5:\n",
        "    print(\"- 1D-CNN ì•„í‚¤í…ì²˜ ì¬ì„¤ê³„ (ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬)\")\n",
        "    print(\"- ë°ì´í„° ì •ê·œí™” ë°©ë²• ë³€ê²½\")\n",
        "    print(\"- ìœˆë„ìš° í¬ê¸° ì¡°ì • (2048 â†’ 4096)\")\n",
        "elif accuracy < 0.7:\n",
        "    print(\"- Dropout ë¹„ìœ¨ ì¡°ì •\")\n",
        "    print(\"- ì»¤ë„ í¬ê¸° ìµœì í™”\")\n",
        "    print(\"- ë°°ì¹˜ ì •ê·œí™” ì¶”ê°€\")\n",
        "else:\n",
        "    print(\"- Vibration EncoderëŠ” ì •ìƒ ì‘ë™\")\n",
        "    print(\"- Text-Vibration ì •ë ¬ ë¬¸ì œì— ì§‘ì¤‘\")\n",
        "\n",
        "# Text Encoder vs Vibration Encoder ë¹„êµ\n",
        "text_encoder_performance = 0.83  # Text Encoder ì„±ëŠ¥\n",
        "textvib_performance = 0.1443  # TextVibCLIP ì „ì²´ ì„±ëŠ¥\n",
        "\n",
        "print(f\"\\nğŸ“ˆ ì„±ëŠ¥ ë¹„êµ:\")\n",
        "print(f\"  Text Encoder ë‹¨ë…: {text_encoder_performance*100:.1f}%\")\n",
        "print(f\"  Vibration Encoder ë‹¨ë…: {accuracy*100:.1f}%\")\n",
        "print(f\"  TextVibCLIP ì „ì²´: {textvib_performance*100:.1f}%\")\n",
        "\n",
        "if accuracy > 0.7 and text_encoder_performance > 0.7:\n",
        "    print(\"ğŸš¨ ë‘ Encoder ëª¨ë‘ ì •ìƒ - ì •ë ¬/í†µí•© ê³¼ì •ì— ì‹¬ê°í•œ ë¬¸ì œ\")\n",
        "elif accuracy < 0.5:\n",
        "    print(\"ğŸ” Vibration Encoder ê°œì„  í•„ìš”\")\n",
        "else:\n",
        "    print(\"âš ï¸ ë¶€ë¶„ì  ë¬¸ì œ - ì¶”ê°€ ë¶„ì„ í•„ìš”\")\n",
        "\n",
        "print(f\"\\nğŸ”§ ì•„í‚¤í…ì²˜ ì •ë³´:\")\n",
        "print(f\"  ì»¤ë„ í¬ê¸°: {vibration_config['kernel_sizes']}\")\n",
        "print(f\"  ì±„ë„ ìˆ˜: {vibration_config['channels']}\")\n",
        "print(f\"  ë“œë¡­ì•„ì›ƒ: {vibration_config['dropout']}\")\n",
        "print(f\"  ì…ë ¥â†’ì¶œë ¥: {vibration_config['input_length']} â†’ {embedding_dim}\")\n",
        "print(f\"  ì´ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TVCLIP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
