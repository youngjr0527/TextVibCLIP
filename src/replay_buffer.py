"""
Replay Buffer: Continual Learningì„ ìœ„í•œ ë©”ëª¨ë¦¬ ë²„í¼
ì´ì „ ë„ë©”ì¸ ì„ë² ë”© ì €ì¥ ë° ìƒ˜í”Œë§
"""

import torch
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from collections import defaultdict
import random

from configs.model_config import TRAINING_CONFIG

logger = logging.getLogger(__name__)


class ReplayBuffer:
    """
    ì„ë² ë”© ê¸°ë°˜ Replay Buffer
    
    ê° ë„ë©”ì¸ë³„ë¡œ representative embeddingsì„ ì €ì¥í•˜ê³ 
    ìƒˆ ë„ë©”ì¸ í•™ìŠµ ì‹œ ì´ì „ ì„ë² ë”©ë“¤ì„ ìƒ˜í”Œë§í•˜ì—¬ catastrophic forgetting ë°©ì§€
    """
    
    def __init__(self, 
                 buffer_size_per_domain: int = TRAINING_CONFIG['replay_buffer_size'],
                 embedding_dim: int = 256,  # 512 â†’ 256: ì„ë² ë”© ì°¨ì› ë³€ê²½ì— ë§ì¶¤
                 sampling_strategy: str = 'representative'):
        """
        Args:
            buffer_size_per_domain (int): ë„ë©”ì¸ë‹¹ ì €ì¥í•  ì„ë² ë”© ìˆ˜
            embedding_dim (int): ì„ë² ë”© ì°¨ì›
            sampling_strategy (str): ìƒ˜í”Œë§ ì „ëµ ('random', 'balanced')
        """
        self.buffer_size_per_domain = buffer_size_per_domain
        self.embedding_dim = embedding_dim
        self.sampling_strategy = sampling_strategy
        
        # ë„ë©”ì¸ë³„ ì €ì¥ì†Œ
        self.text_embeddings = {}  # {domain_id: tensor}
        self.vib_embeddings = {}   # {domain_id: tensor}
        self.metadata_list = {}    # {domain_id: list of metadata}
        
        # ë„ë©”ì¸ ì •ë³´
        self.domains = []  # ìˆœì„œëŒ€ë¡œ ì €ì¥ëœ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸
        self.current_domain = None
        
        logger.info(f"ReplayBuffer ì´ˆê¸°í™”: "
                   f"buffer_size={buffer_size_per_domain}, "
                   f"embedding_dim={embedding_dim}, "
                   f"sampling={sampling_strategy}")
    
    def add_domain_data(self, 
                       domain_id: int,
                       text_embeddings: torch.Tensor,
                       vib_embeddings: torch.Tensor,
                       metadata_list: List[Dict],
                       labels: torch.Tensor = None,
                       selection_strategy: str = 'representative') -> None:
        """
        ìƒˆ ë„ë©”ì¸ ë°ì´í„°ë¥¼ ë²„í¼ì— ì¶”ê°€
        
        Args:
            domain_id (int): ë„ë©”ì¸ ID (ì˜ˆ: RPM ê°’)
            text_embeddings (torch.Tensor): í…ìŠ¤íŠ¸ ì„ë² ë”© (N, embedding_dim)
            vib_embeddings (torch.Tensor): ì§„ë™ ì„ë² ë”© (N, embedding_dim)  
            metadata_list (List[Dict]): ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            selection_strategy (str): ì„ íƒ ì „ëµ ('random', 'diverse', 'representative')
        """
        num_samples = text_embeddings.size(0)
        
        if num_samples == 0:
            logger.warning(f"Domain {domain_id}: ë¹ˆ ë°ì´í„°, ê±´ë„ˆëœ€")
            return
        
        # ë²„í¼ í¬ê¸°ë§Œí¼ ì„ íƒ
        if num_samples <= self.buffer_size_per_domain:
            # ë°ì´í„°ê°€ ì ìœ¼ë©´ ëª¨ë‘ ì €ì¥
            selected_indices = list(range(num_samples))
        else:
            # ì„ íƒ ì „ëµì— ë”°ë¼ ìƒ˜í”Œë§
            selected_indices = self._select_representative_samples(
                text_embeddings, vib_embeddings, metadata_list, selection_strategy
            )
        
        # ì„ íƒëœ ë°ì´í„° ì €ì¥
        selected_text = text_embeddings[selected_indices]
        selected_vib = vib_embeddings[selected_indices]
        selected_metadata = [metadata_list[i] for i in selected_indices]
        
        # ğŸ¯ ë¼ë²¨ ì •ë³´ë„ ì €ì¥ (í´ë˜ìŠ¤ ê¸°ë°˜ contrastive learningìš©)
        if labels is not None:
            selected_labels = labels[selected_indices]
            if not hasattr(self, 'labels'):
                self.labels = {}
            self.labels[domain_id] = selected_labels.detach().cpu()
        
        self.text_embeddings[domain_id] = selected_text.detach().cpu()
        self.vib_embeddings[domain_id] = selected_vib.detach().cpu()
        self.metadata_list[domain_id] = selected_metadata
        
        # ë„ë©”ì¸ ëª©ë¡ ì—…ë°ì´íŠ¸
        if domain_id not in self.domains:
            self.domains.append(domain_id)
        
        self.current_domain = domain_id
        
        logger.info(f"Domain {domain_id} ë°ì´í„° ì¶”ê°€: "
                   f"{len(selected_indices)}ê°œ ìƒ˜í”Œ ì €ì¥ "
                   f"(ì „ì²´ {num_samples}ê°œ ì¤‘)")
    
    def _select_representative_samples(self,
                                     text_embeddings: torch.Tensor,
                                     vib_embeddings: torch.Tensor,
                                     metadata_list: List[Dict],
                                     strategy: str) -> List[int]:
        """
        Representative samples ì„ íƒ
        
        Args:
            text_embeddings: í…ìŠ¤íŠ¸ ì„ë² ë”©
            vib_embeddings: ì§„ë™ ì„ë² ë”©  
            metadata_list: ë©”íƒ€ë°ì´í„°
            strategy: ì„ íƒ ì „ëµ
            
        Returns:
            List[int]: ì„ íƒëœ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        num_samples = text_embeddings.size(0)
        
        if strategy == 'random':
            # ëœë¤ ì„ íƒ
            indices = random.sample(range(num_samples), self.buffer_size_per_domain)
            
        elif strategy == 'diverse':
            # K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´
            indices = self._kmeans_selection(vib_embeddings)
            
        elif strategy == 'representative':
            # í´ë˜ìŠ¤ë³„ ê· ë“± ì„ íƒ
            indices = self._balanced_class_selection(metadata_list)
            
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì„ íƒ ì „ëµ: {strategy}")
        
        return indices
    
    def _kmeans_selection(self, embeddings: torch.Tensor) -> List[int]:
        """K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ë‹¤ì–‘í•œ ìƒ˜í”Œ ì„ íƒ"""
        try:
            from sklearn.cluster import KMeans
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=self.buffer_size_per_domain, random_state=42)
            embeddings_np = embeddings.detach().cpu().numpy()
            clusters = kmeans.fit_predict(embeddings_np)
            
            # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ ìƒ˜í”Œ ì„ íƒ
            selected_indices = []
            for cluster_id in range(self.buffer_size_per_domain):
                cluster_mask = clusters == cluster_id
                if not cluster_mask.any():
                    continue
                
                cluster_indices = np.where(cluster_mask)[0]
                cluster_embeddings = embeddings_np[cluster_indices]
                cluster_center = kmeans.cluster_centers_[cluster_id]
                
                # ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ ìƒ˜í”Œ ì°¾ê¸°
                distances = np.linalg.norm(cluster_embeddings - cluster_center, axis=1)
                closest_idx = cluster_indices[np.argmin(distances)]
                selected_indices.append(closest_idx)
            
            return selected_indices
            
        except ImportError:
            logger.warning("scikit-learn ì—†ìŒ, ëœë¤ ì„ íƒìœ¼ë¡œ ëŒ€ì²´")
            return random.sample(range(embeddings.size(0)), self.buffer_size_per_domain)
    
    def _balanced_class_selection(self, metadata_list: List[Dict]) -> List[int]:
        """í´ë˜ìŠ¤ë³„ ê· ë“±í•œ ìƒ˜í”Œ ì„ íƒ"""
        # í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ ê·¸ë£¹í™”
        class_groups = defaultdict(list)
        for i, metadata in enumerate(metadata_list):
            # íšŒì „ì²´ìƒíƒœ_ë² ì–´ë§ìƒíƒœ_ë² ì–´ë§íƒ€ì…ìœ¼ë¡œ í´ë˜ìŠ¤ ì •ì˜
            class_key = f"{metadata['rotating_component']}_{metadata['bearing_condition']}_{metadata['bearing_type']}"
            class_groups[class_key].append(i)
        
        # í´ë˜ìŠ¤ë³„ ê· ë“± ì„ íƒ
        selected_indices = []
        samples_per_class = max(1, self.buffer_size_per_domain // len(class_groups))
        
        for class_key, indices in class_groups.items():
            num_select = min(samples_per_class, len(indices))
            selected = random.sample(indices, num_select)
            selected_indices.extend(selected)
        
        # ë¶€ì¡±í•˜ë©´ ëœë¤ ì¶”ê°€
        if len(selected_indices) < self.buffer_size_per_domain:
            remaining = self.buffer_size_per_domain - len(selected_indices)
            all_indices = set(range(len(metadata_list)))
            available = list(all_indices - set(selected_indices))
            
            if available:
                additional = random.sample(available, min(remaining, len(available)))
                selected_indices.extend(additional)
        
        return selected_indices[:self.buffer_size_per_domain]
    
    def sample_replay_data(self, 
                          num_samples: int,
                          exclude_current: bool = True,
                          device: torch.device = torch.device('cpu')) -> Optional[Dict[str, torch.Tensor]]:
        """
        Replay ë°ì´í„° ìƒ˜í”Œë§
        
        Args:
            num_samples (int): ìƒ˜í”Œë§í•  ë°ì´í„° ìˆ˜
            exclude_current (bool): í˜„ì¬ ë„ë©”ì¸ ì œì™¸ ì—¬ë¶€
            device (torch.device): íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤
            
        Returns:
            Dict with sampled embeddings or None if no data
        """
        # Replay bufferê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° (buffer_size_per_domain=0)
        if self.buffer_size_per_domain <= 0:
            return None
            
        available_domains = self.domains.copy()
        
        if exclude_current and self.current_domain in available_domains:
            available_domains.remove(self.current_domain)
        
        if not available_domains:
            # ì²« ë„ë©”ì¸ í•™ìŠµ ì‹œì—ëŠ” ì •ìƒì ìœ¼ë¡œ replayí•  ë„ë©”ì¸ì´ ì—†ìŒ
            if len(self.domains) == 0:
                logger.debug("ì²« ë„ë©”ì¸ í•™ìŠµ - replayí•  ë„ë©”ì¸ì´ ì—†ìŒ (ì •ìƒ)")
            else:
                # domainsê°€ dictì¸ì§€ listì¸ì§€ í™•ì¸ í›„ ì²˜ë¦¬
                if isinstance(self.domains, dict):
                    domain_keys = list(self.domains.keys())
                else:
                    domain_keys = self.domains
                logger.debug(f"Replayí•  ë„ë©”ì¸ì´ ì—†ìŒ - ì‚¬ìš© ê°€ëŠ¥: {domain_keys}")
            return None
        
        # ë„ë©”ì¸ë³„ ìƒ˜í”Œ ìˆ˜ ê²°ì •
        if self.sampling_strategy == 'balanced':
            # ë„ë©”ì¸ë³„ ê· ë“± ìƒ˜í”Œë§
            samples_per_domain = max(1, num_samples // len(available_domains))
            domain_samples = {domain: samples_per_domain for domain in available_domains}
            
            # ë‚˜ë¨¸ì§€ ìƒ˜í”Œì„ ëœë¤ ë„ë©”ì¸ì— ë°°ë¶„
            remaining = num_samples - sum(domain_samples.values())
            for _ in range(remaining):
                domain = random.choice(available_domains)
                domain_samples[domain] += 1
        else:
            # ëœë¤ ìƒ˜í”Œë§
            domain_samples = defaultdict(int)
            for _ in range(num_samples):
                domain = random.choice(available_domains)
                domain_samples[domain] += 1
        
        # ì‹¤ì œ ìƒ˜í”Œë§
        sampled_text = []
        sampled_vib = []
        sampled_metadata = []
        sampled_labels = []  # ğŸ¯ ë¼ë²¨ ì •ë³´ ì¶”ê°€
        
        for domain, num_domain_samples in domain_samples.items():
            if domain not in self.text_embeddings:
                continue
            
            domain_text = self.text_embeddings[domain]
            domain_vib = self.vib_embeddings[domain]
            domain_metadata = self.metadata_list[domain]
            domain_labels = getattr(self, 'labels', {}).get(domain, None)
            
            # í•´ë‹¹ ë„ë©”ì¸ì—ì„œ ëœë¤ ìƒ˜í”Œë§
            available_count = domain_text.size(0)
            actual_samples = min(num_domain_samples, available_count)
            
            if actual_samples > 0:
                indices = random.sample(range(available_count), actual_samples)
                
                sampled_text.append(domain_text[indices])
                sampled_vib.append(domain_vib[indices])
                sampled_metadata.extend([domain_metadata[i] for i in indices])
                
                # ğŸ¯ ë¼ë²¨ ì •ë³´ë„ ìƒ˜í”Œë§
                if domain_labels is not None:
                    sampled_labels.append(domain_labels[indices])
        
        if not sampled_text:
            logger.warning("ìƒ˜í”Œë§ëœ ë°ì´í„°ê°€ ì—†ìŒ")
            return None
        
        # í…ì„œ ê²°í•© ë° ë””ë°”ì´ìŠ¤ ì´ë™
        combined_text = torch.cat(sampled_text, dim=0).to(device)
        combined_vib = torch.cat(sampled_vib, dim=0).to(device)
        
        result = {
            'text_embeddings': combined_text,
            'vib_embeddings': combined_vib,
            'metadata': sampled_metadata
        }
        
        # ğŸ¯ ë¼ë²¨ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°ë§Œ)
        if sampled_labels:
            combined_labels = torch.cat(sampled_labels, dim=0).to(device)
            result['labels'] = combined_labels
        
        return result
    
    def get_buffer_info(self) -> Dict:
        """ë²„í¼ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        info = {
            'total_domains': len(self.domains),
            'domains': self.domains.copy(),
            'current_domain': self.current_domain,
            'domain_sizes': {}
        }
        
        for domain in self.domains:
            if domain in self.text_embeddings:
                info['domain_sizes'][domain] = self.text_embeddings[domain].size(0)
        
        return info
    
    def clear_domain(self, domain_id: int):
        """íŠ¹ì • ë„ë©”ì¸ ë°ì´í„° ì‚­ì œ"""
        if domain_id in self.text_embeddings:
            del self.text_embeddings[domain_id]
            del self.vib_embeddings[domain_id]
            del self.metadata_list[domain_id]
            
            if domain_id in self.domains:
                self.domains.remove(domain_id)
            
            logger.info(f"Domain {domain_id} ë°ì´í„° ì‚­ì œë¨")
    
    def clear_all(self):
        """ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        self.text_embeddings.clear()
        self.vib_embeddings.clear()
        self.metadata_list.clear()
        self.domains.clear()
        self.current_domain = None
        
        logger.info("ëª¨ë“  replay ë°ì´í„° ì‚­ì œë¨")
    
    def save_buffer(self, path: str):
        """ë²„í¼ ì €ì¥"""
        buffer_data = {
            'text_embeddings': self.text_embeddings,
            'vib_embeddings': self.vib_embeddings,
            'metadata_list': self.metadata_list,
            'domains': self.domains,
            'current_domain': self.current_domain,
            'config': {
                'buffer_size_per_domain': self.buffer_size_per_domain,
                'embedding_dim': self.embedding_dim,
                'sampling_strategy': self.sampling_strategy
            }
        }
        
        torch.save(buffer_data, path)
        logger.info(f"Replay buffer ì €ì¥ë¨: {path}")
    
    def load_buffer(self, path: str):
        """ë²„í¼ ë¡œë”©"""
        buffer_data = torch.load(path, map_location='cpu')
        
        self.text_embeddings = buffer_data['text_embeddings']
        self.vib_embeddings = buffer_data['vib_embeddings']
        self.metadata_list = buffer_data['metadata_list']
        self.domains = buffer_data['domains']
        self.current_domain = buffer_data['current_domain']
        
        # ì„¤ì • ë³µì›
        config = buffer_data.get('config', {})
        self.buffer_size_per_domain = config.get('buffer_size_per_domain', self.buffer_size_per_domain)
        self.embedding_dim = config.get('embedding_dim', self.embedding_dim)
        self.sampling_strategy = config.get('sampling_strategy', self.sampling_strategy)
        
        logger.info(f"Replay buffer ë¡œë”©ë¨: {path}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    print("=== ReplayBuffer í…ŒìŠ¤íŠ¸ ===")
    
    # ë²„í¼ ìƒì„±
    buffer = ReplayBuffer(buffer_size_per_domain=10, embedding_dim=256)
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    for domain_id in [600, 800, 1000]:
        num_samples = 50
        text_emb = torch.randn(num_samples, 256)
        vib_emb = torch.randn(num_samples, 256)
        metadata = [
            {
                'rotating_component': 'H',
                'bearing_condition': 'B' if i % 2 == 0 else 'H',
                'bearing_type': '6204',
                'rotating_speed': domain_id
            }
            for i in range(num_samples)
        ]
        
        # ë„ë©”ì¸ ë°ì´í„° ì¶”ê°€
        buffer.add_domain_data(domain_id, text_emb, vib_emb, metadata)
    
    # ë²„í¼ ì •ë³´ í™•ì¸
    info = buffer.get_buffer_info()
    print(f"ë²„í¼ ì •ë³´: {info}")
    
    # Replay ë°ì´í„° ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸
    replay_data = buffer.sample_replay_data(20)
    if replay_data:
        print(f"Replay í…ìŠ¤íŠ¸ ì„ë² ë”© shape: {replay_data['text_embeddings'].shape}")
        print(f"Replay ì§„ë™ ì„ë² ë”© shape: {replay_data['vib_embeddings'].shape}")
        print(f"Replay ë©”íƒ€ë°ì´í„° ìˆ˜: {len(replay_data['metadata'])}")
    
    print("\n=== ReplayBuffer í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
