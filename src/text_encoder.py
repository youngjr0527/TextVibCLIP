"""
Text Encoder: DistilBERT + LoRA êµ¬í˜„
ë² ì–´ë§ ì§„ë‹¨ ë„ë©”ì¸ì— íŠ¹í™”ëœ í…ìŠ¤íŠ¸ ì¸ì½”ë”
"""

import torch
import torch.nn as nn
from transformers import DistilBertModel, DistilBertTokenizer, DistilBertConfig
from peft import LoraConfig, get_peft_model, TaskType
from typing import Dict, List, Optional, Tuple
import logging

from configs.model_config import MODEL_CONFIG

logger = logging.getLogger(__name__)


class TextEncoder(nn.Module):
    """
    DistilBERT + LoRA ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¸ì½”ë”
    
    ë² ì–´ë§ ì§„ë‹¨ í…ìŠ¤íŠ¸ ì„¤ëª…ì„ 512ì°¨ì› ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
    """
    
    def __init__(self,
                 model_name: str = MODEL_CONFIG['text_encoder']['model_name'],
                 embedding_dim: int = MODEL_CONFIG['embedding_dim'],
                 enable_lora: bool = True,
                 freeze_base: bool = False):
        """
        Args:
            model_name (str): DistilBERT ëª¨ë¸ ì´ë¦„
            embedding_dim (int): ì¶œë ¥ ì„ë² ë”© ì°¨ì›
            enable_lora (bool): LoRA í™œì„±í™” ì—¬ë¶€
            freeze_base (bool): Base model freeze ì—¬ë¶€ (Domain 2+ì—ì„œ ì‚¬ìš©)
        """
        super(TextEncoder, self).__init__()
        
        self.model_name = model_name
        self.embedding_dim = embedding_dim
        self.enable_lora = enable_lora
        self.freeze_base = freeze_base
        
        # Tokenizer ë¡œë”©
        # í† í¬ë‚˜ì´ì €: í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê°ì²´
        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)
        
        # DistilBERT ëª¨ë¸ ë¡œë”©
        # DistilBERT: í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸
        self.distilbert = DistilBertModel.from_pretrained(model_name)
        
        # LoRA ì ìš©
        if enable_lora:
            self._apply_lora()
        
        # Base model freeze (Domain 2+ì—ì„œ ì‚¬ìš©)
        if freeze_base:
            self._freeze_base_model()
        
        # Projection layer (DistilBERT hidden_size -> embedding_dim)
        bert_hidden_size = self.distilbert.config.hidden_size  # 768 for DistilBERT
        self.projection = nn.Sequential(
            nn.Linear(bert_hidden_size, MODEL_CONFIG['projection']['hidden_dim']),
            nn.ReLU(),
            nn.Dropout(MODEL_CONFIG['projection']['dropout']),
            nn.Linear(MODEL_CONFIG['projection']['hidden_dim'], embedding_dim)
            # ğŸ¯ FIXED: LayerNorm ì œê±° (gradient vanishing ë°©ì§€)
        )
        
        logger.info(f"TextEncoder ì´ˆê¸°í™” ì™„ë£Œ: LoRA={enable_lora}, Freeze={freeze_base}")
    
    def _apply_lora(self):
        """DistilBERTì— LoRA ì ìš©"""
        try:
            lora_config = LoraConfig(
                task_type=TaskType.FEATURE_EXTRACTION,
                r=MODEL_CONFIG['text_encoder']['lora_config']['r'],
                lora_alpha=MODEL_CONFIG['text_encoder']['lora_config']['lora_alpha'],
                target_modules=MODEL_CONFIG['text_encoder']['lora_config']['target_modules'],
                lora_dropout=MODEL_CONFIG['text_encoder']['lora_config']['lora_dropout'],
                bias="none"
            )
            
            self.distilbert = get_peft_model(self.distilbert, lora_config)
            logger.info(f"LoRA ì ìš© ì™„ë£Œ: rank={lora_config.r}, alpha={lora_config.lora_alpha}")
            
        except Exception as e:
            logger.error(f"LoRA ì ìš© ì‹¤íŒ¨: {e}")
            raise e
    
    def _freeze_base_model(self):
        """Base DistilBERT íŒŒë¼ë¯¸í„° freeze (LoRA adapterëŠ” ì œì™¸)"""
        for name, param in self.distilbert.named_parameters():
            if 'lora_' not in name:  # LoRA íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ freeze
                param.requires_grad = False
        
        frozen_params = sum(1 for name, param in self.distilbert.named_parameters() if not param.requires_grad)
        logger.info(f"Base DistilBERT íŒŒë¼ë¯¸í„° freeze ì™„ë£Œ: {frozen_params}ê°œ íŒŒë¼ë¯¸í„°")
    
    def enable_lora_training(self):
        """LoRA í•™ìŠµ í™œì„±í™” (Domain 1ì—ì„œ ì‚¬ìš©)"""
        # PEFT adapter ìƒíƒœ í™•ì¸ ë° í™œì„±í™”
        if hasattr(self.distilbert, 'peft_config') and self.distilbert.peft_config:
            # adapterê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            if hasattr(self.distilbert, 'enable_adapters'):
                try:
                    self.distilbert.enable_adapters()
                    logger.info("LoRA adapter í™œì„±í™”ë¨")
                except Exception as e:
                    # ì¼ë¶€ ë°±ì—”ë“œì—ì„œ adapter registry ë¯¸êµ¬í˜„ìœ¼ë¡œ ValueError("No adapter loaded")ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
                    # íŒŒë¼ë¯¸í„° ë ˆë²¨ì—ì„œ LoRA í•™ìŠµì„ ê°•ì œ í™œì„±í™”í•˜ë¯€ë¡œ ê²½ê³ ê°€ ì•„ë‹ˆë¼ ì •ë³´ë¡œë§Œ ë‚¨ê¹€
                    logger.info(f"LoRA adapter í™œì„±í™” ì‹œë„ ê±´ë„ˆëœ€: {e}")
        else:
            logger.info("PEFT adapterê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ - LoRA íŒŒë¼ë¯¸í„° ì§ì ‘ í™œì„±í™”")
        
        # LoRA íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì • (adapter ìƒíƒœì™€ ë¬´ê´€í•˜ê²Œ ì‹¤í–‰)
        lora_params_found = 0
        for name, param in self.distilbert.named_parameters():
            if 'lora_' in name:
                param.requires_grad = True
                lora_params_found += 1
        
        logger.info(f"LoRA í•™ìŠµ í™œì„±í™”: {lora_params_found}ê°œ LoRA íŒŒë¼ë¯¸í„°")
    
    def disable_lora_training(self):
        """LoRA í•™ìŠµ ë¹„í™œì„±í™” (Domain 2+ì—ì„œ ì‚¬ìš©)"""
        # PEFT adapter ë¹„í™œì„±í™” ì‹œë„ (ì—¬ëŸ¬ ì¡°ê±´ ì²´í¬)
        adapter_disabled = False
        
        if hasattr(self.distilbert, 'disable_adapters'):
            try:
                # ì¡°ê±´ 1: peft_config í™•ì¸
                if hasattr(self.distilbert, 'peft_config') and self.distilbert.peft_config:
                    self.distilbert.disable_adapters()
                    adapter_disabled = True
                    logger.info("PEFT adapter ë¹„í™œì„±í™”ë¨ (peft_config ê²½ë¡œ)")
                
                # ì¡°ê±´ 2: active_adapters í™•ì¸
                elif hasattr(self.distilbert, 'active_adapters') and len(self.distilbert.active_adapters) > 0:
                    self.distilbert.disable_adapters()
                    adapter_disabled = True
                    logger.info("PEFT adapter ë¹„í™œì„±í™”ë¨ (active_adapters ê²½ë¡œ)")
                    
                # ì¡°ê±´ 3: adapter_nameì´ ìˆëŠ”ì§€ í™•ì¸  
                elif hasattr(self.distilbert, 'adapter_name'):
                    try:
                        self.distilbert.disable_adapters()
                        adapter_disabled = True
                        logger.info("PEFT adapter ë¹„í™œì„±í™”ë¨ (adapter_name ê²½ë¡œ)")
                    except:
                        pass
                        
            except ValueError as e:
                if "No adapter loaded" in str(e):
                    logger.info("ë¹„í™œì„±í™”í•  adapterê°€ ì—†ìŠµë‹ˆë‹¤ - ì´ë¯¸ ë¹„í™œì„±í™”ë¨")
                else:
                    logger.warning(f"Adapter ë¹„í™œì„±í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            except Exception as e:
                logger.warning(f"Adapter ë¹„í™œì„±í™” ì‹œë„ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
        
        if not adapter_disabled:
            logger.info("PEFT adapter ìƒíƒœ ë¶ˆëª… - íŒŒë¼ë¯¸í„° ë ˆë²¨ì—ì„œ ì§ì ‘ freeze")
        
        # ëª¨ë“  DistilBERT íŒŒë¼ë¯¸í„° ê°•ì œ freeze (adapter ìƒíƒœì™€ ë¬´ê´€)
        frozen_count = 0
        for param in self.distilbert.parameters():
            param.requires_grad = False
            frozen_count += 1
            
        logger.info(f"LoRA í•™ìŠµ ë¹„í™œì„±í™” ì™„ë£Œ: {frozen_count}ê°œ íŒŒë¼ë¯¸í„° freeze")
    
    def tokenize_text(self, 
                     texts: List[str], 
                     max_length: int = 128) -> Dict[str, torch.Tensor]:
        """
        í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        
        Args:
            texts (List[str]): í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            max_length (int): ìµœëŒ€ ê¸¸ì´
            
        Returns:
            Dict[str, torch.Tensor]: í† í¬ë‚˜ì´ì§•ëœ ê²°ê³¼
        """
        return self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors='pt'
        )
    
    def forward(self, 
                input_ids: torch.Tensor,
                attention_mask: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            input_ids (torch.Tensor): í† í° ID (batch_size, seq_len)
            attention_mask (torch.Tensor): ì–´í…ì…˜ ë§ˆìŠ¤í¬ (batch_size, seq_len)
            
        Returns:
            torch.Tensor: í…ìŠ¤íŠ¸ ì„ë² ë”© (batch_size, embedding_dim)
        """
        # DistilBERT ì¸ì½”ë”©
        outputs = self.distilbert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # [CLS] í† í° ì„ë² ë”© ì¶”ì¶œ (ì²« ë²ˆì§¸ í† í°)
        #  Self-Attentionìœ¼ë¡œ ëª¨ë“  í† í° ì •ë³´ê°€ [CLS]ë¡œ ëª¨ì´ê²Œ í•™ìŠµë˜ê¸° ë•Œë¬¸.
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # (batch_size, hidden_size)
        
        # Projection layerë¥¼ í†µí•´ ëª©í‘œ ì°¨ì›ìœ¼ë¡œ ë³€í™˜
        text_embedding = self.projection(cls_embedding)  # (batch_size, embedding_dim)
        
        return text_embedding
    
    def encode_texts(self, 
                    texts: List[str], 
                    device: torch.device,
                    max_length: int = 128) -> torch.Tensor:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (í¸ì˜ í•¨ìˆ˜)
        
        Args:
            texts (List[str]): í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            device (torch.device): ë””ë°”ì´ìŠ¤
            max_length (int): ìµœëŒ€ ê¸¸ì´
            
        Returns:
            torch.Tensor: í…ìŠ¤íŠ¸ ì„ë² ë”© (batch_size, embedding_dim)
        """
        # í† í¬ë‚˜ì´ì§•
        tokenized = self.tokenize_text(texts, max_length)
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        input_ids = tokenized['input_ids'].to(device)
        attention_mask = tokenized['attention_mask'].to(device)
        
        # ì¸ì½”ë”©
        # í•™ìŠµ ëª¨ë“œì—ì„œëŠ” gradientê°€ íë¥´ë„ë¡ ì„¤ì •
        with torch.set_grad_enabled(self.training):
            embeddings = self.forward(input_ids, attention_mask)
        
        return embeddings
    
    def encode_tokenized(self, 
                        input_ids: torch.Tensor,
                        attention_mask: torch.Tensor) -> torch.Tensor:
        """
        ì´ë¯¸ í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„°ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ë°°ì¹˜ íš¨ìœ¨ì„±)
        
        Args:
            input_ids (torch.Tensor): í† í° ID (batch_size, seq_len)
            attention_mask (torch.Tensor): ì–´í…ì…˜ ë§ˆìŠ¤í¬ (batch_size, seq_len)
            
        Returns:
            torch.Tensor: í…ìŠ¤íŠ¸ ì„ë² ë”© (batch_size, embedding_dim)
        """
        # í•™ìŠµ ëª¨ë“œì—ì„œëŠ” gradientê°€ íë¥´ë„ë¡ ì„¤ì •
        with torch.set_grad_enabled(self.training):
            embeddings = self.forward(input_ids, attention_mask)
        
        return embeddings
    
    def get_trainable_parameters(self) -> int:
        """í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ë°˜í™˜"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def get_lora_parameters(self) -> int:
        """LoRA íŒŒë¼ë¯¸í„° ìˆ˜ ë°˜í™˜"""
        if not self.enable_lora:
            return 0
        
        lora_params = 0
        for name, param in self.distilbert.named_parameters():
            if 'lora_' in name and param.requires_grad:
                lora_params += param.numel()
        
        return lora_params


class TextEncoderConfig:
    """TextEncoder ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod # êµ³ì´ ê°ì²´ë¥¼ ë§Œë“¤ì§€ ì•Šê³ ë„ í˜¸ì¶œ ê°€ëŠ¥í•˜ë„ë¡ @staticmethodë¡œ ì„ ì–¸
    def get_domain1_config() -> Dict:
        """Domain 1 (First Domain Training) ì„¤ì •"""
        return {
            'enable_lora': True,
            'freeze_base': True,
            'learning_rate': 1e-4,
            'weight_decay': 1e-5
        }
    
    @staticmethod  
    def get_continual_config() -> Dict:
        """Domain 2+ (Continual Learning) ì„¤ì •"""
        return {
            'enable_lora': True,
            'freeze_base': True,
            'learning_rate': 0,  # í•™ìŠµí•˜ì§€ ì•ŠìŒ
            'weight_decay': 0
        }


def create_text_encoder(domain_stage: str = 'first_domain') -> TextEncoder:
    """
    ë„ë©”ì¸ ë‹¨ê³„ì— ë”°ë¥¸ TextEncoder ìƒì„±
    
    Args:
        domain_stage (str): 'first_domain' (Domain 1) ë˜ëŠ” 'continual' (Domain 2+)
        
    Returns:
        TextEncoder: ì„¤ì •ëœ í…ìŠ¤íŠ¸ ì¸ì½”ë”
    """
    if domain_stage == 'first_domain':
        config = TextEncoderConfig.get_domain1_config()
    elif domain_stage == 'continual':
        config = TextEncoderConfig.get_continual_config()
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” domain_stage: {domain_stage}")
    
    encoder = TextEncoder(
        enable_lora=config['enable_lora'],
        freeze_base=config['freeze_base']
    )
    
    logger.info(f"TextEncoder ìƒì„± ({domain_stage}): "
               f"Total={encoder.get_trainable_parameters():,}, "
               f"LoRA={encoder.get_lora_parameters():,}")
    
    return encoder


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    print("=== TextEncoder í…ŒìŠ¤íŠ¸ ===")
    
    # Domain 1 (First Domain Training) ì¸ì½”ë” í…ŒìŠ¤íŠ¸
    print("\n1. Domain 1 (First Domain Training) ì¸ì½”ë”")
    encoder_domain1 = create_text_encoder('first_domain')
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_texts = [
        "A deep groove ball bearing operating at 600 rpm with healthy rotating component and ball fault.",
        "A tapered roller bearing operating at 800 rpm with healthy rotating component and healthy bearing.",
        "A cylindrical roller bearing operating at 1000 rpm with unbalanced rotating component and inner race fault."
    ]
    
    # ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    encoder_domain1.to(device)
    
    embeddings = encoder_domain1.encode_texts(test_texts, device)
    print(f"ì„ë² ë”© shape: {embeddings.shape}")
    print(f"ì„ë² ë”© norm: {torch.norm(embeddings, dim=1)}")
    
    # Domain 2+ (Continual Learning) ì¸ì½”ë” í…ŒìŠ¤íŠ¸
    print("\n2. Domain 2+ (Continual Learning) ì¸ì½”ë”")
    encoder_continual = create_text_encoder('continual')
    encoder_continual.to(device)
    
    embeddings_continual = encoder_continual.encode_texts(test_texts, device)
    print(f"ì„ë² ë”© shape: {embeddings_continual.shape}")
    
    # íŒŒë¼ë¯¸í„° ë¹„êµ
    print(f"\nDomain 1 í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {encoder_domain1.get_trainable_parameters():,}")
    print(f"Domain 2+ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {encoder_continual.get_trainable_parameters():,}")
    
    print("\n=== TextEncoder í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
