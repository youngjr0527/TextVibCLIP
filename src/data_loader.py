"""
UOS ë°ì´í„°ì…‹ ë¡œë”
data_scenario1 í´ë” ê¸°ë°˜ìœ¼ë¡œ ì§„ë™ ì‹ í˜¸ì™€ í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë”©
"""

import os
import glob
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional, Union
from sklearn.model_selection import train_test_split
import logging
import warnings
from transformers import DistilBertTokenizer

# Warning ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning)
try:
    import torchvision
    torchvision.disable_beta_transforms_warning()
except:
    pass

from .utils import (
    parse_filename, 
    generate_text_description, 
    load_mat_file,
    create_windowed_signal,
    normalize_signal,
    create_labels
)
from configs.model_config import DATA_CONFIG, CWRU_DATA_CONFIG

# ë¡œê¹… ì„¤ì • (ë©”ì¸ì—ì„œ êµ¬ì„±ë˜ë¯€ë¡œ basicConfig ì œê±°)
logger = logging.getLogger(__name__)


def create_collate_fn(tokenizer: DistilBertTokenizer = None, max_length: int = 128):
    """
    ë°°ì¹˜ í† í¬ë‚˜ì´ì§•ì„ ìœ„í•œ collate_fn ìƒì„±
    
    Args:
        tokenizer: DistilBERT í† í¬ë‚˜ì´ì €
        max_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´
        
    Returns:
        collate_fn: DataLoaderìš© collate í•¨ìˆ˜
    """
    if tokenizer is None:
        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    def collate_fn(batch):
        """
        ë°°ì¹˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì§•
        
        Args:
            batch: UOSDatasetì—ì„œ ë°˜í™˜ëœ ìƒ˜í”Œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ë°°ì¹˜ ë°ì´í„°
        """
        # ê° í•„ë“œë³„ë¡œ ë°ì´í„° ë¶„ë¦¬
        vibrations = torch.stack([item['vibration'] for item in batch])
        texts = [item['text'] for item in batch]
        metadata_list = [item['metadata'] for item in batch]
        labels = torch.stack([item['labels'] for item in batch])
        rpms = torch.tensor([item['domain_key'] for item in batch])
        file_indices = torch.tensor([item['file_idx'] for item in batch])
        
        # ë°°ì¹˜ í† í¬ë‚˜ì´ì§• (ë” íš¨ìœ¨ì )
        tokenized = tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors='pt'
        )
        
        return {
            'vibration': vibrations,
            'text': texts,  # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ìœ ì§€ (ë¡œê¹…ìš©)
            'input_ids': tokenized['input_ids'],
            'attention_mask': tokenized['attention_mask'],
            'metadata': metadata_list,
            'labels': labels,
            'rpm': rpms,
            'file_idx': file_indices
        }
    
    return collate_fn


class BearingDataset(Dataset):
    """
    ë² ì–´ë§ ë°ì´í„°ì…‹ PyTorch Dataset í´ë˜ìŠ¤ (UOS/CWRU ì§€ì›)
    
    ì§„ë™ ì‹ í˜¸ì™€ í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°ë¥¼ ìŒìœ¼ë¡œ ë¡œë”©
    """
    
    def __init__(self, 
                 data_dir: str = DATA_CONFIG['data_dir'],
                 dataset_type: str = 'uos',
                 domain_value: Optional[Union[int, str]] = None,
                 subset: str = 'train',
                 window_size: int = None,
                 overlap_ratio: float = None,
                 normalization: str = None,
                 max_text_length: int = DATA_CONFIG['max_text_length']):
        """
        Args:
            data_dir (str): ë°ì´í„° í´ë” ê²½ë¡œ (data_scenario1 ë˜ëŠ” data_scenario2)
            dataset_type (str): 'uos' ë˜ëŠ” 'cwru'
            domain_value (Union[int, str], optional): 
                - UOS: RPM ê°’ (600, 800, etc.)
                - CWRU: Load ê°’ (0, 1, 2, 3)
            subset (str): 'train', 'val', 'test' ì¤‘ í•˜ë‚˜
            window_size (int): ì‹ í˜¸ ìœˆë„ìš° í¬ê¸°
            overlap_ratio (float): ìœˆë„ìš° ê²¹ì¹¨ ë¹„ìœ¨
            normalization (str): ì‹ í˜¸ ì •ê·œí™” ë°©ë²•
            max_text_length (int): í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´
        """
        self.data_dir = data_dir
        self.dataset_type = dataset_type.lower()
        self.domain_value = domain_value
        self.max_text_length = max_text_length
        self.subset = subset
        
        # ğŸ¯ CRITICAL FIX: ë°ì´í„°ì…‹ë³„ ì„¤ì • ì ìš©
        if self.dataset_type == 'cwru':
            config = CWRU_DATA_CONFIG
        else:
            config = DATA_CONFIG
            
        self.window_size = window_size if window_size is not None else config['window_size']
        self.overlap_ratio = overlap_ratio if overlap_ratio is not None else config['overlap_ratio']
        self.normalization = normalization if normalization is not None else config['signal_normalization']
        
        # ë°ì´í„° ë¡œë”©
        self.file_paths = self._collect_file_paths()
        self.metadata_list = self._extract_metadata()
        
        # ë°ì´í„°ì…‹ ë¶„í•  (train/val/test)
        self.file_paths, self.metadata_list = self._split_dataset()
        
        # ê° íŒŒì¼ì˜ ìœˆë„ìš° ìˆ˜ ê³„ì‚° (ë¶„í•  ì •ë³´ ê³ ë ¤)
        self.windows_per_file = self._calculate_windows_per_file()
        self.total_windows = len(self.file_paths) * self.windows_per_file
        
        logger.info(f"BearingDataset ì´ˆê¸°í™” ì™„ë£Œ ({self.dataset_type.upper()}): "
                   f"{len(self.file_paths)}ê°œ íŒŒì¼, {self.windows_per_file}ê°œ ìœˆë„ìš°/íŒŒì¼, "
                   f"ì´ {self.total_windows}ê°œ ìƒ˜í”Œ, Domain: {domain_value}, Subset: {subset}")


    def _collect_file_paths(self) -> List[str]:
        """ë°ì´í„° íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘"""
        if self.dataset_type == 'uos':
            return self._collect_uos_file_paths()
        elif self.dataset_type == 'cwru':
            return self._collect_cwru_file_paths()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹ íƒ€ì…: {self.dataset_type}")
    
    def _collect_uos_file_paths(self) -> List[str]:
        """UOS ë°ì´í„° íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘ (Deep Groove Ballë§Œ)"""
        if self.domain_value is not None:
            # ğŸ¯ SIMPLIFIED: Deep Groove Ball (6204)ë§Œ ë¡œë”©
            pattern = os.path.join(self.data_dir, f"RotatingSpeed_{self.domain_value}", "*.mat")
        else:
            # ëª¨ë“  RPMì˜ Deep Groove Ball ë¡œë”©
            pattern = os.path.join(self.data_dir, "RotatingSpeed_*", "*.mat")
        
        file_paths = glob.glob(pattern, recursive=False)  # recursive=False (ë‹¨ì¼ ë ˆë²¨)
        
        if len(file_paths) == 0:
            raise ValueError(f"UOS íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern}")
        
        return sorted(file_paths)
    
    def _collect_cwru_file_paths(self) -> List[str]:
        """CWRU ë°ì´í„° íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘"""
        if self.domain_value is not None:
            # íŠ¹ì • Loadë§Œ ë¡œë”©
            if isinstance(self.domain_value, str):
                load_folder = self.domain_value  # 'Load_0hp' í˜•íƒœ
            else:
                load_folder = f"Load_{self.domain_value}hp"  # ìˆ«ì -> 'Load_Xhp' í˜•íƒœ
            
            pattern = os.path.join(self.data_dir, load_folder, "*.mat")
        else:
            # ëª¨ë“  Load ë¡œë”©
            pattern = os.path.join(self.data_dir, "**", "*.mat")
        
        file_paths = glob.glob(pattern, recursive=True)
        
        if len(file_paths) == 0:
            raise ValueError(f"CWRU íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern}")
        
        return sorted(file_paths)
    
    def _extract_metadata(self) -> List[Dict[str, Union[str, int]]]:
        """íŒŒì¼ëª…ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata_list = []
        
        for filepath in self.file_paths:
            try:
                metadata = parse_filename(filepath, dataset_type=self.dataset_type)
                metadata['filepath'] = filepath
                metadata_list.append(metadata)
            except Exception as e:
                logger.warning(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {filepath}, ì˜¤ë¥˜: {e}")
                continue
        
        return metadata_list
    
    def _generate_labels(self, metadata: Dict[str, Union[str, int]]) -> torch.Tensor:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ë¼ë²¨ ìƒì„±"""
        if self.dataset_type == 'uos':
            return self._generate_uos_labels(metadata)
        elif self.dataset_type == 'cwru':
            return self._generate_cwru_labels(metadata)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹ íƒ€ì…: {self.dataset_type}")
    
    def _generate_uos_labels(self, metadata: Dict[str, Union[str, int]]) -> torch.Tensor:
        """ğŸ¯ UOS ë¼ë²¨ ìƒì„± (7-í´ë˜ìŠ¤ ê²°í•© ë¶„ë¥˜)"""
        # ì˜¬ë°”ë¥¸ 7-í´ë˜ìŠ¤ ë¶„ë¥˜: íšŒì „ì²´+ë² ì–´ë§ ìƒíƒœ ì¡°í•©
        rotating_comp = metadata['rotating_component']
        bearing_cond = metadata['bearing_condition']
        
        # ê²°í•© ë¼ë²¨ ìƒì„±
        combined_condition = f"{rotating_comp}_{bearing_cond}"
        
        # 7-í´ë˜ìŠ¤ ë§¤í•‘
        condition_map = {
            'H_H': 0,   # Healthy (ì™„ì „ ì •ìƒ)
            'H_B': 1,   # Ball fault (ë² ì–´ë§ ë³¼ ê²°í•¨)
            'H_IR': 2,  # Inner race fault (ë² ì–´ë§ ë‚´ë¥œ ê²°í•¨)
            'H_OR': 3,  # Outer race fault (ë² ì–´ë§ ì™¸ë¥œ ê²°í•¨)
            'L_H': 4,   # Looseness (íšŒì „ì²´ ëŠìŠ¨í•¨)
            'U_H': 5,   # Unbalance (íšŒì „ì²´ ë¶ˆê· í˜•)
            'M_H': 6    # Misalignment (íšŒì „ì²´ ì •ë ¬ë¶ˆëŸ‰)
        }
        
        # ì¶”ê°€ ì •ë³´: ë² ì–´ë§ íƒ€ì…ë„ ìœ ì§€
        bearing_type_map = {'6204': 0, '30204': 1, 'N204': 2, 'NJ204': 3}
        
        # ë‹¨ì¼ ë¼ë²¨ (ì£¼ ë¶„ë¥˜) + ë² ì–´ë§ íƒ€ì… ì •ë³´
        main_label = condition_map.get(combined_condition, 0)
        bearing_type_label = bearing_type_map.get(metadata['bearing_type'], 0)
        
        labels = torch.tensor([
            main_label,         # ì£¼ ë¶„ë¥˜ (7-í´ë˜ìŠ¤)
            bearing_type_label  # ë² ì–´ë§ íƒ€ì… (4-í´ë˜ìŠ¤)
        ], dtype=torch.long)
        
        return labels
    
    def _generate_cwru_labels(self, metadata: Dict[str, Union[str, int]]) -> torch.Tensor:
        """CWRU ë¼ë²¨ ìƒì„± (1ì°¨ì›: bearing_condition)"""
        bearing_condition_map = {'Normal': 0, 'B': 1, 'IR': 2, 'OR': 3}
        
        # CWRUëŠ” ë² ì–´ë§ ìƒíƒœë§Œ ë¶„ë¥˜í•˜ë¯€ë¡œ 1ì°¨ì› ë¼ë²¨
        label = torch.tensor([
            bearing_condition_map[metadata['bearing_condition']]
        ], dtype=torch.long)
        
        return label
    
    def _get_domain_key(self, metadata: Dict[str, Union[str, int]]) -> Union[int, str]:
        """ë„ë©”ì¸ í‚¤ ë°˜í™˜"""
        if self.dataset_type == 'uos':
            return metadata['rotating_speed']  # RPM ê°’
        elif self.dataset_type == 'cwru':
            return metadata['load']  # Load ê°’ (0, 1, 2, 3)
        else:
            return 0
    
    def _calculate_windows_per_file(self) -> int:
        """ê° íŒŒì¼ë‹¹ ìœˆë„ìš° ìˆ˜ ê³„ì‚°"""
        if len(self.file_paths) == 0:
            return 0
        
        try:
            # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ ìœˆë„ìš° ìˆ˜ ì¶”ì •
            first_file = self.file_paths[0]
            signal = load_mat_file(first_file, dataset_type=self.dataset_type)
            
            # íƒ€ì… ì•ˆì „ì„± í™•ë³´
            window_size = int(self.window_size)
            overlap_ratio = float(self.overlap_ratio)
            
            windowed_signals = create_windowed_signal(
                signal, window_size, overlap_ratio
            )
            
            total_windows = len(windowed_signals)
            
            # ğŸ¯ CWRU ìœˆë„ìš° ë¶„í•  ê³ ë ¤
            if self.dataset_type == 'cwru' and hasattr(self, '_window_split_range'):
                start_ratio, end_ratio = self._window_split_range
                split_windows = int(total_windows * (end_ratio - start_ratio))
                return max(1, split_windows)  # ìµœì†Œ 1ê°œ ìœˆë„ìš°
            
            return total_windows
        except Exception as e:
            logger.warning(f"ìœˆë„ìš° ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ 1 ì‚¬ìš©")
            return 1
    
    def _split_dataset(self) -> Tuple[List[str], List[Dict]]:
        """ë°ì´í„°ì…‹ì„ train/val/testë¡œ ë¶„í• """
        if len(self.file_paths) == 0:
            return [], []
        
        # CWRUëŠ” ë°ì´í„°ê°€ ì ì–´ì„œ split ì—†ì´ ì „ì²´ ë°ì´í„° ì‚¬ìš©
        if self.dataset_type == 'cwru':
            return self._split_cwru_dataset()
        else:
            return self._split_uos_dataset()
    
    def _split_cwru_dataset(self) -> Tuple[List[str], List[Dict]]:
        """CWRU ë°ì´í„°ì…‹ ë¶„í•  (ê°œì„ ëœ ì „ëµ)"""
        # CWRU íŠ¹ì„±: ë„ë©”ì¸ë‹¹ 4ê°œ íŒŒì¼ (Normal, B, IR, OR)
        # ì—°êµ¬ ëª©ì ì— ë§ëŠ” ë¶„í•  ì „ëµ ì ìš©
        
        # ë² ì–´ë§ ìƒíƒœë³„ ë¼ë²¨ (Normal, B, IR, OR)
        bearing_labels = [metadata['bearing_condition'] for metadata in self.metadata_list]
        
        from collections import Counter
        label_counts = Counter(bearing_labels)
        logger.info(f"CWRU ë°ì´í„° ë¼ë²¨ ë¶„í¬: {dict(label_counts)}")
        
        # CWRU íŠ¹ë³„ ì²˜ë¦¬: íŒŒì¼ ìˆ˜ê°€ ì ìœ¼ë¯€ë¡œ ì ì‘ì  ë¶„í• 
        total_files = len(self.file_paths)
        
        if total_files <= 4:
            # ë„ë©”ì¸ë‹¹ 4ê°œ íŒŒì¼ì¸ ê²½ìš° - ì—°êµ¬ ëª©ì ì— ë§ê²Œ ë¶„í• 
            # íŒŒì¼ ìˆœì„œ: [Normal, B, IR, OR] (ì•ŒíŒŒë²³ìˆœ)
            
            # ğŸ¯ CRITICAL FIX: CWRUë„ íŒŒì¼ ë ˆë²¨ ë¶„í• ë¡œ ìˆ˜ì • (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
            # CWRU: 4ê°œ íŒŒì¼ [Normal, B, IR, OR] - ê° í´ë˜ìŠ¤ 1ê°œì”©
            # ëª¨ë“  í´ë˜ìŠ¤ë¥¼ í¬í•¨í•˜ë©´ì„œë„ íŒŒì¼ ë ˆë²¨ì—ì„œ ë¶„í• 
            
            # ğŸ¯ CRITICAL FIX: CWRU ì „ì²´ ìœˆë„ìš° ì‚¬ìš© (ë°ì´í„° ë¶€ì¡± í•´ê²°)
            # CWRUëŠ” ë°ì´í„°ê°€ ì ìœ¼ë¯€ë¡œ ìœˆë„ìš° ë¶„í•  ì—†ì´ ì „ì²´ ì‚¬ìš©
            
            logger.info("CWRU Domain-Incremental ì „ì²´ ìœˆë„ìš° ì‚¬ìš©:")
            logger.info(f"  ëª¨ë“  subsetì— ëª¨ë“  {total_files}ê°œ íŒŒì¼ì˜ ì „ì²´ ìœˆë„ìš° í¬í•¨")
            logger.info(f"  ìœˆë„ìš° ë¶„í•  ì—†ìŒ (ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ìµœëŒ€ í™œìš©)")
            
            # ëª¨ë“  íŒŒì¼ì„ ëª¨ë“  subsetì— í¬í•¨
            selected_files = self.file_paths
            selected_meta = self.metadata_list
            
            # ğŸ¯ ìœˆë„ìš° ë¶„í•  ì •ë³´ ì œê±° (ì „ì²´ ìœˆë„ìš° ì‚¬ìš©)
            # self._window_split_range ì„¤ì •í•˜ì§€ ì•ŠìŒ â†’ ì „ì²´ ìœˆë„ìš° ì‚¬ìš©
            
            # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸ (ëª¨ë“  í´ë˜ìŠ¤ê°€ ëª¨ë“  subsetì— í¬í•¨ë¨)
            from collections import Counter
            all_classes = [meta['bearing_condition'] for meta in self.metadata_list]
            unique_classes = list(Counter(all_classes).keys())
            
            logger.info(f"  ëª¨ë“  subset í´ë˜ìŠ¤: {unique_classes} ({len(unique_classes)}ê°œ)")
            logger.info(f"  í´ë˜ìŠ¤ë³„ íŒŒì¼ ìˆ˜: {dict(Counter(all_classes))}")
            
            logger.info(f"CWRU {self.subset} subset: {len(selected_files)}ê°œ íŒŒì¼ ì‚¬ìš© (ì´ {total_files}ê°œ ì¤‘)")
            
            # ì‹¤ì œ ì„ íƒëœ íŒŒì¼ëª… ë¡œê¹… (ë””ë²„ê¹…ìš©)
            file_names = [os.path.basename(f) for f in selected_files]
            logger.info(f"CWRU {self.subset} íŒŒì¼ë“¤: {file_names}")
            
            return selected_files, selected_meta
            
        else:
            # íŒŒì¼ì´ ë§ì€ ê²½ìš° (ì—¬ëŸ¬ ë„ë©”ì¸ í†µí•© ë“±) - í‘œì¤€ ë¶„í•  ì ìš©
            try:
                # ë² ì–´ë§ ìƒíƒœë¡œ stratified split ì‹œë„
                files_train, files_temp, meta_train, meta_temp = train_test_split(
                    self.file_paths, self.metadata_list,
                    test_size=0.4,  # 40%ë¥¼ test+valìš©ìœ¼ë¡œ
                    stratify=bearing_labels,
                    random_state=42
                )
                
                # Tempë¥¼ val/testë¡œ ë¶„í• 
                temp_labels = [metadata['bearing_condition'] for metadata in meta_temp]
                files_val, files_test, meta_val, meta_test = train_test_split(
                    files_temp, meta_temp,
                    test_size=0.5,  # tempì˜ 50%ì”© val/testë¡œ
                    stratify=temp_labels,
                    random_state=42
                )
                
                logger.info("CWRU stratified split ì„±ê³µ")
                
            except ValueError:
                # Stratify ì‹¤íŒ¨ ì‹œ ëœë¤ ë¶„í• 
                logger.warning("CWRU stratified split ì‹¤íŒ¨ - ëœë¤ ë¶„í•  ì‚¬ìš©")
                files_train, files_temp, meta_train, meta_temp = train_test_split(
                    self.file_paths, self.metadata_list,
                    test_size=0.4,
                    random_state=42
                )
                
                files_val, files_test, meta_val, meta_test = train_test_split(
                    files_temp, meta_temp,
                    test_size=0.5,
                    random_state=42
                )
            
            # ìš”ì²­ëœ subset ë°˜í™˜
            if self.subset == 'train':
                return files_train, meta_train
            elif self.subset == 'val':
                return files_val, meta_val
            elif self.subset == 'test':
                return files_test, meta_test
            else:
                raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” subset: {self.subset}")
    
    def _split_uos_dataset(self) -> Tuple[List[str], List[Dict]]:
        """UOS ë°ì´í„°ì…‹ ë¶„í•  (ê°œì„ ëœ stratified split)"""
        # ì—°êµ¬ ëª©ì ì— ë§ëŠ” stratification ì „ëµ:
        # ë² ì–´ë§ ìƒíƒœ(bearing_condition)ë¥¼ ì£¼ìš” í´ë˜ìŠ¤ë¡œ ì‚¬ìš©
        # - ì§„ë‹¨ ëª¨ë¸ì˜ í•µì‹¬ì€ ë² ì–´ë§ ê²°í•¨ íƒ€ì… ë¶„ë¥˜
        # - H/B/IR/OR 4ê°œ í´ë˜ìŠ¤ë¡œ ê· í˜• ìœ ì§€
        
        # ì£¼ìš” ë¼ë²¨: ë² ì–´ë§ ìƒíƒœ (H/B/IR/OR)
        primary_labels = [metadata['bearing_condition'] for metadata in self.metadata_list]
        
        # ë³´ì¡° ë¼ë²¨: ë² ì–´ë§ íƒ€ì… (stratification ë³´ê°•ìš©)
        secondary_labels = [metadata['bearing_type'] for metadata in self.metadata_list]
        
        # ë³µí•© ë¼ë²¨ ìƒì„± (ë² ì–´ë§ ìƒíƒœ + ë² ì–´ë§ íƒ€ì…)
        # ì˜ˆ: H_DeepGrooveBall, IR_TaperedRoller ë“±
        combined_labels = [f"{primary}_{secondary}" for primary, secondary in zip(primary_labels, secondary_labels)]
        
        # ë¼ë²¨ ë¶„í¬ í™•ì¸
        from collections import Counter
        label_counts = Counter(combined_labels)
        min_samples = min(label_counts.values())
        
        logger.info(f"UOS ë°ì´í„° ë¼ë²¨ ë¶„í¬: {dict(label_counts)}")
        logger.info(f"ìµœì†Œ ìƒ˜í”Œ ìˆ˜: {min_samples}")
        
        # ğŸ¯ CRITICAL FIX: íŒŒì¼ ë ˆë²¨ ë¶„í• ë¡œ ë³µì› (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
        # ìœˆë„ìš° ë ˆë²¨ ë¶„í• ì€ ê°™ì€ ë² ì–´ë§ì˜ ì—°ì† ì‹ í˜¸ë¥¼ train/val/testë¡œ ë¶„í• í•˜ì—¬ 
        # ì‹¬ê°í•œ ë°ì´í„° ëˆ„ìˆ˜ë¥¼ ì•¼ê¸°í•¨ â†’ 100% ì •í™•ë„ì˜ ì›ì¸
        
        # ğŸ¯ CRITICAL FIX: ìœˆë„ìš° ë ˆë²¨ ë¶„í•  (UOS ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ)
        # UOS/CWRUëŠ” í´ë˜ìŠ¤ë³„ë¡œ 1ê°œ íŒŒì¼ë§Œ ì¡´ì¬í•˜ë¯€ë¡œ, ìœˆë„ìš° ë ˆë²¨ì—ì„œ ë¶„í• í•´ì•¼ í•¨
        # ëª¨ë“  íŒŒì¼ì„ ëª¨ë“  subsetì— í¬í•¨í•˜ë˜, ê° íŒŒì¼ ë‚´ì—ì„œ ìœˆë„ìš°ë¥¼ ë¶„í• 
        
        logger.info("UOS Domain-Incremental ìœˆë„ìš° ë ˆë²¨ ë¶„í• :")
        logger.info(f"  ëª¨ë“  subsetì— ëª¨ë“  {len(self.file_paths)}ê°œ íŒŒì¼ í¬í•¨")
        logger.info(f"  ê° íŒŒì¼ ë‚´ì—ì„œ ìœˆë„ìš° ë¶„í• : Train 60%, Val 20%, Test 20%")
        
        # ëª¨ë“  íŒŒì¼ì„ ëª¨ë“  subsetì— í¬í•¨
        files_train = self.file_paths
        files_val = self.file_paths
        files_test = self.file_paths
        meta_train = self.metadata_list
        meta_val = self.metadata_list
        meta_test = self.metadata_list
        
        # ìœˆë„ìš° ë¶„í•  ì •ë³´ ì„¤ì • (ë‚˜ì¤‘ì— __getitem__ì—ì„œ ì‚¬ìš©)
        if self.subset == 'train':
            self._window_split_range = (0.0, 0.6)  # ê° íŒŒì¼ì˜ ì²˜ìŒ 60%
        elif self.subset == 'val':
            self._window_split_range = (0.6, 0.8)  # ê° íŒŒì¼ì˜ 60-80%
        elif self.subset == 'test':
            self._window_split_range = (0.8, 1.0)  # ê° íŒŒì¼ì˜ 80-100%
        
        # ğŸ¯ FIXED: Deep Groove Ball 7-í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        from collections import Counter
        
        # ì‹¤ì œ ë¼ë²¨ ìƒì„±í•˜ì—¬ ë¶„í¬ í™•ì¸
        actual_labels = []
        for meta in self.metadata_list:
            rotating_comp = meta['rotating_component']
            bearing_cond = meta['bearing_condition']
            combined_condition = f"{rotating_comp}_{bearing_cond}"
            
            condition_map = {
                'H_H': 'H',   'H_B': 'B',   'H_IR': 'IR', 'H_OR': 'OR',
                'L_H': 'L',   'U_H': 'U',   'M_H': 'M'
            }
            actual_class = condition_map.get(combined_condition, 'Unknown')
            actual_labels.append(actual_class)
        
        class_distribution = Counter(actual_labels)
        unique_classes = list(class_distribution.keys())
        
        logger.info(f"  Deep Groove Ball 7-í´ë˜ìŠ¤ ë¶„í¬: {dict(class_distribution)}")
        logger.info(f"  í´ë˜ìŠ¤ ìˆ˜: {len(unique_classes)}ê°œ (ê· í˜• í™•ì¸)")
        
        # í´ë˜ìŠ¤ ê· í˜• ê²€ì¦
        counts = list(class_distribution.values())
        if counts and max(counts) == min(counts):
            logger.info("  âœ… ì™„ë²½í•œ í´ë˜ìŠ¤ ê· í˜• ë‹¬ì„±!")
        else:
            logger.warning(f"  âš ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•: ìµœëŒ€ {max(counts) if counts else 0}ê°œ, ìµœì†Œ {min(counts) if counts else 0}ê°œ")
        
        # ë¶„í•  ê²°ê³¼ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        logger.info(f"UOS {self.subset} ë¶„í•  ê²°ê³¼:")
        logger.info(f"  Train: {len(files_train)}ê°œ íŒŒì¼, Val: {len(files_val)}ê°œ íŒŒì¼, Test: {len(files_test)}ê°œ íŒŒì¼")
        
        # ìš”ì²­ëœ subset ë°˜í™˜ (íŒŒì¼ ë ˆë²¨ ë¶„í• )
        if self.subset == 'train':
            return files_train, meta_train
        elif self.subset == 'val':
            return files_val, meta_val
        elif self.subset == 'test':
            return files_test, meta_test
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” subset: {self.subset}")
    
    def __len__(self) -> int:
        return self.total_windows
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        íŒŒì¼ ì¸ë±ìŠ¤ì™€ ìœˆë„ìš° ì¸ë±ìŠ¤ë¥¼ ì¡°í•©í•´ì„œ ìƒ˜í”Œ ë°˜í™˜
        
        Args:
            idx: ì „ì²´ ìœˆë„ìš° ì¸ë±ìŠ¤ (0 ~ total_windows-1)
            
        Returns:
            Dict containing:
                - 'vibration': ì§„ë™ ì‹ í˜¸ (window_size,)
                - 'text': í…ìŠ¤íŠ¸ ì„¤ëª… (str)
                - 'metadata': ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
                - 'labels': ë¼ë²¨
                - 'domain_key': ë„ë©”ì¸ í‚¤
        """
        # ğŸ¯ CRITICAL FIX: í´ë˜ìŠ¤ ê· í˜•ì„ ìœ„í•œ ì¸ë±ìŠ¤ ë¦¬ë§¤í•‘
        # ê¸°ì¡´ ë°©ì‹ì€ ì—°ì†ëœ ì¸ë±ìŠ¤ê°€ ê°™ì€ íŒŒì¼ì—ì„œ ë‚˜ì™€ì„œ ë°°ì¹˜ ë‚´ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì•¼ê¸°
        
        if not hasattr(self, '_index_mapping'):
            # ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„± (í•œ ë²ˆë§Œ)
            total_files = len(self.file_paths)
            mapping = []
            
            # ëª¨ë“  (íŒŒì¼, ìœˆë„ìš°) ìŒ ìƒì„±
            for file_idx in range(total_files):
                for window_idx in range(self.windows_per_file):
                    mapping.append((file_idx, window_idx))
            
            # ì…”í”Œí•˜ì—¬ í´ë˜ìŠ¤ ê· í˜• í™•ë³´
            import random
            random.seed(42)  # ì¬í˜„ ê°€ëŠ¥ì„±
            random.shuffle(mapping)
            
            self._index_mapping = mapping
            logger.info(f"ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„± ì™„ë£Œ: {len(mapping)}ê°œ (íŒŒì¼ {total_files}ê°œ Ã— ìœˆë„ìš° {self.windows_per_file}ê°œ)")
        
        # ì…”í”Œëœ ë§¤í•‘ì—ì„œ (íŒŒì¼, ìœˆë„ìš°) ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        if idx < len(self._index_mapping):
            file_idx, window_idx = self._index_mapping[idx]
        else:
            # ë²”ìœ„ ì´ˆê³¼ ì‹œ ë§ˆì§€ë§‰ ë§¤í•‘ ì‚¬ìš©
            file_idx, window_idx = self._index_mapping[-1]
        
        # íŒŒì¼ ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ë§ˆì§€ë§‰ íŒŒì¼ ì‚¬ìš©
        if file_idx >= len(self.file_paths):
            file_idx = len(self.file_paths) - 1
            window_idx = 0
        
        filepath = self.file_paths[file_idx]
        metadata = self.metadata_list[file_idx]
        
        try:
            # ğŸ¯ CRITICAL FIX: íŒŒì¼ë³„ ìœˆë„ìš° ìºì‹± (ë§¤ìš° ì¤‘ìš”í•œ ìµœì í™”)
            # ê°™ì€ íŒŒì¼ì˜ ìœˆë„ìš°ë“¤ì„ ë°˜ë³µ ìš”ì²­í•  ë•Œ ë§¤ë²ˆ ë¡œë”©í•˜ì§€ ì•Šë„ë¡ ìºì‹±
            if not hasattr(self, '_file_windows_cache'):
                self._file_windows_cache = {}
            
            if file_idx not in self._file_windows_cache:
                # ì§„ë™ ì‹ í˜¸ ë¡œë”© (íŒŒì¼ë‹¹ 1íšŒë§Œ)
                signal = load_mat_file(filepath, dataset_type=self.dataset_type)
                
                # ì‹ í˜¸ ì „ì²˜ë¦¬ (íŒŒì¼ë‹¹ 1íšŒë§Œ)
                signal = normalize_signal(signal, method=self.normalization)
                
                # ìœˆë„ì‰ (íŒŒì¼ë‹¹ 1íšŒë§Œ)
                windowed_signals = create_windowed_signal(
                    signal, self.window_size, self.overlap_ratio
                )
                
                # ìºì‹œì— ì €ì¥
                self._file_windows_cache[file_idx] = windowed_signals
                logger.debug(f"íŒŒì¼ ìºì‹œ ìƒì„±: {os.path.basename(filepath)} ({len(windowed_signals)}ê°œ ìœˆë„ìš°)")
            else:
                # ìºì‹œì—ì„œ ë¡œë“œ
                windowed_signals = self._file_windows_cache[file_idx]
            
            # ğŸ¯ CRITICAL FIX: ìœˆë„ìš° ë ˆë²¨ ë¶„í•  ë³µì› (UOS ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ)
            # UOS/CWRUëŠ” í´ë˜ìŠ¤ë³„ë¡œ 1ê°œ íŒŒì¼ë§Œ ìˆìœ¼ë¯€ë¡œ ìœˆë„ìš° ë ˆë²¨ ë¶„í• ì´ í•„ìš”
            if hasattr(self, '_window_split_range'):
                total_windows = len(windowed_signals)
                start_ratio, end_ratio = self._window_split_range
                start_idx = int(total_windows * start_ratio)
                end_idx = int(total_windows * end_ratio)
                
                # ë²”ìœ„ ë‚´ì—ì„œ ìœˆë„ìš° ì„ íƒ
                valid_range = end_idx - start_idx
                if valid_range > 0:
                    adjusted_window_idx = start_idx + (window_idx % valid_range)
                else:
                    adjusted_window_idx = start_idx
                
                if adjusted_window_idx < len(windowed_signals):
                    selected_signal = windowed_signals[adjusted_window_idx]
                else:
                    selected_signal = windowed_signals[-1]
            else:
                # ê¸°ë³¸ ë¡œì§ (fallback)
                if window_idx < len(windowed_signals):
                    selected_signal = windowed_signals[window_idx]
                else:
                    selected_signal = windowed_signals[-1]
            
            # í…ìŠ¤íŠ¸ ì„¤ëª… ìƒì„±
            text_description = generate_text_description(metadata)
            
            # ë¼ë²¨ ìƒì„±
            labels = self._generate_labels(metadata)
            
            # ë„ë©”ì¸ ê°’ ì„¤ì •
            domain_key = self._get_domain_key(metadata)
            
            return {
                'vibration': torch.tensor(selected_signal, dtype=torch.float32),
                'text': text_description,
                'metadata': metadata,
                'labels': labels,
                'domain_key': domain_key,
                'file_idx': file_idx,
                'window_idx': window_idx
            }
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {filepath}, window {window_idx}, ì˜¤ë¥˜: {e}")
            # ì—ëŸ¬ ì‹œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            dummy_labels = torch.zeros(3 if self.dataset_type == 'uos' else 1, dtype=torch.long)
            dummy_domain_key = metadata.get('rotating_speed' if self.dataset_type == 'uos' else 'load', 0)
            
            return {
                'vibration': torch.zeros(self.window_size, dtype=torch.float32),
                'text': "Error loading data",
                'metadata': metadata,
                'labels': dummy_labels,
                'domain_key': dummy_domain_key,
                'file_idx': file_idx,
                'window_idx': window_idx
            }


# UOSDatasetì€ í•˜ìœ„í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
class UOSDataset(BearingDataset):
    """UOS ë°ì´í„°ì…‹ (í•˜ìœ„í˜¸í™˜ì„±)"""
    def __init__(self, 
                 data_dir: str = DATA_CONFIG['data_dir'],
                 domain_rpm: Optional[int] = None,
                 **kwargs):
        super().__init__(
            data_dir=data_dir,
            dataset_type='uos',
            domain_value=domain_rpm,
            **kwargs
        )
        # í•˜ìœ„í˜¸í™˜ì„±ì„ ìœ„í•œ ì†ì„±
        self.domain_rpm = domain_rpm


def create_domain_dataloaders(data_dir: str = DATA_CONFIG['data_dir'],
                            domain_order: List[int] = DATA_CONFIG['domain_order'],
                            dataset_type: str = 'uos',
                            batch_size: int = 32,
                            num_workers: int = 4,
                            use_collate_fn: bool = True) -> Dict[int, Dict[str, DataLoader]]:
    """
    ë„ë©”ì¸ë³„ DataLoader ìƒì„± (UOS/CWRU ì§€ì›)
    
    Args:
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬
        domain_order (List[int]): ë„ë©”ì¸ ìˆœì„œ (UOS: RPM, CWRU: Load)
        dataset_type (str): 'uos' ë˜ëŠ” 'cwru'
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        num_workers (int): ì›Œì»¤ ìˆ˜
        use_collate_fn (bool): ë°°ì¹˜ í† í¬ë‚˜ì´ì§• ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        Dict[int, Dict[str, DataLoader]]: {domain: {'train': DataLoader, 'val': DataLoader, 'test': DataLoader}}
    """
    domain_dataloaders = {}
    
    # collate_fn ì¤€ë¹„
    collate_fn = create_collate_fn() if use_collate_fn else None
    
    for domain_value in domain_order:
        domain_name = f"{domain_value}RPM" if dataset_type == 'uos' else f"{domain_value}HP"
        logger.info(f"Domain {domain_name} ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
        
        # ê° subsetë³„ Dataset ìƒì„±
        train_dataset = BearingDataset(
            data_dir=data_dir,
            dataset_type=dataset_type,
            domain_value=domain_value,
            subset='train'
        )
        
        val_dataset = BearingDataset(
            data_dir=data_dir,
            dataset_type=dataset_type,
            domain_value=domain_value,
            subset='val'
        )
        
        test_dataset = BearingDataset(
            data_dir=data_dir,
            dataset_type=dataset_type,
            domain_value=domain_value,
            subset='test'
        )
        
        # DataLoader ìƒì„±
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=collate_fn
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=collate_fn
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=collate_fn
        )
        
        domain_dataloaders[domain_value] = {
            'train': train_loader,
            'val': val_loader,
            'test': test_loader
        }
        
        logger.info(f"Domain {domain_name}: Train {len(train_dataset)}, "
                   f"Val {len(val_dataset)}, Test {len(test_dataset)}")
    
    return domain_dataloaders


def create_combined_dataloader(data_dir: str = DATA_CONFIG['data_dir'],
                             subset: str = 'train',
                             batch_size: int = 32,
                             num_workers: int = 4,
                             use_collate_fn: bool = True) -> DataLoader:
    """
    ëª¨ë“  ë„ë©”ì¸ì„ í•©ì¹œ DataLoader ìƒì„± (First Domain Trainingìš©)
    
    Args:
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬
        subset (str): 'train', 'val', 'test' ì¤‘ í•˜ë‚˜
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        num_workers (int): ì›Œì»¤ ìˆ˜
        use_collate_fn (bool): ë°°ì¹˜ í† í¬ë‚˜ì´ì§• ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        DataLoader: ëª¨ë“  ë„ë©”ì¸ í•©ì¹œ DataLoader
    """
    # ëª¨ë“  RPMì„ í¬í•¨í•˜ëŠ” Dataset ìƒì„±
    dataset = UOSDataset(
        data_dir=data_dir,
        domain_rpm=None,  # ëª¨ë“  RPM í¬í•¨
        subset=subset
    )
    
    # collate_fn ì¤€ë¹„
    collate_fn = create_collate_fn() if use_collate_fn else None
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(subset == 'train'),
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn
    )
    
    logger.info(f"Combined DataLoader ìƒì„±: {subset} subset, {len(dataset)}ê°œ ìƒ˜í”Œ")
    
    return dataloader


def create_first_domain_dataloader(data_dir: str = DATA_CONFIG['data_dir'],
                                  domain_order: List[int] = DATA_CONFIG['domain_order'],
                                  dataset_type: str = 'uos',
                                  subset: str = 'train',
                                  batch_size: int = 32,
                                  num_workers: int = 4,
                                  use_collate_fn: bool = True) -> DataLoader:
    """
    ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œì˜ DataLoader ìƒì„± (First Domain Trainingìš©)
    
    Args:
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬
        domain_order (List[int]): RPM ìˆœì„œ (ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©)
        subset (str): 'train', 'val', 'test' ì¤‘ í•˜ë‚˜
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        num_workers (int): ì›Œì»¤ ìˆ˜
        use_collate_fn (bool): ë°°ì¹˜ í† í¬ë‚˜ì´ì§• ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        DataLoader: ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œì˜ DataLoader
    """
    # ì²« ë²ˆì§¸ ë„ë©”ì¸ ê°’
    first_domain_value = domain_order[0]
    
    # ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œì˜ Dataset ìƒì„±
    dataset = BearingDataset(
        data_dir=data_dir,
        dataset_type=dataset_type,
        domain_value=first_domain_value,
        subset=subset
    )
    
    # collate_fn ì¤€ë¹„
    collate_fn = create_collate_fn() if use_collate_fn else None
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(subset == 'train'),
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn
    )
    
    domain_name = f"{first_domain_value}RPM" if dataset_type == 'uos' else f"{first_domain_value}HP"
    logger.info(f"First Domain DataLoader ìƒì„±: Domain {domain_name}, "
               f"{subset} subset, {len(dataset)}ê°œ ìƒ˜í”Œ")
    
    return dataloader


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logger.info("UOS ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ë‹¨ì¼ ë„ë©”ì¸ í…ŒìŠ¤íŠ¸
    dataset = UOSDataset(domain_rpm=600, subset='train')
    print(f"Domain 600 RPM Train Dataset: {len(dataset)}ê°œ ìƒ˜í”Œ")
    
    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
    sample = dataset[0]
    print(f"Vibration shape: {sample['vibration'].shape}")
    print(f"Text: {sample['text']}")
    print(f"Labels: {sample['labels']}")
    print(f"RPM: {sample['rpm']}")
    
    # ë„ë©”ì¸ë³„ DataLoader í…ŒìŠ¤íŠ¸
    domain_loaders = create_domain_dataloaders(batch_size=8)
    print(f"ìƒì„±ëœ ë„ë©”ì¸ ìˆ˜: {len(domain_loaders)}")
    
    # ì²« ë²ˆì§¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
    first_domain = list(domain_loaders.keys())[0]
    first_batch = next(iter(domain_loaders[first_domain]['train']))
    print(f"First batch keys: {first_batch.keys()}")
    print(f"Batch vibration shape: {first_batch['vibration'].shape}")
    print(f"Batch text length: {len(first_batch['text'])}")
    
    logger.info("UOS ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
