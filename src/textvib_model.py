"""
TextVibCLIP v2: Ranking-based ë©€í‹°ëª¨ë‹¬ ë² ì–´ë§ ì§„ë‹¨ ëª¨ë¸
InfoNCE ëŒ€ì‹  Triplet/Margin Ranking Loss ì‚¬ìš©ìœ¼ë¡œ ì†Œê·œëª¨ ë°ì´í„°ì— ìµœì í™”

í•µì‹¬ ì•„ì´ë””ì–´:
1. ê° ì¸ì½”ë”ê°€ ë…ë¦½ì ìœ¼ë¡œ ë¶„ë¥˜ í•™ìŠµ (ì•ˆì •ì )
2. ê°„ë‹¨í•œ ì •ë ¬ í•™ìŠµ (MSE ê¸°ë°˜)
3. ì‹¤ì œ ì‚¬ìš©: ì§„ë™ ì‹ í˜¸ â†’ í›„ë³´ í…ìŠ¤íŠ¸ ì¤‘ ìµœê³  ìœ ì‚¬ë„ ì„ íƒ
4. Continual learning: ì§„ë™ ìœ„ì£¼ ì ì‘
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Union
import logging

from .text_encoder import create_text_encoder
from .vibration_encoder import create_vibration_encoder
from configs.model_config import MODEL_CONFIG, FIRST_DOMAIN_CONFIG, CONTINUAL_CONFIG

logger = logging.getLogger(__name__)


class RankingLoss(nn.Module):
    """
    Ranking-based Loss for Text-Vibration Alignment
    
    í•µì‹¬: ê°™ì€ í´ë˜ìŠ¤ì˜ text-vibëŠ” ê°€ê¹ê²Œ, ë‹¤ë¥¸ í´ë˜ìŠ¤ëŠ” ë©€ê²Œ
    InfoNCEë³´ë‹¤ ì†Œê·œëª¨ ë°ì´í„°ì— ì í•©
    """
    
    def __init__(self, margin: float = 0.2, loss_type: str = 'triplet'):
        """
        Args:
            margin: Ranking margin
            loss_type: 'triplet' ë˜ëŠ” 'margin_ranking'
        """
        super().__init__()
        self.margin = margin
        self.loss_type = loss_type
        logger.info(f"RankingLoss ì´ˆê¸°í™”: margin={margin}, type={loss_type}")
    
    def forward(self, text_embeddings: torch.Tensor, 
                vib_embeddings: torch.Tensor,
                labels: torch.Tensor) -> torch.Tensor:
        """
        Ranking loss ê³„ì‚°
        
        Args:
            text_embeddings: (batch_size, embed_dim)
            vib_embeddings: (batch_size, embed_dim)  
            labels: (batch_size,) í´ë˜ìŠ¤ ë¼ë²¨
            
        Returns:
            torch.Tensor: ranking loss
        """
        batch_size = text_embeddings.size(0)
        
        if self.loss_type == 'triplet':
            return self._triplet_loss(text_embeddings, vib_embeddings, labels)
        else:
            return self._margin_ranking_loss(text_embeddings, vib_embeddings, labels)
    
    def _triplet_loss(self, text_emb: torch.Tensor, vib_emb: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """Triplet Loss: anchor-positive-negative"""
        # ëª¨ë“  ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = torch.matmul(vib_emb, text_emb.t())  # (B, B)
        
        # ê°™ì€ í´ë˜ìŠ¤ ë§ˆìŠ¤í¬
        same_class = (labels.unsqueeze(1) == labels.unsqueeze(0))
        
        losses = []
        for i in range(similarities.size(0)):
            # Positive: ê°™ì€ í´ë˜ìŠ¤ (ìê¸° ì œì™¸)
            positive_mask = same_class[i] & (torch.arange(similarities.size(1), device=similarities.device) != i)
            # Negative: ë‹¤ë¥¸ í´ë˜ìŠ¤
            negative_mask = ~same_class[i]
            
            if positive_mask.any() and negative_mask.any():
                # ê°€ì¥ ê°€ê¹Œìš´ positiveì™€ ê°€ì¥ ê°€ê¹Œìš´ negative
                pos_sim = similarities[i][positive_mask].max()
                neg_sim = similarities[i][negative_mask].max()
                
                # Triplet loss: positiveê°€ negativeë³´ë‹¤ marginë§Œí¼ ë” ë†’ì•„ì•¼ í•¨
                loss = F.relu(self.margin - pos_sim + neg_sim)
                losses.append(loss)
        
        return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=similarities.device)
    
    def _margin_ranking_loss(self, text_emb: torch.Tensor, vib_emb: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """Margin Ranking Loss (ë” ê°„ë‹¨í•œ ë²„ì „)"""
        similarities = torch.matmul(vib_emb, text_emb.t())
        
        # ì •ë‹µ ìœ„ì¹˜ (ëŒ€ê°ì„ )
        correct_sim = torch.diag(similarities)
        
        # ì˜¤ë‹µë“¤ê³¼ì˜ ì°¨ì´
        batch_size = similarities.size(0)
        losses = []
        
        for i in range(batch_size):
            correct = correct_sim[i]
            incorrect = similarities[i]
            incorrect = torch.cat([incorrect[:i], incorrect[i+1:]])  # ìê¸° ì œì™¸
            
            # ëª¨ë“  ì˜¤ë‹µë³´ë‹¤ marginë§Œí¼ ë†’ì•„ì•¼ í•¨
            margin_losses = F.relu(self.margin - correct + incorrect)
            losses.append(margin_losses.mean())
        
        return torch.stack(losses).mean()


class TextVibCLIP(nn.Module):
    """
    TextVibCLIP v2: Ranking-based ì•„í‚¤í…ì²˜
    
    InfoNCE ëŒ€ì‹  Triplet/Ranking Loss ì‚¬ìš©
    ì‹¤ì œ ì‚¬ìš©: ì§„ë™ ì‹ í˜¸ â†’ í›„ë³´ í…ìŠ¤íŠ¸ ì¤‘ ìµœê³  ìœ ì‚¬ë„ ì„ íƒ
    """
    
    def __init__(self,
                 domain_stage: str = 'first_domain',
                 embedding_dim: int = 256,
                 dataset_type: str = 'uos'):
        super().__init__()
        
        self.domain_stage = domain_stage
        self.embedding_dim = embedding_dim
        self.dataset_type = dataset_type.lower()
        self.is_continual_mode = (domain_stage == 'continual')
        
        # ê¸°ì¡´ ì¸ì½”ë”ë“¤ ì¬í™œìš©
        self.text_encoder = create_text_encoder(domain_stage)
        self.vib_encoder = create_vibration_encoder()
        
        # ğŸ¯ í•µì‹¬: ê³µí†µ ì„ë² ë”© ê³µê°„ìœ¼ë¡œì˜ projection
        self.text_projection = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim, embedding_dim),
            nn.LayerNorm(embedding_dim)
        )
        
        self.vib_projection = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),
            nn.ReLU(), 
            nn.Dropout(0.1),
            nn.Linear(embedding_dim, embedding_dim),
            nn.LayerNorm(embedding_dim)
        )
        
        # ğŸ¯ Ranking Loss (InfoNCE ëŒ€ì‹ )
        self.ranking_loss = RankingLoss(margin=0.3, loss_type='triplet')
        
        # ğŸ¯ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ (ì¶”ë¡  ì‹œ ì‚¬ìš©)
        self.ensemble_weight = nn.Parameter(torch.tensor(0.7))  # ì§„ë™ ìœ„ì£¼
        
        # ğŸ¯ ë³´ì¡° ë¶„ë¥˜ í—¤ë“œ (ë°ì´í„°ì…‹ë³„ í´ë˜ìŠ¤ ìˆ˜)
        aux_cfg = MODEL_CONFIG.get('aux_classification', {})
        self.use_aux = aux_cfg.get('enabled', True)
        if self.use_aux:
            # ë°ì´í„°ì…‹ë³„ í´ë˜ìŠ¤ ìˆ˜ ì„¤ì •
            if self.dataset_type == 'cwru':
                num_classes = 4  # CWRU: H, B, IR, OR
            else:
                num_classes = 7  # UOS: H, B, IR, OR, L, U, M
            
            # ë°ì´í„°ì…‹ë³„ ì°¨ë³„í™”ëœ ë¶„ë¥˜ê¸° êµ¬ì¡°
            if self.dataset_type == 'cwru':
                # CWRU: ë§¤ìš° ê°•í•œ ì •ê·œí™”
                dropout_rate = 0.7
                hidden_dim = embedding_dim // 4  # ë” ì‘ì€ hidden
            else:
                # UOS: í‘œì¤€ ì •ê·œí™”
                dropout_rate = 0.2
                hidden_dim = embedding_dim // 2
            
            self.text_classifier = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(embedding_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout_rate * 0.5),
                nn.Linear(hidden_dim, num_classes)
            )
            
            self.vib_classifier = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(embedding_dim, hidden_dim), 
                nn.ReLU(),
                nn.Dropout(dropout_rate * 0.5),
                nn.Linear(hidden_dim, num_classes)
            )
        
        logger.info(f"TextVibCLIP v2 ì´ˆê¸°í™” ì™„ë£Œ: {domain_stage} stage")
    
    def forward(self, 
                batch: Dict[str, Union[torch.Tensor, List[str]]],
                return_embeddings: bool = False) -> Dict[str, torch.Tensor]:
        """
        Forward pass
        
        Args:
            batch: ë°°ì¹˜ ë°ì´í„°
            return_embeddings: ì„ë² ë”© ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            Dict with loss and optionally embeddings
        """
        device = batch['vibration'].device
        
        # ì¸ì½”ë”©
        # CWRUëŠ” í‰ê°€ ì‹œ ê³ ì • í”„ë¡¬í”„íŠ¸(HP ë¬´ê´€) ê¸°ë°˜ retrievalì„ ì‚¬ìš©í•˜ë¯€ë¡œ,
        # í•™ìŠµë„ ë™ì¼í•œ í´ë˜ìŠ¤ í”„ë¡¬í”„íŠ¸(HP ì ‘ë¯¸ì‚¬ ë¯¸í¬í•¨)ë¡œ ì •ë ¬í•œë‹¤.
        # ë¼ë²¨ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ëŠ” í•™ìŠµì‹œì—ë§Œ ì‚¬ìš©(í‰ê°€ ì‹œ ë¼ë²¨ ëˆ„ìˆ˜ ë°©ì§€)
        use_prompt_training = (self.dataset_type == 'cwru') and self.training and ('labels' in batch)
        if use_prompt_training:
            # ë¼ë²¨ì—ì„œ í´ë˜ìŠ¤ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì˜ë¬¸, ë™ì˜ì–´ í…œí”Œë¦¿ ì¤‘ ëŒ€í‘œ ë¬¸êµ¬)
            # bearing_condition_map ì—­ë§¤í•‘
            # 0:H, 1:B, 2:IR, 3:OR
            label_tensor = batch['labels']
            if label_tensor.dim() == 2:
                label_tensor = label_tensor[:, 0]

            prompt_map = {
                0: "healthy bearing",
                1: "bearing with ball fault",
                2: "bearing inner race fault",
                3: "bearing outer race fault"
            }
            prompt_texts = [prompt_map.get(int(c.item()), "healthy bearing") for c in label_tensor]
            text_raw = self.text_encoder.encode_texts(prompt_texts, device)
        else:
            if 'input_ids' in batch and 'attention_mask' in batch:
                text_raw = self.text_encoder.encode_tokenized(
                    batch['input_ids'].to(device), 
                    batch['attention_mask'].to(device)
                )
            else:
                text_raw = self.text_encoder.encode_texts(batch['text'], device)
        
        vib_raw = self.vib_encoder(batch['vibration'])
        
        # ê³µí†µ ì„ë² ë”© ê³µê°„ìœ¼ë¡œ projection
        text_emb = F.normalize(self.text_projection(text_raw), p=2, dim=1)
        vib_emb = F.normalize(self.vib_projection(vib_raw), p=2, dim=1)
        
        # ë¼ë²¨ ì²˜ë¦¬
        labels = batch.get('labels', None)
        if labels is not None:
            if labels.dim() == 2:
                class_labels = labels[:, 0]  # UOS ì£¼ ë¶„ë¥˜, CWRU ì²« ë²ˆì§¸
            else:
                class_labels = labels
        else:
            # ë¼ë²¨ ì—†ìœ¼ë©´ ëŒ€ê°ì„  ë§¤ì¹­ ê°€ì •
            class_labels = torch.arange(text_emb.size(0), device=device)
        
        # ğŸ¯ í•µì‹¬: Ranking Loss (InfoNCE ëŒ€ì‹ )
        ranking_loss = self.ranking_loss(text_emb, vib_emb, class_labels)
        
        total_loss = ranking_loss
        loss_components = {'ranking': ranking_loss}
        
        # ğŸ¯ ë³´ì¡° ë¶„ë¥˜ ì†ì‹¤ (ê°„ë‹¨í•œ ë²„ì „)
        if self.use_aux and labels is not None:
            # ë„ë©”ì¸ë³„ ì°¨ë³„í™”ëœ ê°€ì¤‘ì¹˜
            if self.is_continual_mode:
                aux_weight = CONTINUAL_CONFIG.get('aux_weight', 2.0)
            else:
                aux_weight = FIRST_DOMAIN_CONFIG.get('aux_weight', 5.0)
            
            # í…ìŠ¤íŠ¸ ë¶„ë¥˜ (ê°€ì¤‘ì¹˜ ì—†ìŒ - ê°„ë‹¨í•˜ê²Œ)
            text_logits = self.text_classifier(text_raw)
            text_ce = F.cross_entropy(text_logits, class_labels)
            
            # ì§„ë™ ë¶„ë¥˜ (ê°€ì¤‘ì¹˜ ì—†ìŒ - ê°„ë‹¨í•˜ê²Œ)
            vib_logits = self.vib_classifier(vib_raw)
            vib_ce = F.cross_entropy(vib_logits, class_labels)
            
            aux_loss = (text_ce + vib_ce) / 2.0
            total_loss += aux_weight * aux_loss
            
            loss_components['aux_text'] = text_ce
            loss_components['aux_vib'] = vib_ce
            loss_components['aux_total'] = aux_loss
            loss_components['aux_weight'] = aux_weight
        
        # ê²°ê³¼ êµ¬ì„±
        results = {
            'loss': total_loss,
            'loss_components': loss_components
        }
        
        if return_embeddings:
            results.update({
                'text_embeddings': text_emb,
                'vib_embeddings': vib_emb,
                'text_raw': text_raw,
                'vib_raw': vib_raw
            })
        
        return results

    def predict_best_match(self, 
                          vibration_signal: torch.Tensor,
                          candidate_texts: List[str],
                          device: torch.device) -> Tuple[int, float]:
        """
        ì‹¤ì œ ì‚¬ìš©: ì§„ë™ ì‹ í˜¸ì— ê°€ì¥ ë§ëŠ” í…ìŠ¤íŠ¸ ì°¾ê¸°
        
        Args:
            vibration_signal: (1, signal_length) ë˜ëŠ” (signal_length,)
            candidate_texts: í›„ë³´ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            device: ë””ë°”ì´ìŠ¤
            
        Returns:
            Tuple[int, float]: (best_text_index, confidence_score)
        """
        self.eval()
        
        with torch.no_grad():
            # ì§„ë™ ì‹ í˜¸ ì²˜ë¦¬
            if vibration_signal.dim() == 1:
                vibration_signal = vibration_signal.unsqueeze(0)
            vibration_signal = vibration_signal.to(device)
            
            # ì„ë² ë”© ìƒì„±
            vib_raw = self.vib_encoder(vibration_signal)
            vib_emb = F.normalize(self.vib_projection(vib_raw), p=2, dim=1)
            
            text_raw = self.text_encoder.encode_texts(candidate_texts, device)
            text_emb = F.normalize(self.text_projection(text_raw), p=2, dim=1)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = torch.matmul(vib_emb, text_emb.t())  # (1, N)
            
            # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ ì„ íƒ
            best_idx = torch.argmax(similarities, dim=1).item()
            confidence = similarities.max().item()
            
            return best_idx, confidence
    
    def encode_vibration(self, vibration_signals: torch.Tensor) -> torch.Tensor:
        """ì§„ë™ ì‹ í˜¸ë§Œ ì¸ì½”ë”© (ì¶”ë¡ ìš©)"""
        vib_raw = self.vib_encoder(vibration_signals)
        return F.normalize(self.vib_projection(vib_raw), p=2, dim=1)
    
    def encode_texts(self, texts: List[str], device: torch.device) -> torch.Tensor:
        """í…ìŠ¤íŠ¸ë§Œ ì¸ì½”ë”© (ì¶”ë¡ ìš©)"""
        text_raw = self.text_encoder.encode_texts(texts, device)
        return F.normalize(self.text_projection(text_raw), p=2, dim=1)
    
    def switch_to_continual_mode(self):
        """Continual learning ëª¨ë“œë¡œ ì „í™˜"""
        self.is_continual_mode = True
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë” freeze (LoRAë§Œ)
        self.text_encoder.disable_lora_training()
        
        # Projection layerëŠ” í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ìœ ì§€
        if hasattr(self.text_encoder, 'projection'):
            for param in self.text_encoder.projection.parameters():
                param.requires_grad = True
        
        logger.info("Continual learning ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ (í…ìŠ¤íŠ¸ ì•ˆì •í™”, ì§„ë™ ì ì‘)")
    
    def switch_to_first_domain_mode(self):
        """First domain training ëª¨ë“œë¡œ ì „í™˜"""
        self.is_continual_mode = False
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë” LoRA í™œì„±í™”
        self.text_encoder.enable_lora_training()
        
        logger.info("First domain training ëª¨ë“œë¡œ ì „í™˜ ì™„ë£Œ (ì „ì²´ í•™ìŠµ)")
    
    def get_trainable_parameters(self) -> Dict[str, int]:
        """ê° ì»´í¬ë„ŒíŠ¸ë³„ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ë°˜í™˜"""
        text_params = sum(p.numel() for p in self.text_encoder.parameters() if p.requires_grad)
        vib_params = sum(p.numel() for p in self.vib_encoder.parameters() if p.requires_grad)
        proj_params = sum(p.numel() for p in self.text_projection.parameters() if p.requires_grad)
        proj_params += sum(p.numel() for p in self.vib_projection.parameters() if p.requires_grad)
        
        if self.use_aux:
            aux_params = sum(p.numel() for p in self.text_classifier.parameters() if p.requires_grad)
            aux_params += sum(p.numel() for p in self.vib_classifier.parameters() if p.requires_grad)
        else:
            aux_params = 0
        
        return {
            'text_encoder': text_params,
            'vib_encoder': vib_params,
            'projections': proj_params,
            'classifiers': aux_params,
            'total': text_params + vib_params + proj_params + aux_params
        }
    
    def save_checkpoint(self, path: str, epoch: int, optimizer_state: Optional[Dict] = None):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.state_dict(),
            'domain_stage': self.domain_stage,
            'embedding_dim': self.embedding_dim,
            'is_continual_mode': self.is_continual_mode
        }
        
        if optimizer_state is not None:
            checkpoint['optimizer_state_dict'] = optimizer_state
        
        torch.save(checkpoint, path)
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {path}")
    
    def load_checkpoint(self, path: str, device: torch.device) -> Dict:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        checkpoint = torch.load(path, map_location=device)
        
        self.load_state_dict(checkpoint['model_state_dict'])
        
        # ëª¨ë“œ ë³µì›
        if checkpoint.get('is_continual_mode', False):
            self.switch_to_continual_mode()
        else:
            self.switch_to_first_domain_mode()
        
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ: {path}")
        
        return checkpoint


def create_textvib_model(domain_stage: str = 'first_domain', dataset_type: str = 'uos') -> TextVibCLIP:
    """
    TextVibCLIP v2 ëª¨ë¸ ìƒì„±
    
    Args:
        domain_stage: 'first_domain' ë˜ëŠ” 'continual'
        dataset_type: 'uos' ë˜ëŠ” 'cwru'
        
    Returns:
        TextVibCLIP: ìƒˆë¡œìš´ ranking-based ëª¨ë¸
    """
    model = TextVibCLIP(domain_stage=domain_stage, dataset_type=dataset_type)
    
    # íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥
    param_info = model.get_trainable_parameters()
    logger.info(f"TextVibCLIP v2 ìƒì„± ì™„ë£Œ: {domain_stage} stage")
    logger.info(f"Text encoder: {param_info['text_encoder']:,}")
    logger.info(f"Vibration encoder: {param_info['vib_encoder']:,}")
    logger.info(f"Projections: {param_info['projections']:,}")
    logger.info(f"Classifiers: {param_info['classifiers']:,}")
    logger.info(f"ì´ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {param_info['total']:,}")
    
    return model


def compute_text_vib_similarities(text_embeddings: torch.Tensor, 
                            vib_embeddings: torch.Tensor) -> torch.Tensor:
    """
    í…ìŠ¤íŠ¸-ì§„ë™ ìœ ì‚¬ë„ ê³„ì‚° (ì¶”ë¡ ìš©)
    
    Args:
        text_embeddings: (num_texts, embedding_dim)
        vib_embeddings: (num_vibs, embedding_dim)
        
    Returns:
        torch.Tensor: ìœ ì‚¬ë„ í–‰ë ¬ (num_vibs, num_texts)
    """
    # L2 ì •ê·œí™”
    text_embeddings = F.normalize(text_embeddings, p=2, dim=1)
    vib_embeddings = F.normalize(vib_embeddings, p=2, dim=1)
    
    # Cosine similarity
    similarities = torch.matmul(vib_embeddings, text_embeddings.t())
    
    return similarities


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    print("=== TextVibCLIP v2 í…ŒìŠ¤íŠ¸ ===")
    
    # ëª¨ë¸ ìƒì„±
    model = create_textvib_model('first_domain')
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    batch_size = 4
    test_batch = {
        'vibration': torch.randn(batch_size, 2048),
        'text': [
            "Healthy bearing condition observed",
            "Ball element defect detected", 
            "Inner race fault observed",
            "Outer race defect detected"
        ],
        'labels': torch.tensor([0, 1, 2, 3])
    }
    
    # GPU ì‚¬ìš©
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    test_batch['vibration'] = test_batch['vibration'].to(device)
    test_batch['labels'] = test_batch['labels'].to(device)
    
    # Forward pass í…ŒìŠ¤íŠ¸
    results = model(test_batch, return_embeddings=True)
    
    print(f"Loss: {results['loss'].item():.4f}")
    print(f"Loss components: {results['loss_components']}")
    print(f"Text embeddings shape: {results['text_embeddings'].shape}")
    print(f"Vib embeddings shape: {results['vib_embeddings'].shape}")
    
    # ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    print("\n=== ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ===")
    new_vibration = torch.randn(2048).to(device)
    candidates = [
        "Healthy bearing condition observed",
        "Ball element defect detected",
        "Inner race fault observed", 
        "Outer race defect detected"
    ]
    
    best_idx, confidence = model.predict_best_match(new_vibration, candidates, device)
    print(f"Best match: {candidates[best_idx]}")
    print(f"Confidence: {confidence:.4f}")
    
    print("\n=== TextVibCLIP v2 í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
