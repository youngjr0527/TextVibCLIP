"""
ë°ì´í„° ë¡œë”© ìºì‹± ì‹œìŠ¤í…œ
ë°˜ë³µ ì‹¤í—˜ ì‹œ ë°ì´í„° ë¡œë”© ì‹œê°„ì„ ëŒ€í­ ë‹¨ì¶•
"""

import os
import pickle
import hashlib
import time
import torch
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class DataCache:
    """ë°ì´í„° ë¡œë”© ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self, cache_dir: str = "cache"):
        """
        Args:
            cache_dir: ìºì‹œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # ìºì‹œ í†µê³„
        self.cache_hits = 0
        self.cache_misses = 0
        
        logger.info(f"DataCache ì´ˆê¸°í™”: {self.cache_dir}")
    
    def _get_cache_key(self, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„± (íŒŒë¼ë¯¸í„° ê¸°ë°˜ í•´ì‹±)"""
        # ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ë“¤ë§Œ ì‚¬ìš©í•˜ì—¬ í•´ì‹œ ìƒì„±
        key_params = {
            'data_dir': kwargs.get('data_dir', ''),
            'dataset_type': kwargs.get('dataset_type', ''),
            'domain_value': kwargs.get('domain_value', ''),
            'subset': kwargs.get('subset', ''),
            'window_size': kwargs.get('window_size', 4096),
            'overlap_ratio': kwargs.get('overlap_ratio', 0.5),
            'normalization': kwargs.get('normalization', 'standardize')
        }
        
        # í•´ì‹œ ìƒì„±
        key_str = str(sorted(key_params.items()))
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _get_cache_path(self, cache_key: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        return self.cache_dir / f"data_{cache_key}.pkl"
    
    def _get_metadata_path(self, cache_key: str) -> Path:
        """ìºì‹œ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        return self.cache_dir / f"meta_{cache_key}.pkl"
    
    def get_cached_data(self, **kwargs) -> Optional[Dict[str, Any]]:
        """ìºì‹œëœ ë°ì´í„° ë¡œë“œ"""
        cache_key = self._get_cache_key(**kwargs)
        cache_path = self._get_cache_path(cache_key)
        meta_path = self._get_metadata_path(cache_key)
        
        if not cache_path.exists() or not meta_path.exists():
            self.cache_misses += 1
            return None
        
        try:
            # ë©”íƒ€ë°ì´í„° í™•ì¸
            with open(meta_path, 'rb') as f:
                metadata = pickle.load(f)
            
            # ë°ì´í„° ë””ë ‰í† ë¦¬ ë³€ê²½ ì‹œê°„ í™•ì¸
            data_dir = kwargs.get('data_dir', '')
            if os.path.exists(data_dir):
                dir_mtime = os.path.getmtime(data_dir)
                if dir_mtime > metadata.get('cache_time', 0):
                    logger.info(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ë³€ê²½ë¨, ìºì‹œ ë¬´íš¨í™”: {cache_key[:8]}")
                    self.cache_misses += 1
                    return None
            
            # ìºì‹œëœ ë°ì´í„° ë¡œë“œ
            with open(cache_path, 'rb') as f:
                cached_data = pickle.load(f)
            
            self.cache_hits += 1
            logger.info(f"âœ… ìºì‹œ ì ì¤‘: {cache_key[:8]} (ì ì¤‘ë¥ : {self.cache_hits/(self.cache_hits+self.cache_misses)*100:.1f}%)")
            
            return cached_data
            
        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.cache_misses += 1
            return None
    
    def save_cached_data(self, data: Dict[str, Any], **kwargs):
        """ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_key = self._get_cache_key(**kwargs)
        cache_path = self._get_cache_path(cache_key)
        meta_path = self._get_metadata_path(cache_key)
        
        try:
            # ë°ì´í„° ì €ì¥
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'cache_time': time.time(),
                'cache_key': cache_key,
                'params': kwargs,
                'data_size': len(data.get('samples', []))
            }
            
            with open(meta_path, 'wb') as f:
                pickle.dump(metadata, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {cache_key[:8]} ({metadata['data_size']}ê°œ ìƒ˜í”Œ)")
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def clear_cache(self):
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        import shutil
        if self.cache_dir.exists():
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir()
            logger.info("ğŸ—‘ï¸ ìºì‹œ ì „ì²´ ì‚­ì œ ì™„ë£Œ")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        cache_files = list(self.cache_dir.glob("data_*.pkl"))
        total_size = sum(f.stat().st_size for f in cache_files)
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': self.cache_hits / max(1, self.cache_hits + self.cache_misses) * 100,
            'cached_files': len(cache_files),
            'total_size_mb': total_size / (1024 * 1024)
        }


# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
_global_cache = None

def get_global_cache() -> DataCache:
    """ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_cache
    if _global_cache is None:
        _global_cache = DataCache()
    return _global_cache


def preprocess_and_cache_dataset(data_dir: str, 
                                dataset_type: str,
                                domain_value: Optional[int] = None,
                                subset: str = 'train',
                                window_size: int = 4096,
                                overlap_ratio: float = 0.5,
                                normalization: str = 'standardize') -> Dict[str, Any]:
    """
    ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ìºì‹±
    
    Returns:
        Dict containing:
            - 'samples': List of preprocessed samples
            - 'file_paths': List of file paths
            - 'metadata': List of metadata
    """
    cache = get_global_cache()
    
    # ìºì‹œ í™•ì¸
    cache_params = {
        'data_dir': data_dir,
        'dataset_type': dataset_type,
        'domain_value': domain_value,
        'subset': subset,
        'window_size': window_size,
        'overlap_ratio': overlap_ratio,
        'normalization': normalization
    }
    
    cached_data = cache.get_cached_data(**cache_params)
    if cached_data is not None:
        return cached_data
    
    # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ ë°ì´í„° ë¡œë”©
    logger.info(f"ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘: {dataset_type} {domain_value} {subset}")
    start_time = time.time()
    
    # ì‹¤ì œ ë°ì´í„° ë¡œë”© ë¡œì§ (ê¸°ì¡´ BearingDataset ë¡œì§ ì‚¬ìš©)
    from .data_loader import BearingDataset
    
    # ì„ì‹œ ë°ì´í„°ì…‹ ìƒì„±í•˜ì—¬ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì¶”ì¶œ
    temp_dataset = BearingDataset(
        data_dir=data_dir,
        dataset_type=dataset_type,
        domain_value=domain_value,
        subset=subset,
        window_size=window_size,
        overlap_ratio=overlap_ratio,
        normalization=normalization
    )
    
    # ëª¨ë“  ìƒ˜í”Œ ì „ì²˜ë¦¬
    samples = []
    for idx in range(len(temp_dataset)):
        sample = temp_dataset[idx]
        # í…ì„œë¥¼ numpyë¡œ ë³€í™˜ (pickle í˜¸í™˜ì„±)
        processed_sample = {
            'vibration': sample['vibration'].numpy(),
            'text': sample['text'],
            'metadata': sample['metadata'],
            'labels': sample['labels'].numpy(),
            'domain_key': sample['domain_key'],
            'file_idx': sample['file_idx'],
            'window_idx': sample['window_idx']
        }
        samples.append(processed_sample)
    
    # ìºì‹œí•  ë°ì´í„° êµ¬ì„±
    cached_data = {
        'samples': samples,
        'file_paths': temp_dataset.file_paths,
        'metadata_list': temp_dataset.metadata_list,
        'windows_per_file': temp_dataset.windows_per_file
    }
    
    # ìºì‹œ ì €ì¥
    cache.save_cached_data(cached_data, **cache_params)
    
    elapsed_time = time.time() - start_time
    logger.info(f"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(samples)}ê°œ ìƒ˜í”Œ, {elapsed_time:.1f}ì´ˆ")
    
    return cached_data


class CachedBearingDataset(torch.utils.data.Dataset):
    """ìºì‹œëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê³ ì† Dataset"""
    
    def __init__(self, 
                 data_dir: str,
                 dataset_type: str,
                 domain_value: Optional[int] = None,
                 subset: str = 'train',
                 window_size: int = 4096,
                 overlap_ratio: float = 0.5,
                 normalization: str = 'standardize'):
        """
        Args:
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            dataset_type: 'uos' ë˜ëŠ” 'cwru'
            domain_value: ë„ë©”ì¸ ê°’ (RPM ë˜ëŠ” Load)
            subset: 'train', 'val', 'test'
            window_size: ìœˆë„ìš° í¬ê¸°
            overlap_ratio: ìœˆë„ìš° ê²¹ì¹¨ ë¹„ìœ¨
            normalization: ì •ê·œí™” ë°©ë²•
        """
        # ìºì‹œëœ ë°ì´í„° ë¡œë“œ
        self.cached_data = preprocess_and_cache_dataset(
            data_dir=data_dir,
            dataset_type=dataset_type,
            domain_value=domain_value,
            subset=subset,
            window_size=window_size,
            overlap_ratio=overlap_ratio,
            normalization=normalization
        )
        
        self.samples = self.cached_data['samples']
        self.file_paths = self.cached_data['file_paths']
        self.metadata_list = self.cached_data['metadata_list']
        
        # ì†ì„± ì„¤ì •
        self.dataset_type = dataset_type
        self.domain_value = domain_value
        self.subset = subset
        
        logger.info(f"CachedBearingDataset ì´ˆê¸°í™”: {len(self.samples)}ê°œ ìƒ˜í”Œ (ìºì‹œ ì‚¬ìš©)")
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """ìºì‹œëœ ìƒ˜í”Œ ë°˜í™˜"""
        if idx >= len(self.samples):
            idx = len(self.samples) - 1
        
        sample = self.samples[idx]
        
        # numpyë¥¼ ë‹¤ì‹œ í…ì„œë¡œ ë³€í™˜
        return {
            'vibration': torch.from_numpy(sample['vibration']).float(),
            'text': sample['text'],
            'metadata': sample['metadata'],
            'labels': torch.from_numpy(sample['labels']).long(),
            'domain_key': sample['domain_key'],
            'file_idx': sample['file_idx'],
            'window_idx': sample['window_idx']
        }


def create_cached_domain_dataloaders(data_dir: str,
                                   domain_order: List[int],
                                   dataset_type: str = 'uos',
                                   batch_size: int = 32,
                                   num_workers: int = 0,  # ìºì‹œ ì‚¬ìš© ì‹œ ë©€í‹°í”„ë¡œì„¸ì‹± ë¶ˆí•„ìš”
                                   use_collate_fn: bool = True) -> Dict[int, Dict[str, torch.utils.data.DataLoader]]:
    """
    ìºì‹œëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê³ ì† DataLoader ìƒì„±
    
    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        domain_order: ë„ë©”ì¸ ìˆœì„œ
        dataset_type: 'uos' ë˜ëŠ” 'cwru'
        batch_size: ë°°ì¹˜ í¬ê¸°
        num_workers: ì›Œì»¤ ìˆ˜ (ìºì‹œ ì‚¬ìš© ì‹œ 0 ê¶Œì¥)
        use_collate_fn: collate_fn ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        ë„ë©”ì¸ë³„ DataLoader ë”•ì…”ë„ˆë¦¬
    """
    from .data_loader import create_collate_fn
    
    domain_dataloaders = {}
    collate_fn = create_collate_fn() if use_collate_fn else None
    
    logger.info(f"ğŸš€ ìºì‹œëœ DataLoader ìƒì„± ì‹œì‘: {dataset_type} ({len(domain_order)}ê°œ ë„ë©”ì¸)")
    
    for domain_value in domain_order:
        domain_name = f"{domain_value}RPM" if dataset_type == 'uos' else f"{domain_value}HP"
        
        # ê° subsetë³„ ìºì‹œëœ Dataset ìƒì„±
        train_dataset = CachedBearingDataset(
            data_dir=data_dir,
            dataset_type=dataset_type,
            domain_value=domain_value,
            subset='train'
        )
        
        val_dataset = CachedBearingDataset(
            data_dir=data_dir,
            dataset_type=dataset_type,
            domain_value=domain_value,
            subset='val'
        )
        
        test_dataset = CachedBearingDataset(
            data_dir=data_dir,
            dataset_type=dataset_type,
            domain_value=domain_value,
            subset='test'
        )
        
        # DataLoader ìƒì„± (num_workers=0ìœ¼ë¡œ ê³ ì†í™”)
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=collate_fn
        )
        
        val_loader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=collate_fn
        )
        
        test_loader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=collate_fn
        )
        
        domain_dataloaders[domain_value] = {
            'train': train_loader,
            'val': val_loader,
            'test': test_loader
        }
        
        logger.info(f"Domain {domain_name}: Train {len(train_dataset)}, "
                   f"Val {len(val_dataset)}, Test {len(test_dataset)}")
    
    # ìºì‹œ í†µê³„ ì¶œë ¥
    cache = get_global_cache()
    stats = cache.get_cache_stats()
    logger.info(f"ğŸ“Š ìºì‹œ í†µê³„: ì ì¤‘ë¥  {stats['hit_rate']:.1f}%, "
               f"ìºì‹œ íŒŒì¼ {stats['cached_files']}ê°œ, "
               f"í¬ê¸° {stats['total_size_mb']:.1f}MB")
    
    return domain_dataloaders


def create_cached_first_domain_dataloader(data_dir: str,
                                        domain_order: List[int],
                                        dataset_type: str = 'uos',
                                        subset: str = 'train',
                                        batch_size: int = 32,
                                        num_workers: int = 0,
                                        use_collate_fn: bool = True) -> torch.utils.data.DataLoader:
    """
    ìºì‹œëœ ì²« ë²ˆì§¸ ë„ë©”ì¸ DataLoader ìƒì„±
    
    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        domain_order: ë„ë©”ì¸ ìˆœì„œ (ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©)
        dataset_type: 'uos' ë˜ëŠ” 'cwru'
        subset: 'train', 'val', 'test'
        batch_size: ë°°ì¹˜ í¬ê¸°
        num_workers: ì›Œì»¤ ìˆ˜
        use_collate_fn: collate_fn ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        ì²« ë²ˆì§¸ ë„ë©”ì¸ DataLoader
    """
    from .data_loader import create_collate_fn
    
    first_domain_value = domain_order[0]
    
    # ìºì‹œëœ Dataset ìƒì„±
    dataset = CachedBearingDataset(
        data_dir=data_dir,
        dataset_type=dataset_type,
        domain_value=first_domain_value,
        subset=subset
    )
    
    # collate_fn ì¤€ë¹„
    collate_fn = create_collate_fn() if use_collate_fn else None
    
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(subset == 'train'),
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn
    )
    
    domain_name = f"{first_domain_value}RPM" if dataset_type == 'uos' else f"{first_domain_value}HP"
    logger.info(f"First Domain DataLoader ìƒì„± (ìºì‹œ): Domain {domain_name}, "
               f"{subset} subset, {len(dataset)}ê°œ ìƒ˜í”Œ")
    
    return dataloader


def clear_all_caches():
    """ëª¨ë“  ìºì‹œ ì‚­ì œ (ë””ë²„ê¹…ìš©)"""
    cache = get_global_cache()
    cache.clear_cache()
    logger.info("ğŸ—‘ï¸ ëª¨ë“  ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤")


if __name__ == "__main__":
    # ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ§ª ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    # ì²« ë²ˆì§¸ ë¡œë“œ (ìºì‹œ ë¯¸ìŠ¤)
    start_time = time.time()
    data1 = preprocess_and_cache_dataset(
        data_dir='data_scenario1',
        dataset_type='uos',
        domain_value=600,
        subset='train'
    )
    first_load_time = time.time() - start_time
    print(f"ì²« ë²ˆì§¸ ë¡œë“œ: {first_load_time:.2f}ì´ˆ")
    
    # ë‘ ë²ˆì§¸ ë¡œë“œ (ìºì‹œ ì ì¤‘)
    start_time = time.time()
    data2 = preprocess_and_cache_dataset(
        data_dir='data_scenario1',
        dataset_type='uos',
        domain_value=600,
        subset='train'
    )
    second_load_time = time.time() - start_time
    print(f"ë‘ ë²ˆì§¸ ë¡œë“œ: {second_load_time:.2f}ì´ˆ")
    
    # ì†ë„ í–¥ìƒ ê³„ì‚°
    speedup = first_load_time / max(0.001, second_load_time)
    print(f"ğŸš€ ì†ë„ í–¥ìƒ: {speedup:.1f}ë°°")
    
    # ìºì‹œ í†µê³„
    cache = get_global_cache()
    stats = cache.get_cache_stats()
    print(f"ğŸ“Š ìºì‹œ í†µê³„: {stats}")
