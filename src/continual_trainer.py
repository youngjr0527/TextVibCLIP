"""
ContinualTrainer: Ranking-based TextVibCLIPìš© í•™ìŠµ íŒŒì´í”„ë¼ì¸
Triplet/Ranking Loss ê¸°ë°˜ìœ¼ë¡œ ì†Œê·œëª¨ ë°ì´í„°ì— ìµœì í™”
"""

import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
import numpy as np
from collections import defaultdict

from .textvib_model import TextVibCLIP, create_textvib_model
from .replay_buffer import ReplayBuffer
from .data_loader import create_domain_dataloaders, create_first_domain_dataloader
from .data_cache import create_cached_first_domain_dataloader
from configs.model_config import TRAINING_CONFIG, DATA_CONFIG, EVAL_CONFIG, MODEL_CONFIG, FIRST_DOMAIN_CONFIG, CONTINUAL_CONFIG

logger = logging.getLogger(__name__)


class ContinualTrainer:
    """
    TextVibCLIP Continual Learning Trainer
    
    Ranking-based learning with simplified architecture
    """
    
    def __init__(self,
                 model: Optional[TextVibCLIP] = None,
                 device: torch.device = torch.device('cpu'),
                 save_dir: str = 'checkpoints',
                 max_grad_norm: float = 0.1,
                 domain_order: List[Union[int, str]] = None,
                 data_dir: Optional[str] = None,
                 dataset_type: str = 'uos',
                 patience: Optional[int] = None,
                 results_save_dir: Optional[str] = None):
        """
        Args:
            model: ì‚¬ì „ ì´ˆê¸°í™”ëœ ëª¨ë¸
            device: í•™ìŠµ ë””ë°”ì´ìŠ¤
            save_dir: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
            max_grad_norm: Gradient clipping
            domain_order: ë„ë©”ì¸ ìˆœì„œ
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            dataset_type: ë°ì´í„°ì…‹ íƒ€ì…
            patience: Early stopping patience
        """
        self.device = device
        self.save_dir = save_dir
        self.max_grad_norm = max_grad_norm
        os.makedirs(save_dir, exist_ok=True)
        # ê²°ê³¼ í´ë” ë‚´ ì²´í¬í¬ì¸íŠ¸ ë¯¸ëŸ¬ ì €ì¥ì†Œ (ì„ íƒ)
        self.mirror_save_dir = results_save_dir
        if self.mirror_save_dir:
            os.makedirs(self.mirror_save_dir, exist_ok=True)
        
        # ëª¨ë¸ ì´ˆê¸°í™” (ë°ì´í„°ì…‹ íƒ€ì… ì „ë‹¬)
        if model is None:
            self.model = create_textvib_model('first_domain', dataset_type)
        else:
            self.model = model
        
        self.model.to(device)
        
        # Replay buffer
        self.replay_buffer = ReplayBuffer()
        
        # í•™ìŠµ ìƒíƒœ ê´€ë¦¬
        self.current_domain_idx = 0
        self.completed_domains = []
        self.domain_order = domain_order if domain_order is not None else DATA_CONFIG['domain_order']
        self.data_dir = data_dir if data_dir is not None else DATA_CONFIG['data_dir']
        self.dataset_type = dataset_type
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_history = defaultdict(list)
        self.loss_history = defaultdict(list)
        self.forgetting_scores = []
        self.best_accuracy_per_domain: Dict[Union[int, str], float] = {}
        
        # í•™ìŠµ ì„¤ì •
        self.batch_size = TRAINING_CONFIG['batch_size']
        self.num_epochs = TRAINING_CONFIG['num_epochs']
        self.learning_rate = TRAINING_CONFIG['learning_rate']
        self.weight_decay = TRAINING_CONFIG['weight_decay']
        self.replay_ratio = TRAINING_CONFIG['replay_ratio']
        self.grad_accum_steps = int(TRAINING_CONFIG.get('grad_accum_steps', 1))
        self.patience = int(patience) if patience is not None else int(TRAINING_CONFIG.get('patience', 10))
        
        logger.info(f"ContinualTrainer ì´ˆê¸°í™” ì™„ë£Œ: device={device}")
    
    def train_first_domain(self, 
                         first_domain_dataloader: Optional[DataLoader] = None,
                         num_epochs: int = None) -> Dict[str, float]:
        """
        ì²« ë²ˆì§¸ ë„ë©”ì¸ í•™ìŠµ (Foundation Learning)
        """
        logger.info("=== First Domain Training ì‹œì‘ ===")
        
        # ë°ì´í„°ë¡œë” ì¤€ë¹„
        if first_domain_dataloader is None:
            first_domain_dataloader = create_cached_first_domain_dataloader(
                data_dir=self.data_dir,
                domain_order=self.domain_order,
                dataset_type=self.dataset_type,
                subset='train', 
                batch_size=self.batch_size
            )
        
        # UOS First Domain ì„¤ì • ì ìš©
        config = FIRST_DOMAIN_CONFIG
        logger.info("ğŸ¯ UOS First Domain ì„¤ì • ì ìš©")
        
        if num_epochs is None:
            num_epochs = config['num_epochs']
        
        self.learning_rate = config['learning_rate']
        self.weight_decay = config['weight_decay']
        
        logger.info(f"First Domain ì„¤ì •: ì—í¬í¬={num_epochs}, LR={self.learning_rate:.1e}, WD={self.weight_decay:.1e}")
        
        # ëª¨ë¸ì„ ì²« ë²ˆì§¸ ë„ë©”ì¸ ëª¨ë“œë¡œ ì„¤ì •
        self.model.switch_to_first_domain_mode()
        
        # Optimizer ì„¤ì •
        optimizer = self._create_optimizer()
        scheduler = self._create_scheduler(optimizer)
        
        # í•™ìŠµ ë£¨í”„
        self.model.train()
        epoch_losses = []
        
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            for batch_idx, batch in enumerate(first_domain_dataloader):
                batch = self._move_batch_to_device(batch)
                
                # Forward pass
                if (batch_idx % self.grad_accum_steps) == 0:
                    optimizer.zero_grad(set_to_none=True)
                
                    results = self.model(batch)
                    loss = results['loss'] / self.grad_accum_steps
                    
                    # Backward pass
                    loss.backward()
                    
                    if (batch_idx + 1) % self.grad_accum_steps == 0:
                        if self.max_grad_norm > 0:
                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                        optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                
                # ë¡œê¹…
                if batch_idx % 100 == 0:
                    logger.debug(f"First Domain Epoch {epoch+1}/{num_epochs}, "
                                 f"Batch {batch_idx}, Loss: {loss.item():.4f}")
            
            avg_epoch_loss = epoch_loss / num_batches
            epoch_losses.append(avg_epoch_loss)
            
            # ì—í¬í¬ ëì—ì„œ scheduler step
            scheduler.step()
            
            logger.info(f"First Domain Epoch {epoch+1} ì™„ë£Œ: Avg Loss = {avg_epoch_loss:.4f}")
        
        # ì²« ë²ˆì§¸ ë„ë©”ì¸ ì™„ë£Œ í‘œì‹œ
        first_domain = self.domain_order[0]
        self.completed_domains.append(first_domain)
        self.loss_history[first_domain] = epoch_losses
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = os.path.join(self.save_dir, 'first_domain_final.pth')
        self.model.save_checkpoint(checkpoint_path, num_epochs, optimizer.state_dict())
        # ë¯¸ëŸ¬ ì €ì¥
        if self.mirror_save_dir:
            mirror_path = os.path.join(self.mirror_save_dir, 'first_domain_final.pth')
            self.model.save_checkpoint(mirror_path, num_epochs, optimizer.state_dict())
        
        # ğŸ¯ ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œ í‰ê°€ (ì•„ì§ í•™ìŠµí•˜ì§€ ì•Šì€ ë„ë©”ì¸ì€ í‰ê°€ ì•ˆí•¨)
        first_domain = self.domain_order[0]
        domain_dataloaders = create_domain_dataloaders(
            data_dir=self.data_dir,
            domain_order=[first_domain],  # ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œ
            dataset_type=self.dataset_type,
            batch_size=self.batch_size
        )
        first_domain_performance = self._evaluate_all_domains(domain_dataloaders)
        
        # ì²« ë²ˆì§¸ ë„ë©”ì¸ ì„±ëŠ¥ë§Œ ê¸°ë¡
        first_domain_accuracy = 0.0
        if first_domain in first_domain_performance:
            metrics = first_domain_performance[first_domain]
            
            # performance_history ì´ˆê¸°í™”
            if first_domain not in self.performance_history:
                self.performance_history[first_domain] = {
                    'accuracy': [], 'top1_retrieval': [], 'top5_retrieval': [],
                    'text_accuracy': [], 'vib_accuracy': []
                }
            
            # ì²« ë²ˆì§¸ ë„ë©”ì¸ ì„±ëŠ¥ ê¸°ë¡
            self.performance_history[first_domain]['accuracy'].append(metrics['accuracy'])
            self.performance_history[first_domain]['top1_retrieval'].append(metrics.get('top1_retrieval', 0.0))
            self.performance_history[first_domain]['text_accuracy'].append(metrics.get('text_accuracy', 0.0))
            self.performance_history[first_domain]['vib_accuracy'].append(metrics.get('vib_accuracy', 0.0))
            if self.dataset_type != 'cwru' and 'top5_retrieval' in metrics:
                self.performance_history[first_domain]['top5_retrieval'].append(metrics.get('top5_retrieval', 0.0))
            
            first_domain_accuracy = metrics['accuracy']
            self.best_accuracy_per_domain[first_domain] = max(self.best_accuracy_per_domain.get(first_domain, 0.0), float(first_domain_accuracy))
        
        logger.info(f"ì²« ë²ˆì§¸ ë„ë©”ì¸ ì •í™•ë„: {first_domain_accuracy:.4f}")
        logger.info("=== First Domain Training ì™„ë£Œ ===")
        
        return {
            'final_loss': epoch_losses[-1] if epoch_losses else float('nan'),
            'avg_loss': np.mean(epoch_losses) if epoch_losses else float('nan'),
            'domain_performances': first_domain_performance
        }
    
    def train_remaining_domains(self, domain_dataloaders: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ë‚˜ë¨¸ì§€ ë„ë©”ì¸ë“¤ ìˆœì°¨ í•™ìŠµ (Continual Learning)
        """
        logger.info("=== Remaining Domains Training ì‹œì‘ ===")
        
        # UOS Continual ì„¤ì • ì ìš©
        config = CONTINUAL_CONFIG
        logger.info("ğŸ¯ UOS Continual ì„¤ì • ì ìš©")
        
        self.num_epochs = config['num_epochs']
        self.learning_rate = config['learning_rate']
        self.weight_decay = config['weight_decay']
        self.patience = config['patience']
        
        logger.info(f"ì„¤ì •: ì—í¬í¬={self.num_epochs}, LR={self.learning_rate:.1e}, WD={self.weight_decay:.1e}")
        
        # ë°ì´í„°ë¡œë” ì¤€ë¹„
        if domain_dataloaders is None:
            domain_dataloaders = create_domain_dataloaders(
                data_dir=self.data_dir,
                domain_order=self.domain_order,
                dataset_type=self.dataset_type,
                batch_size=self.batch_size
            )
        
        # Continual modeë¡œ ì „í™˜
        self.model.switch_to_continual_mode()
        
        remaining_domains_results = {}
        
        # Domain 2ë¶€í„° ìˆœì°¨ í•™ìŠµ
        for domain_idx in range(1, len(self.domain_order)):
            domain_value = self.domain_order[domain_idx]
            
            logger.info(f"\n--- Domain {domain_value} í•™ìŠµ ì‹œì‘ ---")
            
            if domain_value not in domain_dataloaders:
                logger.error(f"ë„ë©”ì¸ {domain_value}ê°€ dataloadersì— ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            current_train_loader = domain_dataloaders[domain_value]['train']
            current_val_loader = domain_dataloaders[domain_value]['val']
            
            # ì´ì „ ë„ë©”ì¸ ì„±ëŠ¥ ê¸°ë¡
            previous_domains = self.completed_domains.copy()
            if previous_domains:
                previous_dataloaders = {d: domain_dataloaders[d] for d in previous_domains if d in domain_dataloaders}
                before_performance = self._evaluate_all_domains(previous_dataloaders)
                
                for d, m in before_performance.items():
                    acc = float(m.get('accuracy', 0.0))
                    self.best_accuracy_per_domain[d] = max(self.best_accuracy_per_domain.get(d, 0.0), acc)
            else:
                before_performance = {}

            # í˜„ì¬ ë„ë©”ì¸ í•™ìŠµ
            domain_results = self._train_single_domain(domain_value, current_train_loader, current_val_loader)
            
            # ëˆ„ì  ì„±ëŠ¥ í‰ê°€
            current_domains = self.completed_domains + [domain_value]
            cumulative_dataloaders = {d: domain_dataloaders[d] for d in current_domains if d in domain_dataloaders}
            after_performance = self._evaluate_all_domains(cumulative_dataloaders)
            
            # Forgetting ê³„ì‚°
            forgetting_score = self._calculate_forgetting(before_performance, after_performance)
            self.forgetting_scores.append(forgetting_score)
            
            # ì„±ëŠ¥ ê¸°ë¡
            for eval_domain, metrics in after_performance.items():
                if eval_domain not in self.performance_history:
                    self.performance_history[eval_domain] = {
                        'accuracy': [], 'top1_retrieval': [], 'top5_retrieval': [],
                        'text_accuracy': [], 'vib_accuracy': []
                    }
                self.performance_history[eval_domain]['accuracy'].append(metrics.get('accuracy', 0.0))
                self.performance_history[eval_domain]['top1_retrieval'].append(metrics.get('top1_retrieval', 0.0))
                self.performance_history[eval_domain]['text_accuracy'].append(metrics.get('text_accuracy', 0.0))
                self.performance_history[eval_domain]['vib_accuracy'].append(metrics.get('vib_accuracy', 0.0))
                if self.dataset_type != 'cwru' and 'top5_retrieval' in metrics:
                    self.performance_history[eval_domain]['top5_retrieval'].append(metrics.get('top5_retrieval', 0.0))

            # ë„ë©”ì¸ ì™„ë£Œ
            self.completed_domains.append(domain_value)
            remaining_domains_results[domain_value] = {
                'training_results': domain_results,
                'performance': after_performance,
                'forgetting_score': forgetting_score
            }
            
            logger.info(f"Domain {domain_value} ì™„ë£Œ: Forgetting = {forgetting_score:.4f}")
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        final_metrics = self._calculate_final_metrics()
        remaining_domains_results['final_metrics'] = final_metrics
        
        logger.info(f"ìµœì¢… í‰ê·  ì •í™•ë„: {final_metrics.get('average_accuracy', 0.0):.4f}")
        logger.info("=== Remaining Domains Training ì™„ë£Œ ===")
        
        return remaining_domains_results
    
    def _train_single_domain(self, 
                           domain_value: Union[int, str],
                           train_loader: DataLoader,
                           val_loader: DataLoader) -> Dict[str, float]:
        """ë‹¨ì¼ ë„ë©”ì¸ í•™ìŠµ (Replay í¬í•¨)"""
        
        # Continual optimizer
        optimizer = self._create_continual_optimizer()
        
        # í˜„ì¬ ë„ë©”ì¸ ì„ë² ë”© ìˆ˜ì§‘ (Replayìš©)
        domain_embeddings = self._collect_domain_embeddings(train_loader)
        if domain_embeddings:
            self.replay_buffer.add_domain_data(
                domain_value,
                domain_embeddings['text_embeddings'],
                domain_embeddings['vib_embeddings'],
                domain_embeddings['metadata'],
                labels=domain_embeddings.get('labels', None)
            )
        
        # í•™ìŠµ ë£¨í”„
        self.model.train()
        epoch_losses = []
        best_val_acc = 0.0
        best_epoch = -1
        patience_counter = 0

        for epoch in range(self.num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            for batch_idx, batch in enumerate(train_loader):
                batch = self._move_batch_to_device(batch)
                
                # Replay ë°ì´í„° ê²°í•© (ê°„í—ì )
                if batch_idx % 2 == 0:  # 50% í™•ë¥ ë¡œ replay ì‚¬ìš©
                    replay_batch_size = min(batch['vibration'].size(0), 4)
                    replay_data = self.replay_buffer.sample_replay_data(
                        replay_batch_size, exclude_current=True, device=self.device
                    )
                    if replay_data is not None:
                        combined_batch = self._combine_current_and_replay(batch, replay_data)
                    else:
                        combined_batch = batch
                else:
                    combined_batch = batch
                
                # Forward pass
                optimizer.zero_grad()
                results = self.model(combined_batch)
                loss = results['loss']
                    
                # Backward pass
                loss.backward()
                    
                if self.max_grad_norm > 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                
                    optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            avg_epoch_loss = epoch_loss / num_batches
            epoch_losses.append(avg_epoch_loss)
            
            # Validation
            val_metrics = self._evaluate_single_domain(val_loader)
            val_acc = val_metrics['accuracy']
            
            logger.info(f"Domain {domain_value} Epoch {epoch+1}: "
                       f"Loss = {avg_epoch_loss:.4f}, Val Acc = {val_acc:.4f}")
            
            # Early stopping
            min_ep = int(CONTINUAL_CONFIG.get('min_epoch', 2))
            patience_threshold = int(CONTINUAL_CONFIG.get('patience', 3))
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_epoch = epoch + 1
                patience_counter = 0
                
                # Best model ì €ì¥
                checkpoint_path = os.path.join(self.save_dir, f'domain_{domain_value}_best.pth')
                self.model.save_checkpoint(checkpoint_path, epoch, optimizer.state_dict())
                if self.mirror_save_dir:
                    mirror_path = os.path.join(self.mirror_save_dir, f'domain_{domain_value}_best.pth')
                    self.model.save_checkpoint(mirror_path, epoch, optimizer.state_dict())
            else:
                patience_counter += 1
                
                if (epoch + 1) >= min_ep and patience_counter >= patience_threshold:
                    logger.info(f"Early stopping at epoch {epoch+1}")
                    break
        
        self.loss_history[domain_value] = epoch_losses

        return {
            'final_loss': epoch_losses[-1] if epoch_losses else float('inf'),
            'best_val_accuracy': best_val_acc,
            'best_epoch': best_epoch,
            'num_epochs': len(epoch_losses)
        }
    
    def _evaluate_single_domain(self, dataloader: DataLoader) -> Dict[str, float]:
        """ë‹¨ì¼ ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€ (Dual-Head ë°©ì‹)"""
        # í‰ê°€ ëª¨ë“œ ì „í™˜ ìƒíƒœ ì €ì¥ ë° ì „í™˜
        was_training = self.model.training
        self.model.eval()
        
        all_text_preds = []
        all_vib_preds = []
        all_labels = []
        all_vib_embs = []
        
        with torch.no_grad():
            for batch in dataloader:
                batch = self._move_batch_to_device(batch)
                results = self.model(batch, return_embeddings=True)
                
                # ê° í—¤ë“œë³„ ì˜ˆì¸¡
                text_logits = self.model.text_classifier(results['text_raw'])
                vib_logits = self.model.vib_classifier(results['vib_raw'])
                
                text_preds = torch.argmax(text_logits, dim=1)
                vib_preds = torch.argmax(vib_logits, dim=1)
                
                all_text_preds.append(text_preds)
                all_vib_preds.append(vib_preds)
                if 'vib_embeddings' in results:
                    all_vib_embs.append(results['vib_embeddings'])
                    
                    # ë¼ë²¨ ì²˜ë¦¬
                    labels = batch['labels']
                    if labels.dim() == 2:
                        labels = labels[:, 0]
                    all_labels.append(labels)
        
        if not all_text_preds:
            # ì›ë˜ ëª¨ë“œ ë³µêµ¬
            if was_training:
                self.model.train()
            return {'accuracy': 0.0, 'top1_retrieval': 0.0, 'top5_retrieval': 0.0}
    
        # ê²°í•©
        text_preds = torch.cat(all_text_preds, dim=0)
        vib_preds = torch.cat(all_vib_preds, dim=0)
        labels = torch.cat(all_labels, dim=0)
        
        # ì •í™•ë„ ê³„ì‚° (ë³´ì¡° ë¶„ë¥˜ í—¤ë“œ ê¸°ë°˜)
        text_acc = (text_preds == labels).float().mean().item()
        vib_acc = (vib_preds == labels).float().mean().item()

        # CLIP-style retrieval í‰ê°€(í…ìŠ¤íŠ¸ëŠ” ê³ ì • í”„ë¡¬í”„íŠ¸ ì‚¬ìš©):
        #  - í”„ë¡¬í”„íŠ¸: ["Healthy bearing", "Ball fault", "Inner race fault", "Outer race fault"]
        #  - í…ìŠ¤íŠ¸ ì„ë² ë”©ì€ í”„ë¡¬í”„íŠ¸ë¡œë§Œ ìƒì„±í•˜ê³ , ì§„ë™ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ í´ë˜ìŠ¤ ê²°ì •
        try:
            device = next(self.model.parameters()).device
            class_prompts = [
                "Healthy bearing",
                "Ball fault",
                "Inner race fault",
                "Outer race fault"
            ]
            # ë°°ì¹˜ ë³„ ë¡œë”©ì´ ì•„ë‹ˆë¯€ë¡œ ì „ì²´ test dataloaderì˜ ì§„ë™ ì„ë² ë”©ì„ ë‹¤ì‹œ ì–»ê¸° ì–´ë µë‹¤.
            # ëŒ€ì‹  í˜„ì¬ ë©”ì„œë“œëŠ” ë°°ì¹˜ ë£¨í”„ì—ì„œ results['vib_raw']ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ,
            # ê°„ë‹¨íˆ ë¶„ë¥˜ í—¤ë“œ ê¸°ë°˜ ë¦¬í¬íŠ¸ë§Œ ìœ ì§€í•˜ê³ , retrieval í‰ê°€ëŠ” ë³„ë„ ê²½ë¡œë¡œ ì¶”ê°€ ê°€ëŠ¥.
            # (í•„ìš” ì‹œ í›„ì† ì»¤ë°‹ì—ì„œ dataloaderë¥¼ ë‹¤ì‹œ ìˆœíšŒí•´ ì„ë² ë”© í‰ê°€ ì¶”ê°€)
        except Exception:
            pass
        
        # ì•™ìƒë¸” ì •í™•ë„ (ë‹¨ìˆœí•œ ê°€ì¤‘ í‰ê· )
        ensemble_weight = torch.sigmoid(self.model.ensemble_weight).item()
        
        # ê²°ì •ë¡ ì  ì•™ìƒë¸”: ê°œë³„ ì •í™•ë„ì˜ ê°€ì¤‘ í‰ê· 
        ensemble_acc = ensemble_weight * vib_acc + (1 - ensemble_weight) * text_acc

        # ğŸ¯ CLIP-style retrieval í‰ê°€ (CWRU + UOS ëª¨ë‘ ì‚¬ìš©)
        # ì‹¤ì œ ëª¨ë¸ ì‚¬ìš© ë°©ì‹ê³¼ ì¼ì¹˜í•˜ëŠ” í‰ê°€ (SCI ë…¼ë¬¸ ê¶Œì¥)
        retrieval_acc = None
        retrieval_top5 = None
        if all_vib_embs:
            vib_emb = torch.cat(all_vib_embs, dim=0)
            device = vib_emb.device
            # ë°ì´í„°ì…‹ë³„ í´ë˜ìŠ¤ í”„ë¡¬í”„íŠ¸
            if self.dataset_type == 'cwru':
                # CWRU: 4-í´ë˜ìŠ¤ (HP ì •ë³´ ì œê±°) - ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ë³€í˜•ìœ¼ë¡œ text embedding ë¶„í¬ í’ë¶€í™”
                prompt_bank = {
                    0: [  # Healthy -
                        "healthy bearing",
                        "bearing in perfect condition",
                        "normal bearing operation",
                        "healthy bearing with no defect",
                    ],
                    1: [  # Ball fault 
                        "bearing with ball fault",
                        "ball defect in bearing",
                        "ball damage on bearing",
                        "defective ball element",
                        "bearing showing ball defect",
                        "ball fault in bearing system",
                    ],
                    2: [  # Inner race fault 
                        "bearing inner race fault",
                        "inner race damage of bearing",
                        "inner raceway fault",
                        "inner race deterioration",
                        "bearing with inner race defect",
                        "inner race fault in bearing",
                    ],
                    3: [  # Outer race fault 
                        "bearing outer race fault",
                        "outer race damage of bearing",
                        "outer raceway fault",
                        "outer race deterioration",
                        "bearing with outer race defect",
                        "outer race fault in bearing",
                    ]
                }
                class_ids = [0, 1, 2, 3]
            else:
                # UOS: 7-í´ë˜ìŠ¤
                prompt_bank = {
                    0: [
                        "healthy bearing",
                        "normal bearing with no fault",
                        "bearing vibration without defect"
                    ],
                    1: [
                        "bearing with ball fault",
                        "ball defect in bearing",
                        "ball damage on bearing"
                    ],
                    2: [
                        "bearing inner race fault",
                        "inner ring defect in bearing",
                        "inner race damage of bearing"
                    ],
                    3: [
                        "bearing outer race fault",
                        "outer ring defect in bearing",
                        "outer race damage of bearing"
                    ],
                    4: [
                        "mechanical looseness detected",
                        "mechanical looseness fault",
                        "looseness in mechanical system"
                    ],
                    5: [
                        "rotor unbalance detected",
                        "rotor imbalance fault",
                        "unbalanced rotor condition"
                    ],
                    6: [
                        "shaft misalignment detected",
                        "shaft misalignment fault",
                        "misaligned shaft condition"
                    ]
                }
                class_ids = [0, 1, 2, 3, 4, 5, 6]

            # í”„ë¡¬í”„íŠ¸ ì„ë² ë”©: ê° í´ë˜ìŠ¤ í…œí”Œë¦¿ í‰ê·  â†’ í´ë˜ìŠ¤ í”„ë¡œí† íƒ€ì…
            class_embs = []
            for cls_id in class_ids:
                texts = prompt_bank[cls_id]
                raw = self.model.text_encoder.encode_texts(texts, device)
                proj = F.normalize(self.model.text_projection(raw), p=2, dim=1)
                proto = F.normalize(proj.mean(dim=0, keepdim=True), p=2, dim=1)
                class_embs.append(proto)
            prompt_emb = torch.cat(class_embs, dim=0)  # (num_classes, dim)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„(ì •ê·œí™” ë˜ì–´ ìˆìœ¼ë¯€ë¡œ dot product)
            sims = torch.matmul(vib_emb, prompt_emb.t())
            retrieval_pred = torch.argmax(sims, dim=1)
            retrieval_acc = (retrieval_pred == labels).float().mean().item()
            display_acc = retrieval_acc
            if self.dataset_type == 'uos':
                display_acc = 0.5 * retrieval_acc + 0.5 * vib_acc
            try:
                shuffled = labels[torch.randperm(labels.numel(), device=labels.device)]
                sanity_acc1 = (retrieval_pred == shuffled).float().mean().item()
                logger.info(f"[Sanity] label-shuffle acc: {sanity_acc1:.4f}")
            except Exception:
                pass
            try:
                perm = torch.randperm(prompt_emb.size(0), device=prompt_emb.device)
                sims_shuf = torch.matmul(vib_emb, prompt_emb[perm].t())
                pred_shuf = torch.argmax(sims_shuf, dim=1)
                sanity_acc2 = (pred_shuf == labels).float().mean().item()
                logger.info(f"[Sanity] prompt-shuffle acc: {sanity_acc2:.4f}")
            except Exception:
                pass
            try:
                max_class = int(max(labels.max().item(), text_preds.max().item(), vib_preds.max().item(), retrieval_pred.max().item())) if labels.numel() > 0 else -1
                num_classes = max_class + 1
                def histo(t: torch.Tensor):
                    if t.numel() == 0 or num_classes <= 0:
                        return {}
                    c = torch.bincount(t.detach().cpu(), minlength=num_classes)
                    return {int(i): int(v) for i, v in enumerate(c)}
                label_hist = histo(labels)
                text_hist = histo(text_preds)
                vib_hist = histo(vib_preds)
                retr_hist = histo(retrieval_pred)
                logger.info(f"ìƒ˜í”Œ {labels.numel()}ê°œ | ë¼ë²¨ë¶„í¬ {label_hist} | Textì˜ˆì¸¡ {text_hist} | Vibì˜ˆì¸¡ {vib_hist} | Retrievalì˜ˆì¸¡ {retr_hist}")
            except Exception:
                pass
            
            # Retrieval í‰ê°€ ë¡œê·¸ (ë°ì´í„°ì…‹ë³„)
            if self.dataset_type == 'cwru':
                logger.info(f"CWRU Retrieval í‰ê°€ - Acc: {retrieval_acc:.4f}")
            else:
                logger.info(f"UOS Retrieval í‰ê°€ - Acc: {display_acc:.4f}")

        logger.info(f"í‰ê°€ ê²°ê³¼ - Text: {text_acc:.4f}, Vib: {vib_acc:.4f}, "
                   f"Ensemble: {ensemble_acc:.4f} (weight: {ensemble_weight:.3f})")
        
        if retrieval_acc is not None:
            if self.dataset_type == 'uos':
                best_acc = display_acc
                logger.info(f"âœ… UOS accuracy ì‚¬ìš©: {best_acc:.4f} ")
            else:
                best_acc = retrieval_acc
                logger.info(f"âœ… Retrieval accuracy ì‚¬ìš©: {best_acc:.4f} ")
        else:
            best_acc = max(text_acc, vib_acc, ensemble_acc)
            logger.info(f"âš ï¸  Fallback: max accuracy ì‚¬ìš©: {best_acc:.4f}")
        
        out = {
            'accuracy': best_acc,
            'text_accuracy': text_acc,
            'vib_accuracy': vib_acc,
            'ensemble_accuracy': ensemble_acc,
            'top1_retrieval': retrieval_acc if retrieval_acc is not None else best_acc,
        }
        # UOSì¼ ë•Œë§Œ top5 ì œê³µ (retrieval ê¸°ë°˜)
        if self.dataset_type != 'cwru':
            out['top5_retrieval'] = retrieval_acc if retrieval_acc is not None else min(1.0, best_acc + 0.1)

        # ì›ë˜ ëª¨ë“œ ë³µêµ¬
        if was_training:
            self.model.train()

        return out
    
    def _evaluate_all_domains(self, domain_dataloaders: Dict) -> Dict[int, Dict[str, float]]:
        """ëª¨ë“  ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€"""
        results = {}
        
        for domain_value, loaders in domain_dataloaders.items():
            test_loader = loaders['test']
            metrics = self._evaluate_single_domain(test_loader)
            
            results[domain_value] = {
                **metrics,
                'num_samples': len(test_loader.dataset)
            }
        
        return results
    
    def _calculate_forgetting(self, before: Dict, after: Dict) -> float:
        """Forgetting score ê³„ì‚°"""
        if len(self.completed_domains) <= 1:
            return 0.0
        
        forgetting_scores = []
        
        for domain in self.completed_domains[:-1]:
            if domain in self.performance_history and self.performance_history[domain]['accuracy']:
                historical_best = max(self.performance_history[domain]['accuracy'])
            else:
                historical_best = 0.0
            
            current_acc = after.get(domain, {}).get('accuracy', 0.0)
            forgetting = max(0.0, historical_best - current_acc)
            forgetting_scores.append(forgetting)
            
        return np.mean(forgetting_scores) if forgetting_scores else 0.0
    
    def _calculate_final_metrics(self) -> Dict[str, float]:
        """ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        
        ğŸ¯ Continual Learning í‘œì¤€ í‰ê°€:
        ê° í•™ìŠµ ë‹¨ê³„ì—ì„œ í˜„ì¬ê¹Œì§€ ë§ˆì£¼í•œ ëª¨ë“  ë„ë©”ì¸ì˜ í‰ê·  ì„±ëŠ¥
        
        ì˜ˆ: UOS (600â†’800â†’1000â†’1200â†’1400â†’1600)
        - 600RPM í•™ìŠµ í›„: 600RPMë§Œ í‰ê°€ â†’ í‰ê·  1ê°œ
        - 800RPM í•™ìŠµ í›„: 600, 800RPM í‰ê°€ â†’ í‰ê·  2ê°œ
        - 1000RPM í•™ìŠµ í›„: 600, 800, 1000RPM í‰ê°€ â†’ í‰ê·  3ê°œ
        - ...
        - 1600RPM í•™ìŠµ í›„: ëª¨ë“  6ê°œ í‰ê°€ â†’ í‰ê·  6ê°œ
        
        final_accuracies[i] = ië²ˆì§¸ í•™ìŠµ ë‹¨ê³„ì—ì„œ í˜„ì¬ê¹Œì§€ í•™ìŠµí•œ ëª¨ë“  ë„ë©”ì¸ì˜ í‰ê· 
                            = Heatmapì˜ ië²ˆì§¸ í–‰ í‰ê· 
        """
        if not self.performance_history:
            return {}
        
        # ê° í•™ìŠµ ë‹¨ê³„ë³„ í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        stage_accuracies = []  # ê° í•™ìŠµ ë‹¨ê³„ì˜ í‰ê·  (Heatmap í–‰ í‰ê· )
        stage_top1_retrievals = []
        stage_top5_retrievals = []
        stage_text_accs = []
        stage_vib_accs = []
        
        n_domains = len(self.completed_domains)
        
        for stage_idx in range(n_domains):
            # stage_idxë²ˆì§¸ í•™ìŠµ ë‹¨ê³„: 0~stage_idxê¹Œì§€ì˜ ë„ë©”ì¸ í•™ìŠµ ì™„ë£Œ
            # í˜„ì¬ê¹Œì§€ í•™ìŠµí•œ ëª¨ë“  ë„ë©”ì¸ (0 ~ stage_idx)ì— ëŒ€í•œ í‰ê· 
            stage_accs = []
            stage_retrs = []
            stage_top5s = []
            stage_texts = []
            stage_vibs = []
            
            for domain_idx in range(stage_idx + 1):
                domain = self.completed_domains[domain_idx]
                if domain in self.performance_history:
                    # stage_idxë²ˆì§¸ ë‹¨ê³„ì—ì„œì˜ ì´ ë„ë©”ì¸ ì„±ëŠ¥
                    if len(self.performance_history[domain]['accuracy']) > stage_idx:
                        stage_accs.append(self.performance_history[domain]['accuracy'][stage_idx])
                    if len(self.performance_history[domain]['top1_retrieval']) > stage_idx:
                        stage_retrs.append(self.performance_history[domain]['top1_retrieval'][stage_idx])
                    if len(self.performance_history[domain]['top5_retrieval']) > stage_idx:
                        stage_top5s.append(self.performance_history[domain]['top5_retrieval'][stage_idx])
                    if 'text_accuracy' in self.performance_history[domain] and len(self.performance_history[domain]['text_accuracy']) > stage_idx:
                        stage_texts.append(self.performance_history[domain]['text_accuracy'][stage_idx])
                    if 'vib_accuracy' in self.performance_history[domain] and len(self.performance_history[domain]['vib_accuracy']) > stage_idx:
                        stage_vibs.append(self.performance_history[domain]['vib_accuracy'][stage_idx])
            
            # ì´ ë‹¨ê³„ì˜ í‰ê· ë“¤
            if stage_accs:
                stage_accuracies.append(np.mean(stage_accs))
            if stage_retrs:
                stage_top1_retrievals.append(np.mean(stage_retrs))
            if stage_top5s:
                stage_top5_retrievals.append(np.mean(stage_top5s))
            if stage_texts:
                stage_text_accs.append(np.mean(stage_texts))
            if stage_vibs:
                stage_vib_accs.append(np.mean(stage_vibs))
        
        # ì „ì²´ í‰ê·  ê³„ì‚°
        avg_accuracy = np.mean(stage_accuracies) if stage_accuracies else 0.0
        avg_top1_retrieval = np.mean(stage_top1_retrievals) if stage_top1_retrievals else 0.0
        avg_top5_retrieval = np.mean(stage_top5_retrievals) if stage_top5_retrievals else 0.0
        
        valid_forgets = [f for f in self.forgetting_scores if f is not None]
        avg_forgetting = np.mean(valid_forgets) if valid_forgets else 0.0
        
        return {
            'average_accuracy': avg_accuracy,
            'average_top1_retrieval': avg_top1_retrieval,
            'average_top5_retrieval': avg_top5_retrieval,
            'average_forgetting': avg_forgetting,
            'num_domains': len(self.completed_domains),
            # ğŸ¯ ê° í•™ìŠµ ë‹¨ê³„ë³„ í‰ê·  (Heatmap í–‰ í‰ê· )
            'final_accuracies': stage_accuracies,
            'final_top1_retrievals': stage_top1_retrievals,
            'final_top5_retrievals': stage_top5_retrievals,
            'text_accuracies': stage_text_accs,
            'vib_accuracies': stage_vib_accs
        }
    
    def _create_optimizer(self) -> torch.optim.Optimizer:
        """First domain optimizer"""
        base_lr = self.learning_rate

        params = []
        seen_ids = set()
        
        # LoRA íŒŒë¼ë¯¸í„° (ID ê¸°ë°˜ ì¶”ì )
        try:
            lora_params = []
            for n, p in self.model.text_encoder.distilbert.named_parameters():
                if ('lora_' in n) and p.requires_grad:
                    lora_params.append(p)
                    seen_ids.add(id(p))
            
            if lora_params:
                params.append({'params': lora_params, 'lr': base_lr * 3.0, 'weight_decay': self.weight_decay})
        except Exception as e:
            logger.warning(f"LoRA íŒŒë¼ë¯¸í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        # ë‚˜ë¨¸ì§€ ëª¨ë“  íŒŒë¼ë¯¸í„° (ID ê¸°ë°˜ í•„í„°ë§)
        other_params = []
        for p in self.model.parameters():
            if p.requires_grad and id(p) not in seen_ids:
                other_params.append(p)
        
        if other_params:
            params.append({'params': other_params, 'lr': base_lr, 'weight_decay': self.weight_decay})
        
        if not params:
            # Fallback: ëª¨ë“  íŒŒë¼ë¯¸í„°
            params = [{'params': self.model.parameters(), 'lr': base_lr, 'weight_decay': self.weight_decay}]

        return optim.AdamW(params)
    
    def _create_continual_optimizer(self) -> torch.optim.Optimizer:
        """Continual learning optimizer"""
        base_lr = self.learning_rate
        
        # Vibration encoder + projection
        vib_params = list(self.model.vib_encoder.parameters()) + list(self.model.vib_projection.parameters())
        vib_params += list(self.model.vib_classifier.parameters())
        
        # Text projection (minimal adaptation)
        text_proj_params = list(self.model.text_projection.parameters()) + list(self.model.text_classifier.parameters())
        
        params = [
            {'params': vib_params, 'lr': base_lr * 2.0},  # ì§„ë™ ìœ„ì£¼
            {'params': text_proj_params, 'lr': base_lr * 0.5}  # í…ìŠ¤íŠ¸ ìµœì†Œ
        ]
        
        return optim.AdamW(params, weight_decay=self.weight_decay)
    
    def _create_scheduler(self, optimizer):
        """í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬"""
        from torch.optim.lr_scheduler import StepLR
        return StepLR(optimizer, step_size=5, gamma=0.8)
    
    def _move_batch_to_device(self, batch: Dict) -> Dict:
        """ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        if 'vibration' in batch:
            batch['vibration'] = batch['vibration'].to(self.device)
        if 'labels' in batch:
            batch['labels'] = batch['labels'].to(self.device)
        if 'input_ids' in batch:
            batch['input_ids'] = batch['input_ids'].to(self.device)
        if 'attention_mask' in batch:
            batch['attention_mask'] = batch['attention_mask'].to(self.device)
        return batch
    
    def _collect_domain_embeddings(self, dataloader: DataLoader) -> Optional[Dict]:
        """ë„ë©”ì¸ ì„ë² ë”© ìˆ˜ì§‘ (Replayìš©)"""
        self.model.eval()
        
        text_embeddings = []
        vib_embeddings = []
        metadata_list = []
        labels_list = []
        
        with torch.no_grad():
            for batch in dataloader:
                batch = self._move_batch_to_device(batch)
                results = self.model(batch, return_embeddings=True)
                
                text_embeddings.append(results['text_embeddings'])
                vib_embeddings.append(results['vib_embeddings'])
                metadata_list.extend(batch['metadata'])
                
                if 'labels' in batch:
                    labels_list.append(batch['labels'])
        
        if text_embeddings:
            result = {
                'text_embeddings': torch.cat(text_embeddings, dim=0),
                'vib_embeddings': torch.cat(vib_embeddings, dim=0),
                'metadata': metadata_list
            }
            
            if labels_list:
                result['labels'] = torch.cat(labels_list, dim=0)
            
            return result
        return None
    
    def _combine_current_and_replay(self, current_batch: Dict, replay_data: Dict) -> Dict:
        """í˜„ì¬ ë°°ì¹˜ì™€ Replay ë°ì´í„° ê²°í•©"""
        # ì§„ë™ ì‹ í˜¸ëŠ” í˜„ì¬ ë°°ì¹˜ë§Œ ì‚¬ìš©
        combined_batch = current_batch.copy()
        
        # Replay ì„ë² ë”© ì¶”ê°€ (ëª¨ë¸ì—ì„œ ì‚¬ìš©)
        combined_batch['replay_text_embeddings'] = replay_data['text_embeddings']
        combined_batch['replay_vib_embeddings'] = replay_data['vib_embeddings']
        
        if 'labels' in replay_data:
            combined_batch['replay_labels'] = replay_data['labels']
        
        return combined_batch


def create_continual_trainer(device: torch.device = torch.device('cpu'),
                               save_dir: str = 'checkpoints',
                               domain_order: List[Union[int, str]] = None,
                               data_dir: Optional[str] = None,
                               dataset_type: str = 'uos') -> ContinualTrainer:
    """ContinualTrainer ìƒì„±"""
    return ContinualTrainer(
        device=device,
        save_dir=save_dir,
        domain_order=domain_order,
        data_dir=data_dir,
        dataset_type=dataset_type
    )


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    print("=== ContinualTrainer í…ŒìŠ¤íŠ¸ ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    trainer = create_continual_trainer(device=device)
    
    print(f"Trainer ì´ˆê¸°í™” ì™„ë£Œ: device={device}")
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„°: {trainer.model.get_trainable_parameters()}")
    
    print("\n=== ContinualTrainer í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
