"""
Continual Trainer: TextVibCLIP Continual Learning íŒŒì´í”„ë¼ì¸
Domainë³„ ìˆœì°¨ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€ ê´€ë¦¬
"""

import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
import numpy as np
from collections import defaultdict, Counter
import matplotlib.pyplot as plt

from .textvib_model import TextVibCLIP, create_textvib_model
from .replay_buffer import ReplayBuffer
from .data_loader import create_domain_dataloaders, create_combined_dataloader, create_first_domain_dataloader
from .utils import setup_amp_and_scaler
from configs.model_config import TRAINING_CONFIG, DATA_CONFIG, EVAL_CONFIG, MODEL_CONFIG

logger = logging.getLogger(__name__)


class ContinualTrainer:
    """
    TextVibCLIP Continual Learning Trainer
    
    Domainë³„ ìˆœì°¨ í•™ìŠµ, Replay mechanism, ì„±ëŠ¥ í‰ê°€ ê´€ë¦¬
    """
    
    def __init__(self,
                 model: Optional[TextVibCLIP] = None,
                 device: torch.device = torch.device('cpu'),
                 save_dir: str = 'checkpoints',
                 use_amp: bool = True,
                 max_grad_norm: float = 0.1,
                 domain_order: List[Union[int, str]] = None,
                 data_dir: Optional[str] = None,
                 dataset_type: str = DATA_CONFIG.get('dataset_type', 'uos'),
                 patience: Optional[int] = None):
        """
        Args:
            model (TextVibCLIP, optional): ì‚¬ì „ ì´ˆê¸°í™”ëœ ëª¨ë¸
            device (torch.device): í•™ìŠµ ë””ë°”ì´ìŠ¤
            save_dir (str): ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
            use_amp (bool): AMP ì‚¬ìš© ì—¬ë¶€
            max_grad_norm (float): Gradient clipping ìµœëŒ€ norm
            domain_order (List[Union[int, str]]): ë„ë©”ì¸ ìˆœì„œ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        self.device = device
        self.save_dir = save_dir
        self.max_grad_norm = max_grad_norm
        os.makedirs(save_dir, exist_ok=True)
        
        # AMP ì„¤ì •
        self.scaler, self.use_amp = setup_amp_and_scaler(device, use_amp)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        if model is None:
            self.model = create_textvib_model('first_domain')
        else:
            self.model = model
        
        self.model.to(device)
        
        # Replay buffer
        self.replay_buffer = ReplayBuffer()
        
        # í•™ìŠµ ìƒíƒœ ê´€ë¦¬
        self.current_domain_idx = 0
        self.completed_domains = []
        self.domain_order = domain_order if domain_order is not None else DATA_CONFIG['domain_order']
        # ë°ì´í„° ì„¤ì • (í‰ê°€ ì‹œ ì¼ê´€ì„± ìœ ì§€)
        self.data_dir = data_dir if data_dir is not None else DATA_CONFIG['data_dir']
        self.dataset_type = dataset_type
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_history = defaultdict(list)  # {domain: [accuracy_list]}
        self.loss_history = defaultdict(list)
        self.forgetting_scores = []
        
        # í•™ìŠµ ì„¤ì •
        self.batch_size = TRAINING_CONFIG['batch_size']
        self.num_epochs = TRAINING_CONFIG['num_epochs']
        self.learning_rate = TRAINING_CONFIG['learning_rate']
        self.weight_decay = TRAINING_CONFIG['weight_decay']
        self.replay_ratio = TRAINING_CONFIG['replay_ratio']
        self.grad_accum_steps = int(TRAINING_CONFIG.get('grad_accum_steps', 1))
        self.patience = int(patience) if patience is not None else int(TRAINING_CONFIG.get('patience', 10))
        
        logger.info(f"ContinualTrainer ì´ˆê¸°í™” ì™„ë£Œ: device={device}")
    
    def train_first_domain(self, 
                         first_domain_dataloader: Optional[DataLoader] = None,
                         num_epochs: int = None) -> Dict[str, float]:
        """
        ì²« ë²ˆì§¸ ë„ë©”ì¸(600 RPM) í•™ìŠµ
        í…ìŠ¤íŠ¸-ì§„ë™ ì •ë ¬ì„ ìœ„í•œ ì´ˆê¸° í•™ìŠµ ë‹¨ê³„
        
        Args:
            first_domain_dataloader (DataLoader, optional): ì²« ë²ˆì§¸ ë„ë©”ì¸ ë°ì´í„°ë¡œë”
            num_epochs (int, optional): í•™ìŠµ ì—í¬í¬ ìˆ˜
            
        Returns:
            Dict[str, float]: í•™ìŠµ ê²°ê³¼ ë©”íŠ¸ë¦­
        """
        logger.info("=== First Domain Training ì‹œì‘ (600 RPM) ===")
        
        # ë°ì´í„°ë¡œë” ì¤€ë¹„
        if first_domain_dataloader is None:
            first_domain_dataloader = create_first_domain_dataloader(subset='train', batch_size=self.batch_size)
        
        if num_epochs is None:
            num_epochs = self.num_epochs
        
        # ëª¨ë¸ì„ ì²« ë²ˆì§¸ ë„ë©”ì¸ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
        self.model.switch_to_first_domain_mode()  # First domain mode (LoRA í™œì„±í™”)
        
        # Optimizer ì„¤ì • (Text LoRA + Vibration full)
        optimizer = self._create_optimizer()
        scheduler = self._create_scheduler(optimizer, len(first_domain_dataloader) * num_epochs)
        
        # ë””ë²„ê·¸/ëª¨ë‹ˆí„°ë§ìš©: ê·¸ë¼ë””ì–¸íŠ¸ ë…¸ë¦„ ê¸°ë¡ ë²„í¼
        if not hasattr(self, 'debug_grad_norms'):
            self.debug_grad_norms = []  # ê° í•­ëª©: {'step': int, 'text_lora': float, 'vib': float}

        # í•™ìŠµ ë£¨í”„
        self.model.train()
        epoch_losses = []
        
        for epoch in range(num_epochs):
            # ì²« ë„ë©”ì¸ ì˜¨ë„ ìŠ¤ì¼€ì¤„(ì„ í˜•): init -> final
            inf_cfg = MODEL_CONFIG.get('infonce', {})
            t_text_init = float(inf_cfg.get('first_domain_temperature_text_init',
                                            inf_cfg.get('first_domain_temperature_text', 0.10)))
            t_text_final = float(inf_cfg.get('first_domain_temperature_text_final',
                                             inf_cfg.get('first_domain_temperature_text', 0.10)))
            t_vib_init  = float(inf_cfg.get('first_domain_temperature_vib_init',
                                            inf_cfg.get('first_domain_temperature_vib', 0.10)))
            t_vib_final = float(inf_cfg.get('first_domain_temperature_vib_final',
                                            inf_cfg.get('first_domain_temperature_vib', 0.10)))
            if num_epochs > 1:
                ratio = epoch / (num_epochs - 1)
            else:
                ratio = 1.0
            t_text = t_text_init + (t_text_final - t_text_init) * ratio
            t_vib  = t_vib_init  + (t_vib_final  - t_vib_init)  * ratio
            self.model.infonce_loss.update_temperatures(t_text, t_vib)
            if epoch % 5 == 0 or epoch == 0:
                logger.info(f"[TempSchedule] epoch {epoch+1}/{num_epochs}: Ï„_text={t_text:.3f}, Ï„_vib={t_vib:.3f}")

            epoch_loss = 0.0
            num_batches = 0
            
            for batch_idx, batch in enumerate(first_domain_dataloader):
                # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                batch = self._move_batch_to_device(batch)
                
                # Forward pass
                # grad accumulation: ì‚¬ì´í´ ì‹œì‘ì‹œì—ë§Œ zero_grad
                if (batch_idx % self.grad_accum_steps) == 0:
                    optimizer.zero_grad(set_to_none=True)
                
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        results = self.model(batch)
                        loss = results['loss'] / self.grad_accum_steps
                    
                    # Backward pass with AMP + grad accumulation
                    self.scaler.scale(loss).backward()
                    
                    if (batch_idx + 1) % self.grad_accum_steps == 0:
                        # Gradient clipping ë° grad norm ì¸¡ì • ì¤€ë¹„ë¥¼ ìœ„í•œ unscale (ì‚¬ì´í´ ëì—ì„œë§Œ)
                        if self.max_grad_norm > 0:
                            self.scaler.unscale_(optimizer)
                            # ë””ë²„ê·¸: grad norm ì¸¡ì • (í…ìŠ¤íŠ¸ LoRA, ì§„ë™ ì¸ì½”ë”)
                            try:
                                text_lora_params = [
                                    p for n, p in self.model.text_encoder.distilbert.named_parameters()
                                    if ('lora_' in n) and p.requires_grad and (p.grad is not None)
                                ]
                            except Exception:
                                text_lora_params = []
                            vib_params = [
                                p for p in self.model.vibration_encoder.parameters()
                                if p.requires_grad and (p.grad is not None)
                            ]
                            def _global_grad_norm(params):
                                if not params:
                                    return 0.0
                                import math
                                total = 0.0
                                for p in params:
                                    if p.grad is not None:
                                        param_norm = p.grad.data.float().norm(2).item()
                                        total += param_norm * param_norm
                                return math.sqrt(total)
                            grad_norm_text = _global_grad_norm(text_lora_params)
                            grad_norm_vib = _global_grad_norm(vib_params)
                            if batch_idx % 50 == 0:
                                self.debug_grad_norms.append({
                                    'epoch': epoch + 1,
                                    'batch': batch_idx,
                                    'text_lora': float(grad_norm_text),
                                    'vib': float(grad_norm_vib)
                                })
                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                        self.scaler.step(optimizer)
                        self.scaler.update()
                else:
                    results = self.model(batch)
                    loss = results['loss'] / self.grad_accum_steps
                    
                    # Backward pass
                    loss.backward()
                    
                    if (batch_idx + 1) % self.grad_accum_steps == 0:
                        # Gradient clipping ë° grad norm ì¸¡ì • (ì‚¬ì´í´ ëì—ì„œë§Œ)
                        if self.max_grad_norm > 0:
                            try:
                                text_lora_params = [
                                    p for n, p in self.model.text_encoder.distilbert.named_parameters()
                                    if ('lora_' in n) and p.requires_grad and (p.grad is not None)
                                ]
                            except Exception:
                                text_lora_params = []
                            vib_params = [
                                p for p in self.model.vibration_encoder.parameters()
                                if p.requires_grad and (p.grad is not None)
                            ]
                            def _global_grad_norm(params):
                                if not params:
                                    return 0.0
                                import math
                                total = 0.0
                                for p in params:
                                    if p.grad is not None:
                                        param_norm = p.grad.data.float().norm(2).item()
                                        total += param_norm * param_norm
                                return math.sqrt(total)
                            grad_norm_text = _global_grad_norm(text_lora_params)
                            grad_norm_vib = _global_grad_norm(vib_params)
                            if batch_idx % 50 == 0:
                                self.debug_grad_norms.append({
                                    'epoch': epoch + 1,
                                    'batch': batch_idx,
                                    'text_lora': float(grad_norm_text),
                                    'vib': float(grad_norm_vib)
                                })
                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                        optimizer.step()
                
                if (batch_idx + 1) % self.grad_accum_steps == 0:
                    scheduler.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                
                # ë¡œê¹… (ì ì ˆí•œ ê°„ê²©ìœ¼ë¡œ)
                if batch_idx % 50 == 0:
                    logger.info(f"First Domain Epoch {epoch+1}/{num_epochs}, "
                               f"Batch {batch_idx}, Loss: {loss.item():.4f}")
            
            avg_epoch_loss = epoch_loss / num_batches
            epoch_losses.append(avg_epoch_loss)
            
            logger.info(f"First Domain Epoch {epoch+1} ì™„ë£Œ: Avg Loss = {avg_epoch_loss:.4f}")
        
        # ì²« ë²ˆì§¸ ë„ë©”ì¸ìœ¼ë¡œ í‘œì‹œ
        first_domain = self.domain_order[0]
        self.completed_domains.append(first_domain)
        self.loss_history[first_domain] = epoch_losses
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = os.path.join(self.save_dir, 'first_domain_final.pth')
        self.model.save_checkpoint(checkpoint_path, num_epochs, optimizer.state_dict())
        
        # ì „ì²´ ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€
        domain_dataloaders = create_domain_dataloaders(
            data_dir=self.data_dir,
            domain_order=self.domain_order,
            dataset_type=self.dataset_type,
            batch_size=self.batch_size
        )
        first_domain_performance = self._evaluate_all_domains(domain_dataloaders)
        
        # ì„±ëŠ¥ ê¸°ë¡ (ëª¨ë“  ë©”íŠ¸ë¦­ ì €ì¥)
        first_domain_accuracy = 0.0
        for domain, metrics in first_domain_performance.items():
            if domain not in self.performance_history:
                self.performance_history[domain] = {'accuracy': [], 'top1_retrieval': [], 'top5_retrieval': []}
            
            self.performance_history[domain]['accuracy'].append(metrics['accuracy'])
            self.performance_history[domain]['top1_retrieval'].append(metrics.get('top1_retrieval', 0.0))
            self.performance_history[domain]['top5_retrieval'].append(metrics.get('top5_retrieval', 0.0))
            
            # ì²« ë²ˆì§¸ ë„ë©”ì¸ ì •í™•ë„ ê¸°ë¡
            first_domain_accuracy = metrics['accuracy']
            break  # ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œ í™•ì¸
        
        # ğŸš¨ ì¡°ê¸° ì¢…ë£Œ ì²´í¬: ì²« ë²ˆì§¸ ë„ë©”ì¸ ì •í™•ë„ê°€ 80% ë¯¸ë‹¬ ì‹œ ì‹¤í—˜ ì¤‘ë‹¨
        if first_domain_accuracy < 0.80:
            error_msg = f"âŒ ì²« ë²ˆì§¸ ë„ë©”ì¸ ì •í™•ë„ {first_domain_accuracy:.4f} < 0.80 (80%)"
            logger.error(error_msg)
            logger.error("ğŸ›‘ ì‹¤í—˜ ì˜ë¯¸ ì—†ìŒ - ì¡°ê¸° ì¢…ë£Œ!")
            raise RuntimeError(f"First domain accuracy too low: {first_domain_accuracy:.4f} < 0.80")
        else:
            logger.info(f"âœ… ì²« ë²ˆì§¸ ë„ë©”ì¸ ì •í™•ë„ {first_domain_accuracy:.4f} >= 80% - ê³„ì† ì§„í–‰")
        
        logger.info("=== First Domain Training ì™„ë£Œ ===")
        
        # grad norm ìš”ì•½
        grad_text_vals = [d['text_lora'] for d in self.debug_grad_norms] if hasattr(self, 'debug_grad_norms') else []
        grad_vib_vals = [d['vib'] for d in self.debug_grad_norms] if hasattr(self, 'debug_grad_norms') else []
        grad_summary = {
            'text_lora_mean': float(np.mean(grad_text_vals)) if grad_text_vals else 0.0,
            'vib_mean': float(np.mean(grad_vib_vals)) if grad_vib_vals else 0.0,
            'samples': len(self.debug_grad_norms) if hasattr(self, 'debug_grad_norms') else 0
        }

        return {
            'final_loss': epoch_losses[-1],
            'avg_loss': np.mean(epoch_losses),
            'domain_performances': first_domain_performance,
            'grad_norms_summary': grad_summary
        }
    
    def train_remaining_domains(self, domain_dataloaders: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ë‚˜ë¨¸ì§€ ë„ë©”ì¸ë“¤(800~1600 RPM) ìˆœì°¨ í•™ìŠµ
        Replay mechanismì„ í™œìš©í•œ ì ì§„ì  í•™ìŠµ
        
        Args:
            domain_dataloaders (Dict, optional): ë„ë©”ì¸ë³„ ë°ì´í„°ë¡œë”
            
        Returns:
            Dict[str, Any]: í•™ìŠµ ê²°ê³¼ ë° ë©”íŠ¸ë¦­
        """
        logger.info("=== Remaining Domains Training ì‹œì‘ (800~1600 RPM) ===")
        
        # ë°ì´í„°ë¡œë” ì¤€ë¹„
        if domain_dataloaders is None:
            domain_dataloaders = create_domain_dataloaders(batch_size=self.batch_size)
        
        # Continual modeë¡œ ì „í™˜ (Text freeze, Vibration adaptation)
        self.model.switch_to_continual_mode()
        
        remaining_domains_results = {}
        after_performance = {}  # ì´ˆê¸°í™”
        
        # Domain 2ë¶€í„° ìˆœì°¨ í•™ìŠµ (ì²« ë²ˆì§¸ ë„ë©”ì¸ì€ ì´ë¯¸ ì™„ë£Œ)
        for domain_idx in range(1, len(self.domain_order)):
            domain_value = self.domain_order[domain_idx]
            
            logger.info(f"\n--- Domain {domain_value} í•™ìŠµ ì‹œì‘ ---")
            
            # í˜„ì¬ ë„ë©”ì¸ ë°ì´í„°ë¡œë” (í‚¤ ì¡´ì¬ í™•ì¸)
            if domain_value not in domain_dataloaders:
                logger.error(f"ë„ë©”ì¸ {domain_value}ê°€ dataloadersì— ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë„ë©”ì¸: {list(domain_dataloaders.keys())}")
                continue
                
            current_train_loader = domain_dataloaders[domain_value]['train']
            current_val_loader = domain_dataloaders[domain_value]['val']
            
            # ì´ì „ ë„ë©”ì¸ ì„±ëŠ¥ ê¸°ë¡ (Forgetting ê³„ì‚°ìš©) - ê°„ì†Œí™”
            logger.info(f"Domain {domain_value} í•™ìŠµ ì „ ì„±ëŠ¥ í‰ê°€ ìŠ¤í‚µ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)")
            before_performance = {}  # ì„ì‹œë¡œ ë¹„í™œì„±í™”
            
            # í˜„ì¬ ë„ë©”ì¸ í•™ìŠµ
            domain_results = self._train_single_domain(
                domain_value, current_train_loader, current_val_loader
            )
            
            # í•™ìŠµ í›„ ì„±ëŠ¥ í‰ê°€ - ê°„ì†Œí™”
            logger.info(f"Domain {domain_value} í•™ìŠµ í›„ ì„±ëŠ¥ í‰ê°€ (ë¹ ë¥¸ ë²„ì „)")
            after_performance = self._evaluate_all_domains_fast(domain_dataloaders)
            
            # Forgetting ê³„ì‚°
            forgetting_score = self._calculate_forgetting(before_performance, after_performance)
            self.forgetting_scores.append(forgetting_score)
            
                    # ì„±ëŠ¥ ê¸°ë¡ (ëª¨ë“  ë©”íŠ¸ë¦­ ì €ì¥)
        for domain, metrics in after_performance.items():
            if domain not in self.performance_history:
                self.performance_history[domain] = {'accuracy': [], 'top1_retrieval': [], 'top5_retrieval': []}
            
            self.performance_history[domain]['accuracy'].append(metrics['accuracy'])
            self.performance_history[domain]['top1_retrieval'].append(metrics.get('top1_retrieval', 0.0))
            self.performance_history[domain]['top5_retrieval'].append(metrics.get('top5_retrieval', 0.0))
            
            # ë„ë©”ì¸ ì™„ë£Œ í‘œì‹œ
            self.completed_domains.append(domain_value)
            remaining_domains_results[domain_value] = {
                'training_results': domain_results,
                'performance': after_performance,
                'forgetting_score': forgetting_score
            }
            
            logger.info(f"Domain {domain_value} í•™ìŠµ ì™„ë£Œ: "
                       f"Forgetting Score = {forgetting_score:.4f}")
        
        # ìµœì¢… ì„±ëŠ¥ ìš”ì•½
        final_metrics = self._calculate_final_metrics()
        remaining_domains_results['final_metrics'] = final_metrics
        
        logger.info("=== Remaining Domains Training ì™„ë£Œ ===")
        
        return remaining_domains_results
    
    def _train_single_domain(self, 
                           domain_value: Union[int, str],
                           train_loader: DataLoader,
                           val_loader: DataLoader) -> Dict[str, float]:
        """
        ë‹¨ì¼ ë„ë©”ì¸ í•™ìŠµ (Replay í¬í•¨)
        
        Args:
            domain_value (Union[int, str]): ë„ë©”ì¸ ê°’ (RPM ë˜ëŠ” HP)
            train_loader (DataLoader): í˜„ì¬ ë„ë©”ì¸ í•™ìŠµ ë°ì´í„°
            val_loader (DataLoader): í˜„ì¬ ë„ë©”ì¸ ê²€ì¦ ë°ì´í„°
            
        Returns:
            Dict[str, float]: í•™ìŠµ ê²°ê³¼
        """
        # Optimizer (Vibration encoderë§Œ í•™ìŠµ)
        optimizer = self._create_continual_optimizer()
        
        # í˜„ì¬ ë„ë©”ì¸ ì„ë² ë”© ìˆ˜ì§‘ (Replay bufferìš©)
        domain_embeddings = self._collect_domain_embeddings(train_loader)
        
        # Replay bufferì— ì¶”ê°€
        if domain_embeddings:
            self.replay_buffer.add_domain_data(
                domain_value,
                domain_embeddings['text_embeddings'],
                domain_embeddings['vib_embeddings'],
                domain_embeddings['metadata']
            )
        
        # í•™ìŠµ ë£¨í”„
        self.model.train()
        epoch_losses = []
        best_val_acc = 0.0
        patience_counter = 0
        
        for epoch in range(self.num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            for batch_idx, batch in enumerate(train_loader):
                # í˜„ì¬ ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                batch = self._move_batch_to_device(batch)
                current_batch_size = batch['vibration'].size(0)
                
                # Replay ë°ì´í„° ìƒ˜í”Œë§ (ì„±ëŠ¥ ìµœì í™”: ê°„í—ì  ì‚¬ìš©)
                use_replay = (batch_idx % 3 == 0)  # 3ë°°ì¹˜ë§ˆë‹¤ í•œ ë²ˆë§Œ replay ì‚¬ìš©
                
                if use_replay:
                    replay_batch_size = min(int(current_batch_size * self.replay_ratio), 8)  # í¬ê¸° ì œí•œ
                    replay_data = self.replay_buffer.sample_replay_data(
                        replay_batch_size, exclude_current=True, device=self.device
                    )
                    
                    # í˜„ì¬ ë°ì´í„°ì™€ Replay ë°ì´í„° ê²°í•©
                    if replay_data is not None:
                        combined_batch = self._combine_current_and_replay(batch, replay_data)
                    else:
                        combined_batch = batch
                else:
                    combined_batch = batch
                
                # Forward pass
                optimizer.zero_grad()
                
                if self.use_amp:
                    with torch.cuda.amp.autocast():
                        results = self.model(combined_batch)
                        loss = results['loss']
                    
                    # Backward pass with AMP
                    self.scaler.scale(loss).backward()
                    
                    # Gradient clipping
                    if self.max_grad_norm > 0:
                        self.scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                    
                    self.scaler.step(optimizer)
                    self.scaler.update()
                else:
                    results = self.model(combined_batch)
                    loss = results['loss']
                    
                    # Backward pass
                    loss.backward()
                    
                    # Gradient clipping
                    if self.max_grad_norm > 0:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                    
                    optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                
                # ë¡œê¹… (ë” ê°„ê²°í•˜ê²Œ)
                if batch_idx % 20 == 0:
                    replay_info = f"R:{replay_batch_size}" if replay_data else "No-R"
                    logger.info(f"D{domain_value} E{epoch+1} B{batch_idx}: "
                               f"Loss={loss.item():.4f}, {replay_info}")
            
            avg_epoch_loss = epoch_loss / num_batches
            epoch_losses.append(avg_epoch_loss)
            
            # Validation
            val_metrics = self._evaluate_single_domain(val_loader)
            val_acc = val_metrics['accuracy']
            
            logger.info(f"Domain {domain_value} Epoch {epoch+1}: "
                       f"Loss = {avg_epoch_loss:.4f}, Val Acc = {val_acc:.4f}")
            
            # Early stopping
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                
                # Best model ì €ì¥
                checkpoint_path = os.path.join(self.save_dir, f'domain_{domain_value}_best.pth')
                self.model.save_checkpoint(checkpoint_path, epoch, optimizer.state_dict())
            else:
                patience_counter += 1
                
                if patience_counter >= TRAINING_CONFIG['patience']:
                    logger.info(f"Early stopping at epoch {epoch+1}")
                    break
        
        # ë„ë©”ì¸ í•™ìŠµ ì™„ë£Œ
        self.loss_history[domain_value] = epoch_losses
        
        # ì•ˆì „í•œ ê²°ê³¼ ë°˜í™˜ (epoch_lossesê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ)
        final_loss = epoch_losses[-1] if epoch_losses else float('inf')
        num_epochs = len(epoch_losses)
        
        return {
            'final_loss': final_loss,
            'best_val_accuracy': best_val_acc,
            'num_epochs': num_epochs
        }
    
    def _collect_domain_embeddings(self, dataloader: DataLoader) -> Optional[Dict]:
        """í˜„ì¬ ë„ë©”ì¸ ë°ì´í„°ì˜ ì„ë² ë”© ìˆ˜ì§‘"""
        self.model.eval()
        
        text_embeddings = []
        vib_embeddings = []
        metadata_list = []
        
        with torch.no_grad():
            for batch in dataloader:
                batch = self._move_batch_to_device(batch)
                results = self.model(batch, return_embeddings=True)
                
                text_embeddings.append(results['text_embeddings'])
                vib_embeddings.append(results['vib_embeddings'])
                metadata_list.extend(batch['metadata'])
        
        if text_embeddings:
            return {
                'text_embeddings': torch.cat(text_embeddings, dim=0),
                'vib_embeddings': torch.cat(vib_embeddings, dim=0),
                'metadata': metadata_list
            }
        return None
    
    def _combine_current_and_replay(self, current_batch: Dict, replay_data: Dict) -> Dict:
        """í˜„ì¬ ë°°ì¹˜ì™€ Replay ë°ì´í„° ê²°í•©"""
        # ì§„ë™ ì‹ í˜¸ëŠ” í˜„ì¬ ë°°ì¹˜ì—ì„œë§Œ (ReplayëŠ” ì„ë² ë”©ë§Œ ì €ì¥)
        combined_batch = {
            'vibration': current_batch['vibration'],
            'text': current_batch['text']
        }
        
        # Replay ì„ë² ë”©ì„ ëª¨ë¸ì— ì£¼ì…í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„
        # (ì‹¤ì œë¡œëŠ” loss ê³„ì‚° ì‹œ replay embeddings ì‚¬ìš©)
        combined_batch['replay_text_embeddings'] = replay_data['text_embeddings']
        combined_batch['replay_vib_embeddings'] = replay_data['vib_embeddings']
        
        return combined_batch
    
    def _evaluate_single_domain(self, dataloader: DataLoader) -> Dict[str, float]:
        """ë‹¨ì¼ ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€ (retrieval ë©”íŠ¸ë¦­ í¬í•¨)"""
        self.model.eval()
        
        all_text_embeddings = []
        all_vib_embeddings = []
        all_file_idx = []
        
        # DataLoader ì•ˆì „ì„± í™•ë³´ë¥¼ ìœ„í•œ ì¡°ì¹˜
        try:
            with torch.no_grad():
                max_batches = int(EVAL_CONFIG.get('max_full_eval_batches', -1))
                for batch_idx, batch in enumerate(dataloader):
                    # ì„¤ì • ê¸°ë°˜ ë°°ì¹˜ ì œí•œ (ê¸°ë³¸ ë¬´ì œí•œ)
                    if max_batches >= 0 and batch_idx >= max_batches:
                        logger.debug(f"í‰ê°€ ì¤‘ë‹¨: ì„¤ì •ëœ ìµœëŒ€ ë°°ì¹˜ {max_batches} ë„ë‹¬")
                        break
                        
                    batch = self._move_batch_to_device(batch)
                    results = self.model(batch, return_embeddings=True)
                    
                    all_text_embeddings.append(results['text_embeddings'])
                    all_vib_embeddings.append(results['vib_embeddings'])
                    if 'file_idx' in batch:
                        # ë””ë°”ì´ìŠ¤ ì¼ì¹˜ ìœ ì§€ (GPU ìƒì—ì„œ ë°”ë¡œ ì‚¬ìš©)
                        all_file_idx.append(batch['file_idx'])
                    
        except Exception as e:
            logger.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {'accuracy': 0.0, 'top1_retrieval': 0.0, 'top5_retrieval': 0.0}
        
        if not all_text_embeddings:
            return {'accuracy': 0.0, 'top1_retrieval': 0.0, 'top5_retrieval': 0.0}
        
        # ëª¨ë“  ì„ë² ë”© ê²°í•©
        text_emb = torch.cat(all_text_embeddings, dim=0)
        vib_emb = torch.cat(all_vib_embeddings, dim=0)
        file_idx = torch.cat(all_file_idx, dim=0) if all_file_idx else None
        
        # L2 ì •ê·œí™”
        text_emb = F.normalize(text_emb, dim=1)
        vib_emb = F.normalize(vib_emb, dim=1)
        
        # 1. Retrieval ì •í™•ë„ (ë©€í‹°-í¬ì§€í‹°ë¸Œ: ê°™ì€ íŒŒì¼ì€ ëª¨ë‘ ì •ë‹µ ì²˜ë¦¬)
        similarity_matrix = torch.matmul(text_emb, vib_emb.t())  # (N, N)
        
        # ë””ë²„ê·¸: ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ ìœ ì‚¬ë„ ë¶„í¬ í™•ì¸
        if hasattr(self, '_debug_count'):
            self._debug_count += 1
        else:
            self._debug_count = 1
            
        if self._debug_count <= 2:  # ì²˜ìŒ 2ë²ˆë§Œ ë””ë²„ê·¸
            # ì„ë² ë”© í†µê³„ ë¡œê·¸
            try:
                t_mean = text_emb.mean().item(); t_std = text_emb.std(unbiased=False).item()
                v_mean = vib_emb.mean().item(); v_std = vib_emb.std(unbiased=False).item()
                logger.info(f"ğŸ” DEBUG - ì„ë² ë”© í†µê³„ | text mean/std: {t_mean:.4f}/{t_std:.4f}, vib mean/std: {v_mean:.4f}/{v_std:.4f}")
            except Exception:
                pass
            logger.info(f"ğŸ” DEBUG - ë°°ì¹˜ í¬ê¸°: {text_emb.size(0)}")
            logger.info(f"ğŸ” DEBUG - ëŒ€ê°ì„  ìœ ì‚¬ë„ (ì •ë‹µ): {torch.diag(similarity_matrix)[:5].tolist()}")
            logger.info(f"ğŸ” DEBUG - ì²« í–‰ ìœ ì‚¬ë„ (ì „ì²´): {similarity_matrix[0, :5].tolist()}")
            predicted_tmp = torch.argmax(similarity_matrix, dim=1)
            logger.info(f"ğŸ” DEBUG - ìµœëŒ€ ìœ ì‚¬ë„ ì¸ë±ìŠ¤: {predicted_tmp[:10].tolist()}")

            # ì¶”ê°€ ë¬´ê²°ì„± ì²´í¬: ìœ ì‚¬ë„ í†µê³„ ë° argmax ë¶„í¬
            N = similarity_matrix.size(0)
            diag_vals = torch.diag(similarity_matrix)
            diag_mean = float(diag_vals.mean().item())
            diag_std = float(diag_vals.std(unbiased=False).item())
            diag_min = float(diag_vals.min().item())
            diag_max = float(diag_vals.max().item())
            off_mask = ~torch.eye(N, dtype=torch.bool, device=similarity_matrix.device)
            off_vals = similarity_matrix[off_mask]
            off_mean = float(off_vals.mean().item())
            off_std = float(off_vals.std(unbiased=False).item())
            logger.info(
                f"ğŸ” DEBUG - ìœ ì‚¬ë„ í†µê³„ | diag mean/std/min/max: "
                f"{diag_mean:.4f}/{diag_std:.4f}/{diag_min:.4f}/{diag_max:.4f}, "
                f"off mean/std: {off_mean:.4f}/{off_std:.4f}"
            )

            binc = torch.bincount(predicted_tmp, minlength=N)
            topk = min(10, N)
            top_vals, top_idx = torch.topk(binc, k=topk)
            top_pairs = [(int(i), int(v)) for i, v in zip(top_idx.tolist(), top_vals.tolist())]
            logger.info(f"ğŸ” DEBUG - argmax ìƒìœ„ {topk} ì¸ë±ìŠ¤/ë¹ˆë„: {top_pairs}")

            # ì…”í”Œ ë² ì´ìŠ¤ë¼ì¸ (ë©€í‹°-í¬ì§€í‹°ë¸Œ ê¸°ì¤€)
            if file_idx is not None and N >= 2:
                perm = torch.randperm(N, device=text_emb.device)
                sim_shuf = torch.matmul(text_emb, vib_emb[perm].t())
                pred_shuf = torch.argmax(sim_shuf, dim=1)
                top1_shuf = (file_idx[perm][pred_shuf] == file_idx).float().mean().item()
                k_dbg = min(5, N)
                _, topk_shuf = torch.topk(sim_shuf, k=k_dbg, dim=1)
                top5_shuf = (file_idx.unsqueeze(1) == file_idx[perm][topk_shuf]).any(dim=1).float().mean().item()
                logger.info(f"ğŸ” DEBUG - ì…”í”Œ ë² ì´ìŠ¤ë¼ì¸ Top1/Top5: {top1_shuf:.4f}/{top5_shuf:.4f}")
        
        # ê° textì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ vibrationì´ ê°™ì€ íŒŒì¼ì— ì†í•˜ëŠ”ì§€ í™•ì¸
        _, predicted_indices = torch.max(similarity_matrix, dim=1)
        if file_idx is not None:
            same_file_mask = (file_idx[predicted_indices] == file_idx).float()
            retrieval_accuracy = same_file_mask.mean().item()
        else:
            correct_indices = torch.arange(text_emb.size(0), device=text_emb.device)
            retrieval_accuracy = (predicted_indices == correct_indices).float().mean().item()
        
        # 2. Top-K Retrieval ì •í™•ë„ (ë” ì •í™•í•œ ê³„ì‚°)
        # Top-1ì€ ì´ë¯¸ ìœ„ì—ì„œ ê³„ì‚°ë¨ (retrieval_accuracyì™€ ë™ì¼)
        top1_accuracy = retrieval_accuracy
        
        # Top-5 retrieval accuracy (ë©€í‹°-í¬ì§€í‹°ë¸Œ)
        k = min(5, similarity_matrix.size(1))
        if k > 1:
            _, topk_indices = torch.topk(similarity_matrix, k=k, dim=1)
            if file_idx is not None:
                topk_same = (file_idx.unsqueeze(1) == file_idx[topk_indices])
                top5_accuracy = topk_same.any(dim=1).float().mean().item()
            else:
                correct_indices = torch.arange(text_emb.size(0), device=text_emb.device)
                correct_indices_expanded = correct_indices.unsqueeze(1).expand(-1, k)
                top5_accuracy = (topk_indices == correct_indices_expanded).any(dim=1).float().mean().item()
        else:
            top5_accuracy = top1_accuracy  # k=1ì¼ ë•ŒëŠ” top1ê³¼ ë™ì¼
        
        return {
            'accuracy': retrieval_accuracy,  # ì‹¤ì œ retrieval ì •í™•ë„
            'top1_retrieval': top1_accuracy,
            'top5_retrieval': top5_accuracy
        }
    
    def _evaluate_all_domains(self, domain_dataloaders: Dict) -> Dict[int, Dict[str, float]]:
        """ëª¨ë“  ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€"""
        results = {}
        
        for domain_value, loaders in domain_dataloaders.items():
            test_loader = loaders['test']
            metrics = self._evaluate_single_domain(test_loader)
            
            results[domain_value] = {
                **metrics,  # accuracy, top1_retrieval, top5_retrieval
                'num_samples': len(test_loader.dataset)
            }
        
        return results
    
    def _evaluate_all_domains_fast(self, domain_dataloaders: Dict) -> Dict[str, Dict[str, float]]:
        """ë¹ ë¥¸ ë„ë©”ì¸ í‰ê°€ (ì ì€ ë°°ì¹˜ë§Œ ì‚¬ìš©)"""
        results = {}
        
        for domain_value, loaders in domain_dataloaders.items():
            test_loader = loaders['test']
            
            # ì²« 5ë°°ì¹˜ë§Œ í‰ê°€í•˜ì—¬ ë¹ ë¥¸ ê·¼ì‚¬ì¹˜ ê³„ì‚°
            limited_metrics = self._evaluate_single_domain_fast(test_loader)
            
            results[domain_value] = {
                **limited_metrics,
                'num_samples': min(len(test_loader.dataset), 5 * test_loader.batch_size)
            }
        
        return results
    
    def _evaluate_single_domain_fast(self, dataloader: DataLoader) -> Dict[str, float]:
        """ë¹ ë¥¸ ë‹¨ì¼ ë„ë©”ì¸ í‰ê°€ (5ë°°ì¹˜ë§Œ)"""
        self.model.eval()
        
        all_text_embeddings = []
        all_vib_embeddings = []
        all_file_idx = []
        
        try:
            with torch.no_grad():
                max_fast = int(EVAL_CONFIG.get('max_fast_eval_batches', 5))
                for batch_idx, batch in enumerate(dataloader):
                    if batch_idx >= max_fast:
                        break
                        
                    batch = self._move_batch_to_device(batch)
                    results = self.model(batch, return_embeddings=True)
                    
                    all_text_embeddings.append(results['text_embeddings'])
                    all_vib_embeddings.append(results['vib_embeddings'])
                    if 'file_idx' in batch:
                        all_file_idx.append(batch['file_idx'])
                    
        except Exception as e:
            logger.error(f"ë¹ ë¥¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'accuracy': 0.0, 'top1_retrieval': 0.0, 'top5_retrieval': 0.0}
        
        if not all_text_embeddings:
            return {'accuracy': 0.0, 'top1_retrieval': 0.0, 'top5_retrieval': 0.0}
        
        # ì„ë² ë”© ê²°í•© ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        text_emb = torch.cat(all_text_embeddings, dim=0)
        vib_emb = torch.cat(all_vib_embeddings, dim=0)
        file_idx = torch.cat(all_file_idx, dim=0) if all_file_idx else None

        # L2 ì •ê·œí™” í›„ ìœ ì‚¬ë„ ê³„ì‚°
        text_emb = F.normalize(text_emb, dim=1)
        vib_emb = F.normalize(vib_emb, dim=1)
        similarity = torch.matmul(text_emb, vib_emb.t())

        # Top-1 (ë©€í‹°-í¬ì§€í‹°ë¸Œ ì§€ì›)
        _, pred = torch.max(similarity, dim=1)
        if file_idx is not None:
            same_file = (file_idx[pred] == file_idx).float()
            top1 = same_file.mean().item()
        else:
            target = torch.arange(text_emb.size(0), device=text_emb.device)
            top1 = (pred == target).float().mean().item()

        # Top-5 (ë©€í‹°-í¬ì§€í‹°ë¸Œ ì§€ì›)
        k = min(5, similarity.size(1))
        if k > 1:
            _, topk = torch.topk(similarity, k=k, dim=1)
            if file_idx is not None:
                topk_same = (file_idx.unsqueeze(1) == file_idx[topk]).any(dim=1).float().mean().item()
                top5 = topk_same
            else:
                target = torch.arange(text_emb.size(0), device=text_emb.device).unsqueeze(1).expand(-1, k)
                top5 = (topk == target).any(dim=1).float().mean().item()
        else:
            top5 = top1

        # ë¹ ë¥¸ í‰ê°€ì—ì„œë„ ë¬´ê²°ì„± ë””ë²„ê·¸(í•œ ë²ˆë§Œ)
        if not hasattr(self, '_debug_fast_once'):
            self._debug_fast_once = True
            N = similarity.size(0)
            diag_vals = torch.diag(similarity)
            diag_mean = float(diag_vals.mean().item())
            diag_std = float(diag_vals.std(unbiased=False).item())
            off_mask = ~torch.eye(N, dtype=torch.bool, device=similarity.device)
            off_vals = similarity[off_mask]
            off_mean = float(off_vals.mean().item())
            off_std = float(off_vals.std(unbiased=False).item())
            logger.info(
                f"ğŸ” DEBUG(FAST) - N={N}, diag mean/std={diag_mean:.4f}/{diag_std:.4f}, "
                f"off mean/std={off_mean:.4f}/{off_std:.4f}, Top1={top1:.4f}, Top5={top5:.4f}"
            )
            if file_idx is not None and N >= 2:
                perm = torch.randperm(N, device=text_emb.device)
                sim_shuf = torch.matmul(text_emb, vib_emb[perm].t())
                pred_shuf = torch.argmax(sim_shuf, dim=1)
                top1_shuf = (file_idx[perm][pred_shuf] == file_idx).float().mean().item()
                k_dbg = min(5, N)
                _, topk_shuf = torch.topk(sim_shuf, k=k_dbg, dim=1)
                top5_shuf = (file_idx.unsqueeze(1) == file_idx[perm][topk_shuf]).any(dim=1).float().mean().item()
                logger.info(f"ğŸ” DEBUG(FAST) - ì…”í”Œ ë² ì´ìŠ¤ë¼ì¸ Top1/Top5: {top1_shuf:.4f}/{top5_shuf:.4f}")

        return {
            'accuracy': top1,
            'top1_retrieval': top1,
            'top5_retrieval': top5
        }
    
    def _calculate_forgetting(self, before: Dict, after: Dict) -> float:
        """Forgetting score ê³„ì‚°"""
        if len(self.completed_domains) <= 1:
            return 0.0
        
        forgetting_scores = []
        for domain in self.completed_domains[:-1]:  # ë§ˆì§€ë§‰ ë„ë©”ì¸ ì œì™¸
            before_acc = before.get(domain, {}).get('accuracy', 0.0)
            after_acc = after.get(domain, {}).get('accuracy', 0.0)
            forgetting = max(0.0, before_acc - after_acc)
            forgetting_scores.append(forgetting)
        
        return np.mean(forgetting_scores) if forgetting_scores else 0.0
    
    def _calculate_final_metrics(self) -> Dict[str, float]:
        """ìµœì¢… Continual Learning ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not self.performance_history:
            return {}
        
        # Average metrics (ë§ˆì§€ë§‰ ì„±ëŠ¥)
        final_accuracies = []
        final_top1_retrievals = []
        final_top5_retrievals = []
        
        for domain in self.completed_domains:
            if domain in self.performance_history:
                if self.performance_history[domain]['accuracy']:
                    final_accuracies.append(self.performance_history[domain]['accuracy'][-1])
                if self.performance_history[domain]['top1_retrieval']:
                    final_top1_retrievals.append(self.performance_history[domain]['top1_retrieval'][-1])
                if self.performance_history[domain]['top5_retrieval']:
                    final_top5_retrievals.append(self.performance_history[domain]['top5_retrieval'][-1])
        
        avg_accuracy = np.mean(final_accuracies) if final_accuracies else 0.0
        avg_top1_retrieval = np.mean(final_top1_retrievals) if final_top1_retrievals else 0.0
        avg_top5_retrieval = np.mean(final_top5_retrievals) if final_top5_retrievals else 0.0
        
        # Average Forgetting
        avg_forgetting = np.mean(self.forgetting_scores) if self.forgetting_scores else 0.0
        
        return {
            'average_accuracy': avg_accuracy,
            'average_top1_retrieval': avg_top1_retrieval,
            'average_top5_retrieval': avg_top5_retrieval,
            'average_forgetting': avg_forgetting,
            'num_domains': len(self.completed_domains),
            'final_accuracies': final_accuracies,
            'final_top1_retrievals': final_top1_retrievals,
            'final_top5_retrievals': final_top5_retrievals
        }
    
    def _create_optimizer(self) -> torch.optim.Optimizer:
        """First domain trainingìš© optimizer ìƒì„± (íŒŒë¼ë¯¸í„° ê·¸ë£¹ ë¶„ë¦¬)"""
        base_lr = self.learning_rate
        lora_mult = float(TRAINING_CONFIG.get('lora_lr_mult', 3.0))
        proj_mult = float(TRAINING_CONFIG.get('proj_lr_mult', 3.0))
        vib_mult  = float(TRAINING_CONFIG.get('vib_lr_mult', 1.0))

        params = []
        seen = set()

        # Text LoRA íŒŒë¼ë¯¸í„° ê·¸ë£¹
        try:
            lora_params = [p for n, p in self.model.text_encoder.distilbert.named_parameters()
                           if ('lora_' in n) and p.requires_grad]
            if lora_params:
                params.append({'params': lora_params, 'lr': base_lr * lora_mult, 'weight_decay': self.weight_decay})
                for p in lora_params:
                    seen.add(id(p))
        except Exception:
            pass

        # Text projection íŒŒë¼ë¯¸í„° ê·¸ë£¹
        if hasattr(self.model.text_encoder, 'projection'):
            proj_params = [p for p in self.model.text_encoder.projection.parameters() if p.requires_grad]
            if proj_params:
                params.append({'params': proj_params, 'lr': base_lr * proj_mult, 'weight_decay': self.weight_decay})
                for p in proj_params:
                    seen.add(id(p))

        # Vibration encoder íŒŒë¼ë¯¸í„° ê·¸ë£¹
        vib_params = [p for p in self.model.vibration_encoder.parameters() if p.requires_grad]
        vib_params = [p for p in vib_params if id(p) not in seen]
        if vib_params:
            params.append({'params': vib_params, 'lr': base_lr * vib_mult, 'weight_decay': self.weight_decay})
            for p in vib_params:
                seen.add(id(p))

        # ëˆ„ë½ íŒŒë¼ë¯¸í„° ë³´ì™„
        remain = [p for p in self.model.parameters() if p.requires_grad and id(p) not in seen]
        if remain:
            params.append({'params': remain, 'lr': base_lr, 'weight_decay': self.weight_decay})

        return optim.AdamW(params)
    
    def _create_scheduler(self, optimizer, total_steps):
        """í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± (ë‹¨ì¼ êµ¬í˜„)"""
        from torch.optim.lr_scheduler import CosineAnnealingLR
        return CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)
    
    def _create_continual_optimizer(self) -> torch.optim.Optimizer:
        """Continual learningìš© optimizer ìƒì„± (Vibration encoderë§Œ)"""
        return optim.AdamW(
            self.model.vibration_encoder.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )
    
    def _create_scheduler(self, optimizer: torch.optim.Optimizer, total_steps: int):
        """í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± (ë‹¨ì¼ êµ¬í˜„)"""
        from torch.optim.lr_scheduler import CosineAnnealingLR
        return CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)
    
    def _move_batch_to_device(self, batch: Dict) -> Dict:
        """ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        if 'vibration' in batch:
            batch['vibration'] = batch['vibration'].to(self.device)
        if 'labels' in batch:
            batch['labels'] = batch['labels'].to(self.device)
        if 'file_idx' in batch:
            batch['file_idx'] = batch['file_idx'].to(self.device)
        return batch
    
    def save_training_history(self, path: str):
        """í•™ìŠµ ì´ë ¥ ì €ì¥"""
        history = {
            'performance_history': dict(self.performance_history),
            'loss_history': dict(self.loss_history),
            'forgetting_scores': self.forgetting_scores,
            'completed_domains': self.completed_domains,
            'domain_order': self.domain_order
        }
        
        torch.save(history, path)
        logger.info(f"í•™ìŠµ ì´ë ¥ ì €ì¥ë¨: {path}")
    
    def plot_continual_learning_curves(self, save_path: Optional[str] = None):
        """Continual learning ê²°ê³¼ ì‹œê°í™”"""
        if not self.performance_history:
            logger.warning("ì‹œê°í™”í•  ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŒ")
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. Domainë³„ ì„±ëŠ¥ ë³€í™”
        for domain in self.completed_domains:
            if domain in self.performance_history:
                ax1.plot(self.performance_history[domain], label=f'Domain {domain}')
        ax1.set_xlabel('Learning Phase')
        ax1.set_ylabel('Accuracy')
        ax1.set_title('Performance Evolution by Domain')
        ax1.legend()
        ax1.grid(True)
        
        # 2. Forgetting scores
        if self.forgetting_scores:
            ax2.plot(self.forgetting_scores, 'r-o')
            ax2.set_xlabel('Domain')
            ax2.set_ylabel('Forgetting Score')
            ax2.set_title('Catastrophic Forgetting')
            ax2.grid(True)
        
        # 3. ìµœì¢… ì„±ëŠ¥ ë¹„êµ
        final_accs = [self.performance_history[d][-1] if d in self.performance_history else 0
                     for d in self.completed_domains]
        ax3.bar(range(len(self.completed_domains)), final_accs)
        ax3.set_xlabel('Domain')
        ax3.set_ylabel('Final Accuracy')
        ax3.set_title('Final Performance by Domain')
        ax3.set_xticks(range(len(self.completed_domains)))
        ax3.set_xticklabels([str(d) for d in self.completed_domains])
        
        # 4. Loss curves
        for domain in self.completed_domains:
            if domain in self.loss_history:
                ax4.plot(self.loss_history[domain], label=f'Domain {domain}')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Loss')
        ax4.set_title('Training Loss by Domain')
        ax4.legend()
        ax4.grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"í•™ìŠµ ê³¡ì„  ì €ì¥ë¨: {save_path}")
        else:
            plt.show()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    print("=== ContinualTrainer í…ŒìŠ¤íŠ¸ ===")
    
    # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPU ì‚¬ìš©
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Trainer ìƒì„±
    trainer = ContinualTrainer(device=device)
    
    print(f"Trainer ì´ˆê¸°í™” ì™„ë£Œ: device={device}")
    print(f"ë„ë©”ì¸ ìˆœì„œ: {trainer.domain_order}")
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„°: {trainer.model.get_trainable_parameters()}")
    
    print("\n=== ContinualTrainer í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
