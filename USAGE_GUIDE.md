# TextVibCLIP ì‚¬ìš© ê°€ì´ë“œ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch transformers peft scikit-learn matplotlib seaborn tqdm
```

### 2. ë°ì´í„° ì¤€ë¹„

UOS ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•œ í›„, ì‹¤í—˜ìš© ë°ì´í„° ì¤€ë¹„:

```bash
python prepare_uos_scenario1.py
```

### 3. ì „ì²´ ì‹¤í—˜ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì‹¤í—˜ (Joint Training + Continual Learning)
python main.py --experiment_name "my_experiment" --batch_size 32 --joint_epochs 50 --continual_epochs 30

# GPU ì‚¬ìš©
python main.py --device cuda --experiment_name "gpu_experiment"

# ê²°ê³¼ í”Œë¡¯ ì €ì¥
python main.py --experiment_name "experiment_with_plots" --save_plots
```

## ğŸ“‹ ì‹¤í—˜ ëª¨ë“œ

### 1. ì „ì²´ íŒŒì´í”„ë¼ì¸ (ì¶”ì²œ)

```bash
python main.py --mode full --experiment_name "full_pipeline"
```

- Joint Training (Domain 1)
- Continual Learning (Domain 2-6)
- ìë™ ê²°ê³¼ ì €ì¥ ë° ë¶„ì„

### 2. Joint Trainingë§Œ

```bash
python main.py --mode joint_only --experiment_name "joint_only"
# ë˜ëŠ”
python experiments/run_joint_training.py
```

### 3. Continual Learningë§Œ

```bash
python main.py --mode continual_only --load_joint_checkpoint checkpoints/joint_model.pth
# ë˜ëŠ”
python experiments/run_continual_learning.py --joint_checkpoint checkpoints/joint_model.pth
```

## âš™ï¸ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

### ëª¨ë¸ ì„¤ì •

```bash
--embedding_dim 512           # ì„ë² ë”© ì°¨ì›
--batch_size 32              # ë°°ì¹˜ í¬ê¸°
--learning_rate 1e-4         # í•™ìŠµë¥ 
```

### Continual Learning ì„¤ì •

```bash
--replay_buffer_size 500     # ë„ë©”ì¸ë‹¹ ì €ì¥í•  ìƒ˜í”Œ ìˆ˜
--replay_ratio 0.3           # Replay ë°ì´í„° ë¹„ìœ¨
--joint_epochs 50            # Joint training ì—í¬í¬
--continual_epochs 30        # Domainë³„ í•™ìŠµ ì—í¬í¬
```

## ğŸ“Š ëª¨ë¸ í‰ê°€

### í•™ìŠµëœ ëª¨ë¸ í‰ê°€

```bash
python experiments/evaluate_model.py \
    --model_checkpoint results/my_experiment_20250101_120000/checkpoints/final_model.pth \
    --subset test \
    --save_results \
    --output_dir evaluation_results
```

### í‰ê°€ ë©”íŠ¸ë¦­

- **Classification Accuracy**: í…ìŠ¤íŠ¸-ì§„ë™ ìŒ ë¶„ë¥˜ ì •í™•ë„
- **Cross-domain Retrieval**: Top-k retrieval ì„±ëŠ¥
- **Embedding Similarity**: í‰ê·  cosine similarity
- **Continual Learning**: Average accuracy, Forgetting rate

## ğŸ“ ì¶œë ¥ êµ¬ì¡°

ì‹¤í—˜ ì™„ë£Œ í›„ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ê²°ê³¼ê°€ ì €ì¥ë©ë‹ˆë‹¤:

```
results/my_experiment_20250101_120000/
â”œâ”€â”€ checkpoints/                 # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ joint_training_final.pth
â”‚   â”œâ”€â”€ domain_600_best.pth
â”‚   â”œâ”€â”€ domain_800_best.pth
â”‚   â””â”€â”€ final_model.pth
â”œâ”€â”€ logs/                        # í•™ìŠµ ë¡œê·¸
â”‚   â””â”€â”€ textvibclip_20250101_120000.log
â”œâ”€â”€ plots/                       # ì‹œê°í™” ê²°ê³¼
â”‚   â””â”€â”€ continual_learning_curves.png
â””â”€â”€ results/                     # ì‹¤í—˜ ê²°ê³¼
    â”œâ”€â”€ training_history.pth
    â”œâ”€â”€ replay_buffer.pth
    â””â”€â”€ experiment_config.pth
```

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. ì»¤ìŠ¤í…€ ì„¤ì •

`configs/model_config.py`ì—ì„œ ëª¨ë¸ êµ¬ì„± ìˆ˜ì •:

```python
MODEL_CONFIG = {
    'embedding_dim': 1024,  # ë” í° ì„ë² ë”©
    'vibration_encoder': {
        'num_layers': 8,    # ë” ê¹Šì€ TST
        # ...
    }
}
```

### 2. ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ì¶”ê°€

```python
# src/data_loader.py í™•ì¥
class CustomDataset(Dataset):
    # ìƒˆë¡œìš´ ë°ì´í„°ì…‹ êµ¬í˜„
    pass
```

### 3. ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
# experiments/ í´ë”ì— ìƒˆë¡œìš´ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
# ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±
```

## ğŸ› ë¬¸ì œ í•´ê²°

### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python main.py --batch_size 16

# Mixed precision ì‚¬ìš© (model_config.pyì—ì„œ ì„¤ì •)
DEVICE_CONFIG['mixed_precision'] = True
```

### 2. ë°ì´í„° ë¡œë”© ì˜¤ë¥˜

```bash
# ë°ì´í„° ê²½ë¡œ í™•ì¸
ls data_scenario1/

# ë°ì´í„° ì¬ì¤€ë¹„
python prepare_uos_scenario1.py
```

### 3. ì˜ì¡´ì„± ì˜¤ë¥˜

```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
pip install --upgrade torch transformers peft

# CUDA ë²„ì „ í™•ì¸ (GPU ì‚¬ìš© ì‹œ)
python -c "import torch; print(torch.cuda.is_available())"
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. í•™ìŠµ ì†ë„ í–¥ìƒ

- GPU ì‚¬ìš©: `--device cuda`
- ë” ë§ì€ ì›Œì»¤: `--num_workers 8`
- Mixed precision í™œì„±í™”

### 2. ë©”ëª¨ë¦¬ ìµœì í™”

- ë°°ì¹˜ í¬ê¸° ì¡°ì •: `--batch_size 16`
- Replay buffer í¬ê¸° ì¡°ì •: `--replay_buffer_size 300`

### 3. ìˆ˜ë ´ ê°œì„ 

- í•™ìŠµë¥  ì¡°ì •: `--learning_rate 5e-5`
- ì—í¬í¬ ìˆ˜ ì¦ê°€: `--joint_epochs 100`

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- **ì—°êµ¬ ë…¼ë¬¸**: `ì—°êµ¬paper.txt` ì°¸ì¡°
- **ëª¨ë¸ êµ¬ì¡°**: `PROJECT_STRUCTURE.md` ì°¸ì¡°
- **API ë¬¸ì„œ**: ê° ëª¨ë“ˆì˜ docstring ì°¸ì¡°

## ğŸ¤ ê¸°ì—¬ ë°©ë²•

1. ìƒˆë¡œìš´ ê¸°ëŠ¥ì´ë‚˜ ë²„ê·¸ ìˆ˜ì •
2. ë¬¸ì„œ ê°œì„ 
3. ì‹¤í—˜ ê²°ê³¼ ê³µìœ 
4. ì½”ë“œ ë¦¬ë·° ë° ìµœì í™” ì œì•ˆ

---

## ğŸ’¡ ì‹¤í—˜ ì˜ˆì‹œ

### ê¸°ë³¸ ì‹¤í—˜

```bash
# 1. ë°ì´í„° ì¤€ë¹„
python prepare_uos_scenario1.py

# 2. ì „ì²´ ì‹¤í—˜ ì‹¤í–‰
python main.py \
    --experiment_name "baseline_experiment" \
    --batch_size 32 \
    --joint_epochs 50 \
    --continual_epochs 30 \
    --replay_buffer_size 500 \
    --save_plots

# 3. ê²°ê³¼ í‰ê°€
python experiments/evaluate_model.py \
    --model_checkpoint results/baseline_experiment_*/checkpoints/final_model.pth \
    --save_results
```

### Ablation Study

```bash
# LoRA rank ë¹„êµ
python main.py --experiment_name "lora_rank_16" # r=16 (ê¸°ë³¸ê°’)
python main.py --experiment_name "lora_rank_64" # configsì—ì„œ r=64ë¡œ ìˆ˜ì •

# Replay buffer í¬ê¸° ë¹„êµ
python main.py --experiment_name "replay_100" --replay_buffer_size 100
python main.py --experiment_name "replay_1000" --replay_buffer_size 1000

# Temperature ì„¤ì • ë¹„êµ (configsì—ì„œ ìˆ˜ì •)
```

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼í•˜ë©´ TextVibCLIPì„ ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
