#!/usr/bin/env python3
"""
CWRU ë°ì´í„° ê²€ì¦ ë° ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „)

ê¸°ëŠ¥:
1. ëœë¤ ìƒ˜í”Œ ì¶”ë¡  ê²€ì¦
2. ë°ì´í„° ë¶„í•  ë° ë¡œë”© ê²€ì¦  
3. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
4. íŒŒì¼ëª… ë° ë©”íƒ€ë°ì´í„° ê²€ì¦
5. ë°ì´í„° ëˆ„ìˆ˜ ê°€ëŠ¥ì„± ì²´í¬
"""

import os
import sys
import argparse
import random
from pathlib import Path
from typing import List, Tuple, Dict
from collections import Counter, defaultdict
import numpy as np

import torch
import torch.nn.functional as F

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(ROOT))

from src.textvib_model import create_textvib_model
from src.data_cache import CachedBearingDataset
from src.data_loader import BearingDataset
from configs.model_config import CWRU_DATA_CONFIG

CWRU_CLASS_ID_TO_NAME = {0: "H", 1: "B", 2: "IR", 3: "OR"}
CWRU_CLASS_ID_TO_TEXT = {
    0: ["healthy bearing", "normal bearing with no fault", "bearing vibration without defect"],
    1: ["bearing with ball fault", "ball defect in bearing", "ball damage on bearing"],
    2: ["bearing inner race fault", "inner ring defect in bearing", "inner race damage of bearing"],
    3: ["bearing outer race fault", "outer ring defect in bearing", "outer race damage of bearing"],
}

def pick_device(arg: str) -> torch.device:
    if arg == "cpu":
        return torch.device("cpu")
    if arg == "cuda":
        return torch.device("cuda")
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

def find_latest_experiment_dir(results_dir: Path, scenario_name: str) -> Path:
    if not results_dir.exists():
        raise FileNotFoundError(f"Results dir not found: {results_dir}")
    candidates = sorted([p for p in results_dir.iterdir() if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)
    for c in candidates:
        ckpt_dir = c / "checkpoints" / scenario_name
        if ckpt_dir.exists():
            return c
    raise FileNotFoundError(f"No experiment dir with checkpoints/{scenario_name} under {results_dir}")

def load_model_for_domain(ckpt_base: Path, domain_value: int, device: torch.device) -> torch.nn.Module:
    # ë„ë©”ì¸ë³„ best ì²´í¬í¬ì¸íŠ¸ ìš°ì„ , ì—†ìœ¼ë©´ first_domain_final.pthë¡œ í´ë°±(0HP)
    candidates = [
        ckpt_base / f"domain_{domain_value}_best.pth",
        ckpt_base / "first_domain_final.pth"
    ]
    ckpt_path = None
    for p in candidates:
        if p.exists():
            ckpt_path = p
            break
    if ckpt_path is None:
        raise FileNotFoundError(f"Checkpoint not found under {ckpt_base}")
    model = create_textvib_model(domain_stage="continual", dataset_type="cwru").to(device)
    model.load_checkpoint(str(ckpt_path), device=device)
    model.eval()
    return model

def build_class_prototypes(model: torch.nn.Module, device: torch.device) -> torch.Tensor:
    # ê° í´ë˜ìŠ¤ì˜ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í‰ê·  â†’ í”„ë¡œí† íƒ€ì… (4, dim)
    class_embs = []
    for cls_id in [0, 1, 2, 3]:
        texts = CWRU_CLASS_ID_TO_TEXT[cls_id]
        raw = model.text_encoder.encode_texts(texts, device)
        proj = F.normalize(model.text_projection(raw), p=2, dim=1)
        proto = F.normalize(proj.mean(dim=0, keepdim=True), p=2, dim=1)
        class_embs.append(proto)
    return torch.cat(class_embs, dim=0)  # (4, dim)

def sample_indices(n_total: int, k: int, seed: int = 42) -> List[int]:
    k = max(1, min(k, n_total))
    rng = random.Random(seed)
    return rng.sample(range(n_total), k)

def verify_data_splitting(data_dir: Path, domain_value: int) -> None:
    """ë°ì´í„° ë¶„í•  ë° íŒŒì¼ êµ¬ì¡° ê²€ì¦"""
    print(f"\nğŸ” === ë°ì´í„° ë¶„í•  ê²€ì¦: {domain_value}HP ===")
    
    domain_dir = data_dir / f"Load_{domain_value}hp"
    if not domain_dir.exists():
        print(f"âŒ ë„ë©”ì¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {domain_dir}")
        return
    
    # íŒŒì¼ êµ¬ì¡° í™•ì¸
    classes = ['H', 'B', 'IR', 'OR']
    subsets = ['train', 'val', 'test']
    
    print("ğŸ“ íŒŒì¼ êµ¬ì¡°:")
    for cls in classes:
        print(f"  {cls}: ", end="")
        for subset in subsets:
            fname = f"{cls}_{domain_value}hp_{subset}_01.mat"
            fpath = domain_dir / fname
            if fpath.exists():
                print(f"{subset}âœ“ ", end="")
            else:
                print(f"{subset}âœ— ", end="")
        print()
    
    # ì›ë³¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„í•  ê²€ì¦
    try:
        print("\nğŸ“Š ì›ë³¸ ë°ì´í„°ì…‹ ë¶„í•  ê²€ì¦:")
        for subset in subsets:
            ds = BearingDataset(
                data_dir=str(data_dir),
                dataset_type="cwru", 
                domain_value=domain_value,
                subset=subset
            )
            
            if len(ds.file_paths) > 0:
                # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
                labels = [ds.metadata_list[i]['bearing_condition'] for i in range(len(ds.metadata_list))]
                label_counts = Counter(labels)
                print(f"  {subset}: {len(ds.file_paths)}ê°œ íŒŒì¼, í´ë˜ìŠ¤ ë¶„í¬: {dict(label_counts)}")
                
                # íŒŒì¼ëª… í™•ì¸
                file_names = [os.path.basename(f) for f in ds.file_paths]
                print(f"    íŒŒì¼ë“¤: {file_names}")
            else:
                print(f"  {subset}: íŒŒì¼ ì—†ìŒ")
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤íŒ¨: {e}")


def verify_cached_dataset(data_dir: Path, domain_value: int) -> None:
    """ìºì‹œëœ ë°ì´í„°ì…‹ ê²€ì¦"""
    print(f"\nğŸ’¾ === ìºì‹œ ë°ì´í„°ì…‹ ê²€ì¦: {domain_value}HP ===")
    
    try:
        test_ds = CachedBearingDataset(
            data_dir=str(data_dir),
            dataset_type="cwru",
            domain_value=domain_value,
            subset="test"
        )
        
        print(f"ğŸ“Š ìºì‹œëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_ds)}ê°œ ìƒ˜í”Œ")
        
        if len(test_ds) > 0:
            # ì²« ëª‡ ê°œ ìƒ˜í”Œì˜ ë©”íƒ€ë°ì´í„° í™•ì¸
            print("ğŸ” ìƒ˜í”Œ ë©”íƒ€ë°ì´í„°:")
            for i in range(min(5, len(test_ds))):
                sample = test_ds[i]
                meta = sample.get("metadata", {})
                lbl = sample["labels"]
                lbl_id = int(lbl[0].item()) if lbl.ndim > 0 else int(lbl.item())
                
                print(f"  ìƒ˜í”Œ {i}: í´ë˜ìŠ¤={CWRU_CLASS_ID_TO_NAME[lbl_id]}, "
                      f"íŒŒì¼={os.path.basename(meta.get('filepath', 'unknown'))}, "
                      f"ìœˆë„ìš°={sample.get('window_idx', -1)}")
        
        # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
        class_counts = defaultdict(int)
        for i in range(len(test_ds)):
            sample = test_ds[i]
            lbl = sample["labels"]
            lbl_id = int(lbl[0].item()) if lbl.ndim > 0 else int(lbl.item())
            class_counts[lbl_id] += 1
        
        print(f"ğŸ“ˆ í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜: {dict(class_counts)}")
        
    except Exception as e:
        print(f"âŒ ìºì‹œ ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤íŒ¨: {e}")


def run_comprehensive_verify(experiment_dir: Path, samples_per_domain: int, device: torch.device, domains: List[int]) -> None:
    """ì¢…í•© ê²€ì¦ ì‹¤í–‰"""
    scenario_name = "CWRU_Scenario2_VaryingLoad"
    ckpt_base = experiment_dir / "checkpoints" / scenario_name
    if not ckpt_base.exists():
        raise FileNotFoundError(f"Checkpoint base not found: {ckpt_base}")

    data_dir = Path(CWRU_DATA_CONFIG["data_dir"])
    
    # 1. ë°ì´í„° ë¶„í•  ê²€ì¦
    for domain_value in domains:
        verify_data_splitting(data_dir, domain_value)
        verify_cached_dataset(data_dir, domain_value)
    
    # 2. ëª¨ë¸ ì¶”ë¡  ê²€ì¦
    print(f"\nğŸ¤– === ëª¨ë¸ ì¶”ë¡  ê²€ì¦ ===")
    overall_correct = 0
    overall_total = 0
    domain_results = {}
    
    for domain_value in domains:
        print(f"\n--- {domain_value}HP ë„ë©”ì¸ ---")
        
        # ëª¨ë¸ ë° í”„ë¡œí† íƒ€ì…
        model = load_model_for_domain(ckpt_base, domain_value, device)
        prototypes = build_class_prototypes(model, device)

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
        test_ds = CachedBearingDataset(
            data_dir=str(data_dir),
            dataset_type="cwru",
            domain_value=domain_value,
            subset="test"
        )

        n = len(test_ds)
        if n == 0:
            print(f"âŒ {domain_value}HP: test samples = 0 (skip)")
            continue

        picked = sample_indices(n, samples_per_domain, seed=42 + domain_value)
        correct = 0
        class_results = defaultdict(lambda: {'correct': 0, 'total': 0})

        print(f"ğŸ¯ {len(picked)}/{n} ëœë¤ ìƒ˜í”Œ ê²€ì¦:")
        for idx in picked:
            sample = test_ds[idx]
            vib = sample["vibration"].unsqueeze(0).to(device)
            lbl = sample["labels"]
            lbl_id = int(lbl[0].item()) if lbl.ndim > 0 else int(lbl.item())
            meta = sample.get("metadata", {})
            fpath = meta.get("filepath", "unknown")
            widx = int(sample.get("window_idx", -1))

            with torch.no_grad():
                vib_raw = model.vib_encoder(vib)
                vib_emb = F.normalize(model.vib_projection(vib_raw), p=2, dim=1)
                sims = torch.matmul(vib_emb, prototypes.t()).squeeze(0)
                pred_id = int(torch.argmax(sims).item())
                topk_vals, topk_ids = torch.topk(sims, k=3)

            is_ok = (pred_id == lbl_id)
            correct += int(is_ok)
            overall_correct += int(is_ok)
            overall_total += 1
            
            # í´ë˜ìŠ¤ë³„ ê²°ê³¼ ì¶”ì 
            class_results[lbl_id]['total'] += 1
            if is_ok:
                class_results[lbl_id]['correct'] += 1

            status = "âœ“" if is_ok else "âœ—"
            topk_str = ", ".join([f"{CWRU_CLASS_ID_TO_NAME[int(cid)]}:{float(v):.3f}" for v, cid in zip(topk_vals.tolist(), topk_ids.tolist())])
            print(f"  {status} {CWRU_CLASS_ID_TO_NAME[lbl_id]}â†’{CWRU_CLASS_ID_TO_NAME[pred_id]} | {topk_str}")

        acc = 100.0 * correct / max(1, len(picked))
        domain_results[domain_value] = {
            'accuracy': acc,
            'correct': correct,
            'total': len(picked),
            'class_results': dict(class_results)
        }
        
        print(f"ğŸ“Š {domain_value}HP ê²°ê³¼: {correct}/{len(picked)} ({acc:.2f}%)")
        
        # í´ë˜ìŠ¤ë³„ ìƒì„¸ ê²°ê³¼
        for cls_id, results in class_results.items():
            cls_acc = 100.0 * results['correct'] / max(1, results['total'])
            print(f"  {CWRU_CLASS_ID_TO_NAME[cls_id]}: {results['correct']}/{results['total']} ({cls_acc:.1f}%)")

    # 3. ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“ˆ === ì „ì²´ ê²°ê³¼ ìš”ì•½ ===")
    if overall_total > 0:
        overall_acc = 100.0 * overall_correct / overall_total
        print(f"ğŸ¯ ì „ì²´ ìƒ˜í”Œ ì •í™•ë„: {overall_correct}/{overall_total} ({overall_acc:.2f}%)")
        
        print(f"\nğŸ“Š ë„ë©”ì¸ë³„ ê²°ê³¼:")
        for domain_value, results in domain_results.items():
            print(f"  {domain_value}HP: {results['correct']}/{results['total']} ({results['accuracy']:.2f}%)")
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²°ê³¼ ê°ì§€
        if overall_acc > 95.0:
            print(f"\nâš ï¸  ê²½ê³ : ì „ì²´ ì •í™•ë„ê°€ 95%ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤ ({overall_acc:.2f}%)")
            print("   ì´ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            print("   1. ë°ì´í„° ëˆ„ìˆ˜ (train/val/test ê°„ ì¤‘ë³µ)")
            print("   2. ê³¼ì í•© (ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ)")
            print("   3. í´ë˜ìŠ¤ ë¶„ë¦¬ê°€ ë„ˆë¬´ ëª…í™•í•¨")
            print("   â†’ ë°ì´í„° ë¶„í•  ë¡œì§ì„ ë‹¤ì‹œ ê²€í† í•´ë³´ì„¸ìš”!")

def check_data_leakage(data_dir: Path, domains: List[int]) -> None:
    """ë°ì´í„° ëˆ„ìˆ˜ ê°€ëŠ¥ì„± ì²´í¬"""
    print(f"\nğŸ” === ë°ì´í„° ëˆ„ìˆ˜ ì²´í¬ ===")
    
    all_files = set()
    domain_files = {}
    
    for domain_value in domains:
        domain_dir = data_dir / f"Load_{domain_value}hp"
        if not domain_dir.exists():
            continue
            
        domain_files[domain_value] = set()
        
        # ëª¨ë“  .mat íŒŒì¼ ìˆ˜ì§‘
        for fpath in domain_dir.glob("*.mat"):
            filename = fpath.name
            domain_files[domain_value].add(filename)
            all_files.add(filename)
    
    # íŒŒì¼ëª… ì¤‘ë³µ ì²´í¬
    print("ğŸ“ ë„ë©”ì¸ë³„ íŒŒì¼ ì¤‘ë³µ ì²´í¬:")
    for d1 in domains:
        for d2 in domains:
            if d1 < d2:  # ì¤‘ë³µ ì²´í¬ ë°©ì§€
                common = domain_files.get(d1, set()) & domain_files.get(d2, set())
                if common:
                    print(f"  âš ï¸ {d1}HP â†” {d2}HP: {len(common)}ê°œ ì¤‘ë³µ íŒŒì¼")
                    for f in sorted(common):
                        print(f"    - {f}")
                else:
                    print(f"  âœ“ {d1}HP â†” {d2}HP: ì¤‘ë³µ ì—†ìŒ")
    
    # ì „ì²´ íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„
    print(f"\nğŸ“Š íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„:")
    train_files = [f for f in all_files if '_train_' in f]
    val_files = [f for f in all_files if '_val_' in f]
    test_files = [f for f in all_files if '_test_' in f]
    
    print(f"  Train íŒŒì¼: {len(train_files)}ê°œ")
    print(f"  Val íŒŒì¼: {len(val_files)}ê°œ")
    print(f"  Test íŒŒì¼: {len(test_files)}ê°œ")
    
    # í´ë˜ìŠ¤ë³„ íŒŒì¼ ë¶„í¬
    classes = ['H', 'B', 'IR', 'OR']
    for cls in classes:
        cls_train = [f for f in train_files if f.startswith(f'{cls}_')]
        cls_val = [f for f in val_files if f.startswith(f'{cls}_')]
        cls_test = [f for f in test_files if f.startswith(f'{cls}_')]
        print(f"  {cls}: Train({len(cls_train)}) Val({len(cls_val)}) Test({len(cls_test)})")


def main():
    parser = argparse.ArgumentParser(description="CWRU ë°ì´í„° ì¢…í•© ê²€ì¦ ë° ë””ë²„ê¹…")
    parser.add_argument("--experiment_dir", type=str, default="", help="results/{timestamp} ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--samples_per_domain", type=int, default=20, help="ë„ë©”ì¸ë‹¹ ê²€ì¦í•  ìƒ˜í”Œ ìˆ˜")
    parser.add_argument("--domains", type=str, default="0,1,2,3", help="ê²€ì¦í•  ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸. ì˜ˆ: 0,1,2,3")
    parser.add_argument("--device", type=str, default="auto", choices=["auto", "cpu", "cuda"])
    parser.add_argument("--data_only", action="store_true", help="ë°ì´í„° ê²€ì¦ë§Œ ì‹¤í–‰ (ëª¨ë¸ ì¶”ë¡  ìƒëµ)")
    parser.add_argument("--leakage_check", action="store_true", help="ë°ì´í„° ëˆ„ìˆ˜ ì²´í¬ë§Œ ì‹¤í–‰")
    args = parser.parse_args()

    device = pick_device(args.device)
    results_dir = ROOT / "results"
    data_dir = Path(CWRU_DATA_CONFIG["data_dir"])
    
    if args.experiment_dir:
        experiment_dir = Path(args.experiment_dir)
    else:
        experiment_dir = find_latest_experiment_dir(results_dir, "CWRU_Scenario2_VaryingLoad")

    domains = [int(x) for x in args.domains.split(",") if x.strip() != ""]
    
    print("ğŸ” CWRU ë°ì´í„° ì¢…í•© ê²€ì¦ ì‹œì‘")
    print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    print(f"ğŸ“ ì‹¤í—˜ ë””ë ‰í† ë¦¬: {experiment_dir}")
    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ¯ ë„ë©”ì¸: {domains}, ìƒ˜í”Œ/ë„ë©”ì¸: {args.samples_per_domain}")
    
    # ë°ì´í„° ëˆ„ìˆ˜ ì²´í¬
    if args.leakage_check:
        check_data_leakage(data_dir, domains)
        return
    
    # ë°ì´í„° ë¶„í•  ê²€ì¦
    print(f"\n{'='*60}")
    print("1ë‹¨ê³„: ë°ì´í„° ë¶„í•  ë° êµ¬ì¡° ê²€ì¦")
    print(f"{'='*60}")
    for domain_value in domains:
        verify_data_splitting(data_dir, domain_value)
        verify_cached_dataset(data_dir, domain_value)
    
    # ë°ì´í„° ëˆ„ìˆ˜ ì²´í¬
    check_data_leakage(data_dir, domains)
    
    if args.data_only:
        print(f"\nâœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ (ëª¨ë¸ ì¶”ë¡  ìƒëµ)")
        return
    
    # ëª¨ë¸ ì¶”ë¡  ê²€ì¦
    print(f"\n{'='*60}")
    print("2ë‹¨ê³„: ëª¨ë¸ ì¶”ë¡  ê²€ì¦")
    print(f"{'='*60}")
    run_comprehensive_verify(experiment_dir, args.samples_per_domain, device, domains)
    
    print(f"\nâœ… CWRU ì¢…í•© ê²€ì¦ ì™„ë£Œ!")

if __name__ == "__main__":
    main()