#!/usr/bin/env python3
"""
CWRU Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù Î∞è ÎîîÎ≤ÑÍπÖ Ïä§ÌÅ¨Î¶ΩÌä∏ (Í∞úÏÑ† Î≤ÑÏ†Ñ)

Í∏∞Îä•:
1. ÎûúÎç§ ÏÉòÌîå Ï∂îÎ°† Í≤ÄÏ¶ù
2. Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† Î∞è Î°úÎî© Í≤ÄÏ¶ù  
3. ÌÅ¥ÎûòÏä§Î≥Ñ ÏÑ±Îä• Î∂ÑÏÑù
4. ÌååÏùºÎ™Ö Î∞è Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù
5. Îç∞Ïù¥ÌÑ∞ ÎàÑÏàò Í∞ÄÎä•ÏÑ± Ï≤¥ÌÅ¨
"""

import os
import sys
import argparse
import random
from pathlib import Path
from typing import List, Tuple, Dict
from collections import Counter, defaultdict
import numpy as np

import torch
import torch.nn.functional as F

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Í≤ΩÎ°ú Ï∂îÍ∞Ä
ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(ROOT))

from src.textvib_model import create_textvib_model
from src.data_cache import CachedBearingDataset
from src.data_loader import BearingDataset
from configs.model_config import CWRU_DATA_CONFIG

CWRU_CLASS_ID_TO_NAME = {0: "H", 1: "B", 2: "IR", 3: "OR"}
CWRU_CLASS_ID_TO_TEXT = {
    0: ["healthy bearing", "normal bearing with no fault", "bearing vibration without defect"],
    1: ["bearing with ball fault", "ball defect in bearing", "ball damage on bearing"],
    2: ["bearing inner race fault", "inner ring defect in bearing", "inner race damage of bearing"],
    3: ["bearing outer race fault", "outer ring defect in bearing", "outer race damage of bearing"],
}

def pick_device(arg: str) -> torch.device:
    if arg == "cpu":
        return torch.device("cpu")
    if arg == "cuda":
        return torch.device("cuda")
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

def find_latest_experiment_dir(results_dir: Path, scenario_name: str) -> Path:
    if not results_dir.exists():
        raise FileNotFoundError(f"Results dir not found: {results_dir}")
    candidates = sorted([p for p in results_dir.iterdir() if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)
    for c in candidates:
        ckpt_dir = c / "checkpoints" / scenario_name
        if ckpt_dir.exists():
            return c
    raise FileNotFoundError(f"No experiment dir with checkpoints/{scenario_name} under {results_dir}")

def load_model_for_domain(ckpt_base: Path, domain_value: int, device: torch.device) -> torch.nn.Module:
    # ÎèÑÎ©îÏù∏Î≥Ñ best Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ïö∞ÏÑ†, ÏóÜÏúºÎ©¥ first_domain_final.pthÎ°ú Ìè¥Î∞±(0HP)
    candidates = [
        ckpt_base / f"domain_{domain_value}_best.pth",
        ckpt_base / "first_domain_final.pth"
    ]
    ckpt_path = None
    for p in candidates:
        if p.exists():
            ckpt_path = p
            break
    if ckpt_path is None:
        raise FileNotFoundError(f"Checkpoint not found under {ckpt_base}")
    model = create_textvib_model(domain_stage="continual", dataset_type="cwru").to(device)
    model.load_checkpoint(str(ckpt_path), device=device)
    model.eval()
    return model

def build_class_prototypes(model: torch.nn.Module, device: torch.device) -> torch.Tensor:
    # Í∞Å ÌÅ¥ÎûòÏä§Ïùò ÌîÑÎ°¨ÌîÑÌä∏ ÏûÑÎ≤†Îî© ÌèâÍ∑† ‚Üí ÌîÑÎ°úÌÜ†ÌÉÄÏûÖ (4, dim)
    class_embs = []
    for cls_id in [0, 1, 2, 3]:
        texts = CWRU_CLASS_ID_TO_TEXT[cls_id]
        raw = model.text_encoder.encode_texts(texts, device)
        proj = F.normalize(model.text_projection(raw), p=2, dim=1)
        proto = F.normalize(proj.mean(dim=0, keepdim=True), p=2, dim=1)
        class_embs.append(proto)
    return torch.cat(class_embs, dim=0)  # (4, dim)

def sample_indices(n_total: int, k: int, seed: int = 42) -> List[int]:
    k = max(1, min(k, n_total))
    rng = random.Random(seed)
    return rng.sample(range(n_total), k)

def verify_data_splitting(data_dir: Path, domain_value: int) -> None:
    """Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† Î∞è ÌååÏùº Íµ¨Ï°∞ Í≤ÄÏ¶ù"""
    print(f"\nüîç === Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† Í≤ÄÏ¶ù: {domain_value}HP ===")
    
    domain_dir = data_dir / f"Load_{domain_value}hp"
    if not domain_dir.exists():
        print(f"‚ùå ÎèÑÎ©îÏù∏ ÎîîÎ†âÌÜ†Î¶¨ ÏóÜÏùå: {domain_dir}")
        return
    
    # ÌååÏùº Íµ¨Ï°∞ ÌôïÏù∏
    classes = ['H', 'B', 'IR', 'OR']
    subsets = ['train', 'val', 'test']
    
    print("üìÅ ÌååÏùº Íµ¨Ï°∞:")
    for cls in classes:
        print(f"  {cls}: ", end="")
        for subset in subsets:
            fname = f"{cls}_{domain_value}hp_{subset}_01.mat"
            fpath = domain_dir / fname
            if fpath.exists():
                print(f"{subset}‚úì ", end="")
            else:
                print(f"{subset}‚úó ", end="")
        print()
    
    # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú Î∂ÑÌï† Í≤ÄÏ¶ù
    try:
        print("\nüìä ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÏÖã Î∂ÑÌï† Í≤ÄÏ¶ù:")
        for subset in subsets:
            ds = BearingDataset(
                data_dir=str(data_dir),
                dataset_type="cwru", 
                domain_value=domain_value,
                subset=subset
            )
            
            if len(ds.file_paths) > 0:
                # ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÌôïÏù∏
                labels = [ds.metadata_list[i]['bearing_condition'] for i in range(len(ds.metadata_list))]
                label_counts = Counter(labels)
                print(f"  {subset}: {len(ds.file_paths)}Í∞ú ÌååÏùº, ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨: {dict(label_counts)}")
                
                # ÌååÏùºÎ™Ö ÌôïÏù∏
                file_names = [os.path.basename(f) for f in ds.file_paths]
                print(f"    ÌååÏùºÎì§: {file_names}")
            else:
                print(f"  {subset}: ÌååÏùº ÏóÜÏùå")
    except Exception as e:
        print(f"‚ùå Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ÄÏ¶ù Ïã§Ìå®: {e}")


def verify_cached_dataset(data_dir: Path, domain_value: int) -> None:
    """Ï∫êÏãúÎêú Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ÄÏ¶ù"""
    print(f"\nüíæ === Ï∫êÏãú Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ÄÏ¶ù: {domain_value}HP ===")
    
    try:
        test_ds = CachedBearingDataset(
            data_dir=str(data_dir),
            dataset_type="cwru",
            domain_value=domain_value,
            subset="test"
        )
        
        print(f"üìä Ï∫êÏãúÎêú ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞: {len(test_ds)}Í∞ú ÏÉòÌîå")
        
        if len(test_ds) > 0:
            # Ï≤´ Î™á Í∞ú ÏÉòÌîåÏùò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌôïÏù∏
            print("üîç ÏÉòÌîå Î©îÌÉÄÎç∞Ïù¥ÌÑ∞:")
            for i in range(min(5, len(test_ds))):
                sample = test_ds[i]
                meta = sample.get("metadata", {})
                lbl = sample["labels"]
                lbl_id = int(lbl[0].item()) if lbl.ndim > 0 else int(lbl.item())
                
                print(f"  ÏÉòÌîå {i}: ÌÅ¥ÎûòÏä§={CWRU_CLASS_ID_TO_NAME[lbl_id]}, "
                      f"ÌååÏùº={os.path.basename(meta.get('filepath', 'unknown'))}, "
                      f"ÏúàÎèÑÏö∞={sample.get('window_idx', -1)}")
        
        # ÌÅ¥ÎûòÏä§Î≥Ñ ÏÉòÌîå Ïàò Í≥ÑÏÇ∞
        class_counts = defaultdict(int)
        for i in range(len(test_ds)):
            sample = test_ds[i]
            lbl = sample["labels"]
            lbl_id = int(lbl[0].item()) if lbl.ndim > 0 else int(lbl.item())
            class_counts[lbl_id] += 1
        
        print(f"üìà ÌÅ¥ÎûòÏä§Î≥Ñ ÏÉòÌîå Ïàò: {dict(class_counts)}")
        
    except Exception as e:
        print(f"‚ùå Ï∫êÏãú Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ÄÏ¶ù Ïã§Ìå®: {e}")


def run_comprehensive_verify(experiment_dir: Path, samples_per_domain: int, device: torch.device, domains: List[int]) -> None:
    """Ï¢ÖÌï© Í≤ÄÏ¶ù Ïã§Ìñâ"""
    scenario_name = "CWRU_Scenario2_VaryingLoad"
    ckpt_base = experiment_dir / "checkpoints" / scenario_name
    if not ckpt_base.exists():
        raise FileNotFoundError(f"Checkpoint base not found: {ckpt_base}")

    data_dir = Path(CWRU_DATA_CONFIG["data_dir"])
    
    # 1. Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† Í≤ÄÏ¶ù
    for domain_value in domains:
        verify_data_splitting(data_dir, domain_value)
        verify_cached_dataset(data_dir, domain_value)
    
    # 2. Î™®Îç∏ Ï∂îÎ°† Í≤ÄÏ¶ù
    print(f"\nü§ñ === Î™®Îç∏ Ï∂îÎ°† Í≤ÄÏ¶ù ===")
    overall_correct = 0
    overall_total = 0
    domain_results = {}
    
    for domain_value in domains:
        print(f"\n--- {domain_value}HP ÎèÑÎ©îÏù∏ ---")
        
        # Î™®Îç∏ Î∞è ÌîÑÎ°úÌÜ†ÌÉÄÏûÖ
        model = load_model_for_domain(ckpt_base, domain_value, device)
        prototypes = build_class_prototypes(model, device)

        # ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã
        test_ds = CachedBearingDataset(
            data_dir=str(data_dir),
            dataset_type="cwru",
            domain_value=domain_value,
            subset="test"
        )

        n = len(test_ds)
        if n == 0:
            print(f"‚ùå {domain_value}HP: test samples = 0 (skip)")
            continue

        picked = sample_indices(n, samples_per_domain, seed=42 + domain_value)
        correct = 0
        class_results = defaultdict(lambda: {'correct': 0, 'total': 0})

        print(f"üéØ {len(picked)}/{n} ÎûúÎç§ ÏÉòÌîå Í≤ÄÏ¶ù:")
        for idx in picked:
            sample = test_ds[idx]
            vib = sample["vibration"].unsqueeze(0).to(device)
            lbl = sample["labels"]
            lbl_id = int(lbl[0].item()) if lbl.ndim > 0 else int(lbl.item())
            meta = sample.get("metadata", {})
            fpath = meta.get("filepath", "unknown")
            widx = int(sample.get("window_idx", -1))

            with torch.no_grad():
                vib_raw = model.vib_encoder(vib)
                vib_emb = F.normalize(model.vib_projection(vib_raw), p=2, dim=1)
                sims = torch.matmul(vib_emb, prototypes.t()).squeeze(0)
                pred_id = int(torch.argmax(sims).item())
                topk_vals, topk_ids = torch.topk(sims, k=3)

            is_ok = (pred_id == lbl_id)
            correct += int(is_ok)
            overall_correct += int(is_ok)
            overall_total += 1
            
            # ÌÅ¥ÎûòÏä§Î≥Ñ Í≤∞Í≥º Ï∂îÏ†Å
            class_results[lbl_id]['total'] += 1
            if is_ok:
                class_results[lbl_id]['correct'] += 1

            status = "‚úì" if is_ok else "‚úó"
            topk_str = ", ".join([f"{CWRU_CLASS_ID_TO_NAME[int(cid)]}:{float(v):.3f}" for v, cid in zip(topk_vals.tolist(), topk_ids.tolist())])
            print(f"  {status} {CWRU_CLASS_ID_TO_NAME[lbl_id]}‚Üí{CWRU_CLASS_ID_TO_NAME[pred_id]} | {topk_str}")

        acc = 100.0 * correct / max(1, len(picked))
        domain_results[domain_value] = {
            'accuracy': acc,
            'correct': correct,
            'total': len(picked),
            'class_results': dict(class_results)
        }
        
        print(f"üìä {domain_value}HP Í≤∞Í≥º: {correct}/{len(picked)} ({acc:.2f}%)")
        
        # ÌÅ¥ÎûòÏä§Î≥Ñ ÏÉÅÏÑ∏ Í≤∞Í≥º
        for cls_id, results in class_results.items():
            cls_acc = 100.0 * results['correct'] / max(1, results['total'])
            print(f"  {CWRU_CLASS_ID_TO_NAME[cls_id]}: {results['correct']}/{results['total']} ({cls_acc:.1f}%)")

    # 3. Ï†ÑÏ≤¥ Í≤∞Í≥º ÏöîÏïΩ
    print(f"\nüìà === Ï†ÑÏ≤¥ Í≤∞Í≥º ÏöîÏïΩ ===")
    if overall_total > 0:
        overall_acc = 100.0 * overall_correct / overall_total
        print(f"üéØ Ï†ÑÏ≤¥ ÏÉòÌîå Ï†ïÌôïÎèÑ: {overall_correct}/{overall_total} ({overall_acc:.2f}%)")
        
        print(f"\nüìä ÎèÑÎ©îÏù∏Î≥Ñ Í≤∞Í≥º:")
        for domain_value, results in domain_results.items():
            print(f"  {domain_value}HP: {results['correct']}/{results['total']} ({results['accuracy']:.2f}%)")
        
        # ÏùòÏã¨Ïä§Îü¨Ïö¥ Í≤∞Í≥º Í∞êÏßÄ
        if overall_acc > 95.0:
            print(f"\n‚ö†Ô∏è  Í≤ΩÍ≥†: Ï†ÑÏ≤¥ Ï†ïÌôïÎèÑÍ∞Ä 95%Î•º Ï¥àÍ≥ºÌï©ÎãàÎã§ ({overall_acc:.2f}%)")
            print("   Ïù¥Îäî Îã§Ïùå Ï§ë ÌïòÎÇòÏùº Ïàò ÏûàÏäµÎãàÎã§:")
            print("   1. Îç∞Ïù¥ÌÑ∞ ÎàÑÏàò (train/val/test Í∞Ñ Ï§ëÎ≥µ)")
            print("   2. Í≥ºÏ†ÅÌï© (Îç∞Ïù¥ÌÑ∞Í∞Ä ÎÑàÎ¨¥ Ï†ÅÏùå)")
            print("   3. ÌÅ¥ÎûòÏä§ Î∂ÑÎ¶¨Í∞Ä ÎÑàÎ¨¥ Î™ÖÌôïÌï®")
            print("   ‚Üí Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† Î°úÏßÅÏùÑ Îã§Ïãú Í≤ÄÌÜ†Ìï¥Î≥¥ÏÑ∏Ïöî!")

def check_data_leakage(data_dir: Path, domains: List[int]) -> None:
    """Îç∞Ïù¥ÌÑ∞ ÎàÑÏàò Í∞ÄÎä•ÏÑ± Ï≤¥ÌÅ¨"""
    print(f"\nüîç === Îç∞Ïù¥ÌÑ∞ ÎàÑÏàò Ï≤¥ÌÅ¨ ===")
    
    all_files = set()
    domain_files = {}
    
    for domain_value in domains:
        domain_dir = data_dir / f"Load_{domain_value}hp"
        if not domain_dir.exists():
            continue
            
        domain_files[domain_value] = set()
        
        # Î™®Îì† .mat ÌååÏùº ÏàòÏßë
        for fpath in domain_dir.glob("*.mat"):
            filename = fpath.name
            domain_files[domain_value].add(filename)
            all_files.add(filename)
    
    # ÌååÏùºÎ™Ö Ï§ëÎ≥µ Ï≤¥ÌÅ¨
    print("üìÅ ÎèÑÎ©îÏù∏Î≥Ñ ÌååÏùº Ï§ëÎ≥µ Ï≤¥ÌÅ¨:")
    for d1 in domains:
        for d2 in domains:
            if d1 < d2:  # Ï§ëÎ≥µ Ï≤¥ÌÅ¨ Î∞©ÏßÄ
                common = domain_files.get(d1, set()) & domain_files.get(d2, set())
                if common:
                    print(f"  ‚ö†Ô∏è {d1}HP ‚Üî {d2}HP: {len(common)}Í∞ú Ï§ëÎ≥µ ÌååÏùº")
                    for f in sorted(common):
                        print(f"    - {f}")
                else:
                    print(f"  ‚úì {d1}HP ‚Üî {d2}HP: Ï§ëÎ≥µ ÏóÜÏùå")
    
    # Ï†ÑÏ≤¥ ÌååÏùºÎ™Ö Ìå®ÌÑ¥ Î∂ÑÏÑù
    print(f"\nüìä ÌååÏùºÎ™Ö Ìå®ÌÑ¥ Î∂ÑÏÑù:")
    train_files = [f for f in all_files if '_train_' in f]
    val_files = [f for f in all_files if '_val_' in f]
    test_files = [f for f in all_files if '_test_' in f]
    
    print(f"  Train ÌååÏùº: {len(train_files)}Í∞ú")
    print(f"  Val ÌååÏùº: {len(val_files)}Í∞ú")
    print(f"  Test ÌååÏùº: {len(test_files)}Í∞ú")
    
    # ÌÅ¥ÎûòÏä§Î≥Ñ ÌååÏùº Î∂ÑÌè¨
    classes = ['H', 'B', 'IR', 'OR']
    for cls in classes:
        cls_train = [f for f in train_files if f.startswith(f'{cls}_')]
        cls_val = [f for f in val_files if f.startswith(f'{cls}_')]
        cls_test = [f for f in test_files if f.startswith(f'{cls}_')]
        print(f"  {cls}: Train({len(cls_train)}) Val({len(cls_val)}) Test({len(cls_test)})")


def main():
    parser = argparse.ArgumentParser(description="CWRU Îç∞Ïù¥ÌÑ∞ Ï¢ÖÌï© Í≤ÄÏ¶ù Î∞è ÎîîÎ≤ÑÍπÖ")
    parser.add_argument("--experiment_dir", type=str, default="", help="results/{timestamp} ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú")
    parser.add_argument("--samples_per_domain", type=int, default=20, help="ÎèÑÎ©îÏù∏Îãπ Í≤ÄÏ¶ùÌï† ÏÉòÌîå Ïàò")
    parser.add_argument("--domains", type=str, default="0,1,2,3", help="Í≤ÄÏ¶ùÌï† ÎèÑÎ©îÏù∏ Î¶¨Ïä§Ìä∏. Ïòà: 0,1,2,3")
    parser.add_argument("--device", type=str, default="auto", choices=["auto", "cpu", "cuda"])
    parser.add_argument("--data_only", action="store_true", help="Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ùÎßå Ïã§Ìñâ (Î™®Îç∏ Ï∂îÎ°† ÏÉùÎûµ)")
    parser.add_argument("--leakage_check", action="store_true", help="Îç∞Ïù¥ÌÑ∞ ÎàÑÏàò Ï≤¥ÌÅ¨Îßå Ïã§Ìñâ")
    args = parser.parse_args()

    device = pick_device(args.device)
    results_dir = ROOT / "results"
    data_dir = Path(CWRU_DATA_CONFIG["data_dir"])
    
    if args.experiment_dir:
        experiment_dir = Path(args.experiment_dir)
    else:
        experiment_dir = find_latest_experiment_dir(results_dir, "CWRU_Scenario2_VaryingLoad")

    domains = [int(x) for x in args.domains.split(",") if x.strip() != ""]
    
    print("üîç CWRU Îç∞Ïù¥ÌÑ∞ Ï¢ÖÌï© Í≤ÄÏ¶ù ÏãúÏûë")
    print(f"üìÅ Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨: {data_dir}")
    print(f"üìÅ Ïã§Ìóò ÎîîÎ†âÌÜ†Î¶¨: {experiment_dir}")
    print(f"üîß ÎîîÎ∞îÏù¥Ïä§: {device}")
    print(f"üéØ ÎèÑÎ©îÏù∏: {domains}, ÏÉòÌîå/ÎèÑÎ©îÏù∏: {args.samples_per_domain}")
    
    # Îç∞Ïù¥ÌÑ∞ ÎàÑÏàò Ï≤¥ÌÅ¨
    if args.leakage_check:
        check_data_leakage(data_dir, domains)
        return
    
    # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† Í≤ÄÏ¶ù
    print(f"\n{'='*60}")
    print("1Îã®Í≥Ñ: Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï† Î∞è Íµ¨Ï°∞ Í≤ÄÏ¶ù")
    print(f"{'='*60}")
    for domain_value in domains:
        verify_data_splitting(data_dir, domain_value)
        verify_cached_dataset(data_dir, domain_value)
    
    # Îç∞Ïù¥ÌÑ∞ ÎàÑÏàò Ï≤¥ÌÅ¨
    check_data_leakage(data_dir, domains)
    
    if args.data_only:
        print(f"\n‚úÖ Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù ÏôÑÎ£å (Î™®Îç∏ Ï∂îÎ°† ÏÉùÎûµ)")
        return
    
    # Î™®Îç∏ Ï∂îÎ°† Í≤ÄÏ¶ù
    print(f"\n{'='*60}")
    print("2Îã®Í≥Ñ: Î™®Îç∏ Ï∂îÎ°† Í≤ÄÏ¶ù")
    print(f"{'='*60}")
    run_comprehensive_verify(experiment_dir, args.samples_per_domain, device, domains)
    
    print(f"\n‚úÖ CWRU Ï¢ÖÌï© Í≤ÄÏ¶ù ÏôÑÎ£å!")

if __name__ == "__main__":
    main()