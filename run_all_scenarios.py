#!/usr/bin/env python3
"""
TextVibCLIP ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ í†µí•© ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
1. ì‹œë‚˜ë¦¬ì˜¤ 1 (UOS) + ì‹œë‚˜ë¦¬ì˜¤ 2 (CWRU) ìë™ ì‹¤í–‰
2. ì‹œë‚˜ë¦¬ì˜¤ë³„/ë„ë©”ì¸ë³„ ì„±ëŠ¥ ì§€í‘œ CSV ì €ì¥
3. ì¢…í•© ê²°ê³¼ ìš”ì•½ ë° ë¹„êµ ë¶„ì„
4. ì‹¤í—˜ ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

Usage:
    python run_all_scenarios.py --output_dir results
    python run_all_scenarios.py --quick_test --epochs 10
"""

import argparse
import logging
import os
import torch
import time
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
import warnings

# Torchvision beta warning ë¹„í™œì„±í™”
try:
    import torchvision
    torchvision.disable_beta_transforms_warning()
except:
    pass

# ê¸°íƒ€ warning ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning, module="torchvision")

# pandas import (CSV ì €ì¥ìš©)
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("âš ï¸ pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CSV ì €ì¥ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install pandas")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
import sys
from pathlib import Path
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

from src.continual_trainer import ContinualTrainer
from src.data_loader import BearingDataset, create_domain_dataloaders
from src.data_cache import create_cached_domain_dataloaders, create_cached_first_domain_dataloader, get_global_cache, clear_all_caches
from src.textvib_model import create_textvib_model
from src.utils import set_seed
from src.visualization import create_visualizer
from configs.model_config import TRAINING_CONFIG, DATA_CONFIG, CWRU_DATA_CONFIG

# ë¡œê¹… ì„¤ì •
def setup_logging(log_dir: str) -> Tuple[logging.Logger, str]:
    """í†µí•© ì‹¤í—˜ìš© ë¡œê¹… ì„¤ì • ë° ì‹¤í—˜ë³„ í´ë” ìƒì„±"""
    # ì‹¤í—˜ë³„ ê³ ìœ  í´ë” ìƒì„± (ë‚ ì§œì‹œê°„ ê¸°ì¤€)
    experiment_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    # í´ë”ëª…ì„ ê°„ê²°í•˜ê²Œ: ì ‘ë‘ì‚¬ ì—†ì´ íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ ì‚¬ìš©
    experiment_dir = os.path.join(log_dir, experiment_timestamp)
    os.makedirs(experiment_dir, exist_ok=True)
    
    log_filename = f"all_scenarios_{experiment_timestamp}.log"
    log_path = os.path.join(experiment_dir, log_filename)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"í†µí•© ì‹¤í—˜ ë¡œê¹… ì‹œì‘: {log_path}")
    logger.info(f"ì‹¤í—˜ ê²°ê³¼ í´ë”: {experiment_dir}")
    
    return logger, experiment_dir


class ScenarioConfig:
    """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„¤ì • ê´€ë¦¬"""
    
    UOS_CONFIG = {
        'name': 'UOS_Scenario1_VaryingSpeed',
        'data_dir': 'data_scenario1',
        'dataset_type': 'uos',
        'domain_order': [600, 800, 1000, 1200, 1400, 1600],
        'domain_names': ['600RPM', '800RPM', '1000RPM', '1200RPM', '1400RPM', '1600RPM'],
        'shift_type': 'Varying Speed',
        'first_domain_epochs': 15,  # í˜„ì‹¤ì  ì—í¬í¬ ìˆ˜
        'remaining_epochs': 10,
        'batch_size': 4,  # ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê°•í™”
        'replay_buffer_size': 1000,
        'patience': 5
    }
    
    CWRU_CONFIG = {
        'name': 'CWRU_Scenario2_VaryingLoad',
        'data_dir': 'data_scenario2',
        'dataset_type': 'cwru',
        'domain_order': [0, 1, 2, 3],
        'domain_names': ['0HP', '1HP', '2HP', '3HP'],
        'shift_type': 'Varying Load',
        'first_domain_epochs': 20,  # í˜„ì‹¤ì  ì—í¬í¬ ìˆ˜
        'remaining_epochs': 10,
        'batch_size': 8,
        'replay_buffer_size': 200,
        'patience': 15
    }


class ExperimentResults:
    """ì‹¤í—˜ ê²°ê³¼ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scenario_results = {}
        self.detailed_results = []
        self.summary_results = []
    
    def add_scenario_result(self, scenario_name: str, results: Dict):
        """ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ì¶”ê°€"""
        self.scenario_results[scenario_name] = results
        
        # ìƒì„¸ ê²°ê³¼ ì¶”ê°€ (ë„ë©”ì¸ë³„)
        for domain_idx, domain_name in enumerate(results['domain_names']):
            if domain_idx < len(results['final_accuracies']):
                self.detailed_results.append({
                    'Scenario': scenario_name,
                    'Domain_Index': domain_idx + 1,
                    'Domain_Name': domain_name,
                    'Shift_Type': results['shift_type'],
                    'Accuracy': results['final_accuracies'][domain_idx],
                    'Top1_Retrieval': results.get('final_top1_retrievals', [0] * len(results['domain_names']))[domain_idx],
                    'Top5_Retrieval': results.get('final_top5_retrievals', [0] * len(results['domain_names']))[domain_idx],
                    'Samples_Per_Domain': results.get('samples_per_domain', 0),
                    'Total_Training_Time': results.get('total_time', 0)
                })
        
        # ìš”ì•½ ê²°ê³¼ ì¶”ê°€ (ì‹œë‚˜ë¦¬ì˜¤ë³„)
        self.summary_results.append({
            'Scenario': scenario_name,
            'Shift_Type': results['shift_type'],
            'Num_Domains': len(results['domain_names']),
            'Avg_Accuracy': results.get('average_accuracy', 0),
            'Avg_Forgetting': results.get('average_forgetting', 0),
            'Avg_Top1_Retrieval': np.mean(results.get('final_top1_retrievals', [0])),
            'Avg_Top5_Retrieval': np.mean(results.get('final_top5_retrievals', [0])),
            'Total_Samples': results.get('total_samples', 0),
            'Total_Time_Minutes': results.get('total_time', 0) / 60,
            'First_Domain_Epochs': results.get('first_domain_epochs', 0),
            'Remaining_Epochs': results.get('remaining_epochs', 0),
            'Batch_Size': results.get('batch_size', 0)
        })
    
    def save_to_csv(self, output_dir: str):
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if not PANDAS_AVAILABLE:
            # pandasê°€ ì—†ìœ¼ë©´ JSONìœ¼ë¡œ ì €ì¥
            detailed_path = os.path.join(output_dir, f'detailed_results_{timestamp}.json')
            summary_path = os.path.join(output_dir, f'summary_results_{timestamp}.json')
            
            with open(detailed_path, 'w') as f:
                json.dump(self.detailed_results, f, indent=2)
            with open(summary_path, 'w') as f:
                json.dump(self.summary_results, f, indent=2)
            
            return detailed_path, summary_path, None
        
        # 1. ìƒì„¸ ê²°ê³¼ (ë„ë©”ì¸ë³„)
        detailed_df = pd.DataFrame(self.detailed_results)
        detailed_path = os.path.join(output_dir, f'detailed_results_{timestamp}.csv')
        detailed_df.to_csv(detailed_path, index=False)
        
        # 2. ìš”ì•½ ê²°ê³¼ (ì‹œë‚˜ë¦¬ì˜¤ë³„)
        summary_df = pd.DataFrame(self.summary_results)
        summary_path = os.path.join(output_dir, f'summary_results_{timestamp}.csv')
        summary_df.to_csv(summary_path, index=False)
        
        # 3. ë¹„êµ ê²°ê³¼ (í”¼ë²— í…Œì´ë¸”)
        pivot_path = None
        if len(self.detailed_results) > 0:
            try:
                pivot_df = detailed_df.pivot_table(
                    index=['Domain_Index', 'Domain_Name'],
                    columns='Scenario',
                    values=['Accuracy', 'Top1_Retrieval', 'Top5_Retrieval'],
                    aggfunc='first'
                )
                pivot_path = os.path.join(output_dir, f'comparison_results_{timestamp}.csv')
                pivot_df.to_csv(pivot_path)
            except Exception as e:
                print(f"âš ï¸ í”¼ë²— í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
        
        return detailed_path, summary_path, pivot_path


def run_single_scenario(config: Dict, logger: logging.Logger, device: torch.device, args) -> Dict:
    """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰"""
    logger.info(f"ğŸš€ {config['name']} ì‹œì‘!")
    logger.info(f"   Domain Shift: {config['shift_type']}")
    logger.info(f"   Domains: {' â†’ '.join(config['domain_names'])}")
    
    start_time = time.time()
    
    try:
        # Trainer ìƒì„±
        trainer = ContinualTrainer(
            device=device,
            save_dir=f"checkpoints/{config['name']}",
            use_amp=False,  # ğŸ¯ AMP ë¹„í™œì„±í™” (ìˆ˜ì¹˜ ì•ˆì •ì„± í™•ë³´)
            max_grad_norm=0.1,
            domain_order=config['domain_order'],
            data_dir=config['data_dir'],
            dataset_type=config['dataset_type'],
            patience=config.get('patience', TRAINING_CONFIG.get('patience', 10))
        )
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        trainer.batch_size = config['batch_size']
        trainer.learning_rate = 3e-4  # configì™€ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •
        trainer.replay_buffer.buffer_size_per_domain = config['replay_buffer_size']
        
        # ğŸš€ ìºì‹œëœ ë°ì´í„°ì…‹ ì •ë³´ ìˆ˜ì§‘ (ê³ ì†í™”)
        from src.data_cache import CachedBearingDataset
        sample_dataset = CachedBearingDataset(
            data_dir=config['data_dir'],
            dataset_type=config['dataset_type'],
            domain_value=config['domain_order'][0],
            subset='train'
        )
        samples_per_domain = len(sample_dataset)
        total_samples = samples_per_domain * len(config['domain_order'])
        
        logger.info(f"   ë°ì´í„°: {samples_per_domain:,} ìƒ˜í”Œ/ë„ë©”ì¸, ì´ {total_samples:,} ìƒ˜í”Œ")
        
        # First Domain Training
        logger.info("ğŸ“š First Domain Training...")
        
        # ğŸš€ ìºì‹œëœ DataLoader ì‚¬ìš© (ê³ ì†í™”)
        if args.no_cache:
            from src.data_loader import create_first_domain_dataloader
            first_loader = create_first_domain_dataloader(
                data_dir=config['data_dir'],
                domain_order=config['domain_order'],
                dataset_type=config['dataset_type'],
                subset='train',
                batch_size=config['batch_size']
            )
        else:
            first_loader = create_cached_first_domain_dataloader(
                data_dir=config['data_dir'],
                domain_order=config['domain_order'],
                dataset_type=config['dataset_type'],
                subset='train',
                batch_size=config['batch_size']
            )
        
        # ğŸ¯ FIXED: ì¡°ê¸° ì¢…ë£Œ ì˜ˆì™¸ ì²˜ë¦¬ ì œê±° (ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸)
        first_results = trainer.train_first_domain(
            first_domain_dataloader=first_loader,
            num_epochs=config['first_domain_epochs']
        )
        
        # SANITY ëª¨ë“œ: Remaining domains ì™„ì „ ìŠ¤í‚µí•˜ê³  ì²« ë„ë©”ì¸ë§Œ ê²°ê³¼ ë°˜í™˜
        if config.get('remaining_epochs', 0) == 0:
            logger.info("ğŸ§ª SANITY ëª¨ë“œ ê°ì§€: Remaining domains í•™ìŠµ/í‰ê°€ ìŠ¤í‚µ")
            first_domain_value = config['domain_order'][0]
            first_domain_name = config['domain_names'][0]
            perf = first_results.get('domain_performances', {})
            metrics = perf.get(first_domain_value, {'accuracy': 0.0, 'top1_retrieval': 0.0, 'top5_retrieval': 0.0})

            total_time = time.time() - start_time

            results = {
                'domain_names': [first_domain_name],
                'shift_type': config['shift_type'],
                'final_accuracies': [metrics.get('accuracy', 0.0)],
                'final_top1_retrievals': [metrics.get('top1_retrieval', 0.0)],
                'final_top5_retrievals': [metrics.get('top5_retrieval', 0.0)],
                'average_accuracy': metrics.get('accuracy', 0.0),
                'average_forgetting': 0.0,
                'samples_per_domain': samples_per_domain,
                'total_samples': samples_per_domain,
                'total_time': total_time,
                'first_domain_epochs': config['first_domain_epochs'],
                'remaining_epochs': 0,
                'batch_size': config['batch_size']
            }

            # ì²« ë„ë©”ì¸ë§Œ ì„ë² ë”© ìˆ˜ì§‘ (ì‹œê°í™”ìš©)
            logger.info("ğŸ“Š SANITY - ì²« ë„ë©”ì¸ ì„ë² ë”© ìˆ˜ì§‘ ì¤‘...")
            domain = first_domain_value
            test_dataset = BearingDataset(
                data_dir=config['data_dir'],
                dataset_type=config['dataset_type'],
                domain_value=domain,
                subset='test'
            )
            domain_embeddings = {}
            if len(test_dataset) > 0:
                max_viz_samples = min(config.get('sanity_samples', 100), len(test_dataset))
                indices = torch.randperm(len(test_dataset))[:max_viz_samples]
                text_embeddings = []
                vib_embeddings = []
                metadata_list = []
                trainer.model.eval()
                # ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ íš¨ìœ¨í™”
                from torch.utils.data import DataLoader, Subset
                subset = Subset(test_dataset, indices.tolist())
                # ê¸°ë³¸ ì½œë ˆì´íŠ¸ëŠ” dict-of-listsë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ list-of-dictsë¡œ ë³µì›
                def _collate_identity(samples):
                    return samples  # list of dicts ìœ ì§€
                dl = DataLoader(subset, batch_size=16, shuffle=False, collate_fn=_collate_identity)
                with torch.no_grad():
                    for samples in dl:
                        # samples: list of dicts
                        vib_batch = torch.stack([s['vibration'] for s in samples], dim=0).to(device)
                        text_batch = [s['text'] for s in samples]
                        meta_batch = [s.get('metadata', {}) for s in samples]
                        batch = {
                            'vibration': vib_batch,
                            'text': text_batch
                        }
                        model_results = trainer.model(batch, return_embeddings=True)
                        text_embeddings.append(model_results['text_embeddings'])
                        vib_embeddings.append(model_results['vib_embeddings'])
                        metadata_list.extend(meta_batch)
                if text_embeddings:
                    domain_embeddings[domain] = {
                        'text': torch.cat(text_embeddings, dim=0),
                        'vib': torch.cat(vib_embeddings, dim=0),
                        'metadata': metadata_list
                    }
            results['domain_embeddings'] = domain_embeddings

            logger.info("âœ… SANITY - ì²« ë„ë©”ì¸ ê²°ê³¼ ë°˜í™˜ ì™„ë£Œ")
            return results

        # Remaining Domains Training (SANITYê°€ ì•„ë‹ˆë©´ ì§„í–‰)
        logger.info("ğŸ”„ Remaining Domains Training...")
        trainer.num_epochs = config['remaining_epochs']
        
        # ğŸš€ ìºì‹œëœ ë„ë©”ì¸ë³„ ë°ì´í„°ë¡œë” ìƒì„± (ê³ ì†í™”)
        if args.no_cache:
            domain_loaders = create_domain_dataloaders(
                data_dir=config['data_dir'],
                domain_order=config['domain_order'],
                dataset_type=config['dataset_type'],
                batch_size=config['batch_size']
            )
        else:
            domain_loaders = create_cached_domain_dataloaders(
                data_dir=config['data_dir'],
                domain_order=config['domain_order'],
                dataset_type=config['dataset_type'],
                batch_size=config['batch_size']
            )
        
        remaining_results = trainer.train_remaining_domains(domain_loaders)
        
        # ê²°ê³¼ ì •ë¦¬
        final_metrics = remaining_results['final_metrics']
        total_time = time.time() - start_time
        
        results = {
            'domain_names': config['domain_names'],
            'shift_type': config['shift_type'],
            'final_accuracies': final_metrics['final_accuracies'],
            'final_top1_retrievals': final_metrics.get('final_top1_retrievals', []),
            'final_top5_retrievals': final_metrics.get('final_top5_retrievals', []),
            'average_accuracy': final_metrics['average_accuracy'],
            'average_forgetting': final_metrics['average_forgetting'],
            'samples_per_domain': samples_per_domain,
            'total_samples': total_samples,
            'total_time': total_time,
            'first_domain_epochs': config['first_domain_epochs'],
            'remaining_epochs': config['remaining_epochs'],
            'batch_size': config['batch_size']
        }
        
        logger.info(f"âœ… {config['name']} ì™„ë£Œ!")
        logger.info(f"   í‰ê·  ì •í™•ë„: {final_metrics['average_accuracy']:.4f}")
        logger.info(f"   í‰ê·  ë§ê°ë„: {final_metrics['average_forgetting']:.4f}")
        logger.info(f"   ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        
        # ë„ë©”ì¸ë³„ ì„ë² ë”© ìˆ˜ì§‘ (ì‹œê°í™”ìš©)
        logger.info("ğŸ“Š ì‹œê°í™”ìš© ì„ë² ë”© ìˆ˜ì§‘ ì¤‘...")
        domain_embeddings = {}
        
        for domain in config['domain_order']:
            # ğŸš€ ìºì‹œëœ ë°ì´í„°ì…‹ ì‚¬ìš© (ê³ ì†í™”)
            test_dataset = CachedBearingDataset(
                data_dir=config['data_dir'],
                dataset_type=config['dataset_type'],
                domain_value=domain,
                subset='test'
            )
            
            if len(test_dataset) > 0:
                # ìƒ˜í”Œë§ (ì‹œê°í™”ìš©ìœ¼ë¡œ ì ë‹¹í•œ ìˆ˜ë§Œ)
                max_viz_samples = min(100, len(test_dataset))
                indices = torch.randperm(len(test_dataset))[:max_viz_samples]
                
                text_embeddings = []
                vib_embeddings = []
                metadata_list = []
                
                trainer.model.eval()
                with torch.no_grad():
                    for idx in indices:
                        sample = test_dataset[idx]
                        batch = {
                            'vibration': sample['vibration'].unsqueeze(0).to(device),
                            'text': [sample['text']]
                        }
                        
                        model_results = trainer.model(batch, return_embeddings=True)
                        text_embeddings.append(model_results['text_embeddings'])
                        vib_embeddings.append(model_results['vib_embeddings'])
                        metadata_list.append(sample['metadata'])
                
                if text_embeddings:
                    # ì‹œê°í™” ëª¨ë“ˆì˜ ê¸°ëŒ€ í‚¤ ì´ë¦„ì— ë§ì¶° ì €ì¥ ('text', 'vib')
                    domain_embeddings[domain] = {
                        'text': torch.cat(text_embeddings, dim=0),
                        'vib': torch.cat(vib_embeddings, dim=0),
                        'metadata': metadata_list
                    }
        
        results['domain_embeddings'] = domain_embeddings
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ {config['name']} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        logger.exception("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
        return None


def print_final_summary(results: ExperimentResults, logger: logging.Logger):
    """ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ‰ ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í—˜ ì™„ë£Œ!")
    logger.info("="*80)
    
    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ìš”ì•½
    for summary in results.summary_results:
        logger.info(f"\nğŸ“Š {summary['Scenario']}:")
        logger.info(f"   Shift Type: {summary['Shift_Type']}")
        logger.info(f"   Domains: {summary['Num_Domains']}ê°œ")
        logger.info(f"   Avg Accuracy: {summary['Avg_Accuracy']:.4f}")
        logger.info(f"   Avg Forgetting: {summary['Avg_Forgetting']:.4f}")
        logger.info(f"   Total Time: {summary['Total_Time_Minutes']:.1f}ë¶„")
        logger.info(f"   Total Samples: {summary['Total_Samples']:,}ê°œ")
    
    # ë¹„êµ ë¶„ì„
    if len(results.summary_results) >= 2:
        uos_result = results.summary_results[0]
        cwru_result = results.summary_results[1]
        
        logger.info(f"\nğŸ” ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ:")
        logger.info(f"   ì •í™•ë„ ì°¨ì´: {abs(uos_result['Avg_Accuracy'] - cwru_result['Avg_Accuracy']):.4f}")
        logger.info(f"   ë§ê°ë„ ì°¨ì´: {abs(uos_result['Avg_Forgetting'] - cwru_result['Avg_Forgetting']):.4f}")
        logger.info(f"   ë°ì´í„° ê·œëª¨ ë¹„ìœ¨: {uos_result['Total_Samples'] / cwru_result['Total_Samples']:.1f}:1")
    
    logger.info("="*80)


def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='TextVibCLIP ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ í†µí•© ì‹¤í—˜')
    
    parser.add_argument('--output_dir', type=str, default='results',
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--quick_test', action='store_true',
                       help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì—í¬í¬ ìˆ˜ ê°ì†Œ)')
    parser.add_argument('--epochs', type=int, default=None,
                       help='ì—í¬í¬ ìˆ˜ (quick_test ëª¨ë“œìš©)')
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cpu', 'cuda'], help='í•™ìŠµ ë””ë°”ì´ìŠ¤')
    parser.add_argument('--seed', type=int, default=42,
                       help='ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œê°’')
    parser.add_argument('--skip_uos', action='store_true',
                       help='UOS ì‹œë‚˜ë¦¬ì˜¤ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip_cwru', action='store_true',
                       help='CWRU ì‹œë‚˜ë¦¬ì˜¤ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--sanity_check', action='store_true',
                       help='First domain sanity checkë§Œ ìˆ˜í–‰ (í•™ìŠµ/ì •í™•ë„/ì„ë² ë”© ì •ë ¬ í™•ì¸)')
    parser.add_argument('--sanity_samples', type=int, default=100,
                       help='sanity checkì—ì„œ ìˆ˜ì§‘í•  ìƒ˜í”Œ ìˆ˜(ë„ë©”ì¸ ë‚´)')
    parser.add_argument('--clear_cache', action='store_true',
                       help='ì‹¤í—˜ ì‹œì‘ ì „ ëª¨ë“  ìºì‹œ ì‚­ì œ')
    parser.add_argument('--no_cache', action='store_true',
                       help='ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš© ì•ˆí•¨ (ë””ë²„ê¹…ìš©)')
    
    return parser.parse_args()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_arguments()
    
    # ì¬í˜„ì„± ì„¤ì •
    set_seed(args.seed)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ë¡œê¹… ì„¤ì • ë° ì‹¤í—˜ í´ë” ìƒì„±
    logger, experiment_dir = setup_logging(args.output_dir)
    
    # ìºì‹œ ê´€ë¦¬ (ë¡œê¹… ì´ˆê¸°í™” í›„)
    if args.clear_cache:
        logger.info("ğŸ—‘ï¸ ìºì‹œ ì‚­ì œ ì¤‘...")
        clear_all_caches()
    logger.info("ğŸ¯ TextVibCLIP ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ í†µí•© ì‹¤í—˜ ì‹œì‘!")
    logger.info(f"ğŸ“ ê¸°ë³¸ ì €ì¥ ê²½ë¡œ: {args.output_dir}")
    logger.info(f"ğŸ“ ì‹¤í—˜ ê²°ê³¼ í´ë”: {experiment_dir}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    if device.type == 'cuda':
        logger.info(f"   GPU: {torch.cuda.get_device_name()}")
    
    # ì‹¤í—˜ ê²°ê³¼ ê´€ë¦¬ì
    results = ExperimentResults()
    
    # ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
    scenarios = []
    if not args.skip_uos:
        scenarios.append(ScenarioConfig.UOS_CONFIG)
    if not args.skip_cwru:
        scenarios.append(ScenarioConfig.CWRU_CONFIG)
    
    if not scenarios:
        logger.error("âŒ ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # Quick test ëª¨ë“œ ì„¤ì •
    if args.quick_test:
        test_epochs = args.epochs if args.epochs else 5
        logger.info(f"âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì—í¬í¬ ìˆ˜ë¥¼ {test_epochs}ë¡œ ì¶•ì†Œ")
        for scenario in scenarios:
            scenario['first_domain_epochs'] = test_epochs
            scenario['remaining_epochs'] = test_epochs // 2
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ì—ì„œë„ ë©”ëª¨ë¦¬ ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
            scenario['batch_size'] = min(scenario.get('batch_size', 4), 4)
    
    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì‹¤í–‰
    total_start_time = time.time()
    
    for i, scenario in enumerate(scenarios, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"ì‹œë‚˜ë¦¬ì˜¤ {i}/{len(scenarios)}: {scenario['name']}")
        logger.info(f"{'='*60}")
        
        # Sanity check ëª¨ë“œ: ì²« ë„ë©”ì¸ë§Œ ë¹ ë¥´ê²Œ ê²€ì¦í•˜ê³  ë¦¬í¬íŠ¸/ì‹œê°í™”
        if args.sanity_check:
            scenario_result = run_single_scenario({
                **scenario,
                'remaining_epochs': 0  # ë‚˜ë¨¸ì§€ ë„ë©”ì¸ ìŠ¤í‚µ
            }, logger, device, args)
            if scenario_result:
                # First-domain ì „ìš© ë¦¬í¬íŠ¸
                domain_names = scenario_result['domain_names']
                if len(domain_names) > 0 and 'final_accuracies' in scenario_result:
                    first_acc = scenario_result['final_accuracies'][0] if scenario_result['final_accuracies'] else 0.0
                    logger.info(f"ğŸ§ª Sanity - First domain accuracy: {first_acc:.4f}")
                # ì‹œê°í™”ê°€ ìˆìœ¼ë©´ ì €ì¥
                if 'domain_embeddings' in scenario_result and scenario_result['domain_embeddings']:
                    visualizer = create_visualizer(experiment_dir)
                    viz_path = visualizer.create_continual_learning_performance_plot(
                        domain_names=domain_names,
                        accuracies=scenario_result.get('final_accuracies', [0.0]*len(domain_names)),
                        forgetting_scores=[0.0]*len(domain_names),
                        scenario_name=f"SANITY_{scenario['name']}"
                    )
                    logger.info(f"ğŸ§ª Sanity - ì„±ëŠ¥ í”Œë¡¯: {os.path.basename(viz_path)}")
                # í•œ ì‹œë‚˜ë¦¬ì˜¤ë§Œ ê²€ì‚¬í•˜ê³  ì¢…ë£Œ
                results.add_scenario_result(scenario['name'], scenario_result)
            else:
                logger.error(f"âŒ {scenario['name']} ì‹¤í–‰ ì‹¤íŒ¨!")
            break
        else:
            scenario_result = run_single_scenario(scenario, logger, device, args)
        
        if scenario_result:
            results.add_scenario_result(scenario['name'], scenario_result)
        else:
            logger.error(f"âŒ {scenario['name']} ì‹¤í–‰ ì‹¤íŒ¨!")
    
    # ê²°ê³¼ ì €ì¥ (ì‹¤í—˜ í´ë”ì—)
    logger.info("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    try:
        detailed_path, summary_path, comparison_path = results.save_to_csv(experiment_dir)
        logger.info(f"âœ… ìƒì„¸ ê²°ê³¼: {detailed_path}")
        logger.info(f"âœ… ìš”ì•½ ê²°ê³¼: {summary_path}")
        logger.info(f"âœ… ë¹„êµ ê²°ê³¼: {comparison_path}")
    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    # ë…¼ë¬¸ìš© ê³ í’ˆì§ˆ ì‹œê°í™” ìƒì„± (ì‹¤í—˜ í´ë”ì—)
    logger.info("\nğŸ¨ ë…¼ë¬¸ìš© ê³ í’ˆì§ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
    try:
        visualizer = create_visualizer(experiment_dir)
        figure_count = 0
        
        # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ê°œë³„ ì‹œê°í™” ìƒì„±
        for scenario_name, scenario_result in results.scenario_results.items():
            logger.info(f"ğŸ“Š {scenario_name} ì‹œê°í™” ìƒì„± ì¤‘...")
            
            # 1. Continual Learning ì„±ëŠ¥ ì‹œê°í™” (ë„ë©”ì¸ë³„ ì •í™•ë„ + Forgetting)
            if 'domain_names' in scenario_result and 'final_accuracies' in scenario_result:
                domain_names = scenario_result['domain_names']
                accuracies = scenario_result['final_accuracies']
                forgetting_scores = scenario_result.get('forgetting_scores', [0.0] * len(domain_names))
                
                perf_path = visualizer.create_continual_learning_performance_plot(
                    domain_names=domain_names,
                    accuracies=accuracies,
                    forgetting_scores=forgetting_scores,
                    scenario_name=scenario_name
                )
                if perf_path:
                    figure_count += 1
                    logger.info(f"   âœ… ì„±ëŠ¥ ì‹œê°í™”: {os.path.basename(perf_path)}")
            
            # 2. Domain Shift Robustness ì‹œê°í™” (ë„ë©”ì¸ë³„ ì„ë² ë”© ë¶„í¬)
            if 'domain_embeddings' in scenario_result:
                domain_embeddings = scenario_result['domain_embeddings']
                
                robustness_path = visualizer.create_domain_shift_robustness_plot(
                    domain_embeddings=domain_embeddings,
                    scenario_name=scenario_name
                )
                if robustness_path:
                    figure_count += 1
                    logger.info(f"   âœ… ë„ë©”ì¸ ì‹œí”„íŠ¸ ì‹œê°í™”: {os.path.basename(robustness_path)}")
            
            # 3. Encoder Alignment ì‹œê°í™” (ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œ)
            if 'domain_embeddings' in scenario_result:
                domain_embeddings = scenario_result['domain_embeddings']
                first_domain = list(domain_embeddings.keys())[0] if domain_embeddings else None
                
                if first_domain and 'text' in domain_embeddings[first_domain] and 'vib' in domain_embeddings[first_domain]:
                    # ì‹¤ì œ ë©”íƒ€ë°ì´í„°ì—ì„œ ë¼ë²¨ ì‚¬ìš©
                    text_emb = domain_embeddings[first_domain]['text'][:100]
                    vib_emb = domain_embeddings[first_domain]['vib'][:100]
                    meta = domain_embeddings[first_domain].get('metadata', [])[:len(text_emb)]
                    labels = [m.get('bearing_condition', 'H') for m in meta]
                    types = [m.get('bearing_type', '6204') for m in meta]
                    
                    alignment_path = visualizer.create_encoder_alignment_plot(
                        text_embeddings=text_emb,
                        vib_embeddings=vib_emb,
                        labels=labels,
                        bearing_types=types,
                        domain_name=first_domain,
                        save_name=f"encoder_alignment_{scenario_name}"
                    )
                    if alignment_path:
                        figure_count += 1
                        logger.info(f"   âœ… Encoder alignment ì‹œê°í™”: {os.path.basename(alignment_path)}")
        
        logger.info(f"âœ… ë…¼ë¬¸ìš© Figure {figure_count}ê°œ ìƒì„± ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")
        logger.exception("ìƒì„¸ ì˜¤ë¥˜:")
    
    # ìºì‹œ í†µê³„ ì¶œë ¥
    cache = get_global_cache()
    cache_stats = cache.get_cache_stats()
    logger.info(f"\nğŸ“Š ìºì‹œ ì„±ëŠ¥:")
    logger.info(f"   ì ì¤‘ë¥ : {cache_stats['hit_rate']:.1f}%")
    logger.info(f"   ìºì‹œ íŒŒì¼: {cache_stats['cached_files']}ê°œ")
    logger.info(f"   ìºì‹œ í¬ê¸°: {cache_stats['total_size_mb']:.1f}MB")
    
    # ìµœì¢… ìš”ì•½
    total_time = time.time() - total_start_time
    logger.info(f"\nâ±ï¸ ì „ì²´ ì‹¤í—˜ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
    
    print_final_summary(results, logger)
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    logger.info("ğŸ‰ ëª¨ë“  ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
