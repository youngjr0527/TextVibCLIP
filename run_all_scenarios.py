#!/usr/bin/env python3
"""
TextVibCLIP ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ í†µí•© ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
1. ì‹œë‚˜ë¦¬ì˜¤ 1 (UOS) + ì‹œë‚˜ë¦¬ì˜¤ 2 (CWRU) ìë™ ì‹¤í–‰
2. ì‹œë‚˜ë¦¬ì˜¤ë³„/ë„ë©”ì¸ë³„ ì„±ëŠ¥ ì§€í‘œ CSV ì €ì¥
3. ì¢…í•© ê²°ê³¼ ìš”ì•½ ë° ë¹„êµ ë¶„ì„
4. ì‹¤í—˜ ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

Usage:
    python run_all_scenarios.py --output_dir results
    python run_all_scenarios.py --quick_test --epochs 10
"""

import argparse
import logging
import os
import torch
import time
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
import warnings

# Torchvision beta warning ë¹„í™œì„±í™”
try:
    import torchvision
    torchvision.disable_beta_transforms_warning()
except:
    pass

# ê¸°íƒ€ warning ì–µì œ
warnings.filterwarnings("ignore", category=UserWarning, module="torchvision")

# pandas import (CSV ì €ì¥ìš©)
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("âš ï¸ pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CSV ì €ì¥ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install pandas")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
import sys
from pathlib import Path
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

from src.continual_trainer import ContinualTrainer
from src.data_loader import BearingDataset, create_domain_dataloaders
from src.data_cache import create_cached_domain_dataloaders, create_cached_first_domain_dataloader, get_global_cache, clear_all_caches
from src.textvib_model import create_textvib_model
from src.utils import set_seed
from src.visualization import create_visualizer
from src.utils_diagnostics import compute_subset_overlap_ratio, evaluate_linear_probe, collect_embeddings, save_diagnostics_report
from configs.model_config import TRAINING_CONFIG, DATA_CONFIG, CWRU_DATA_CONFIG

# ë¡œê¹… ì„¤ì •
def setup_logging(log_dir: str) -> Tuple[logging.Logger, str]:
    """í†µí•© ì‹¤í—˜ìš© ë¡œê¹… ì„¤ì • ë° ì‹¤í—˜ë³„ í´ë” ìƒì„±"""
    # ì‹¤í—˜ë³„ ê³ ìœ  í´ë” ìƒì„± (ë‚ ì§œì‹œê°„ ê¸°ì¤€)
    experiment_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    # í´ë”ëª…ì„ ê°„ê²°í•˜ê²Œ: ì ‘ë‘ì‚¬ ì—†ì´ íƒ€ì„ìŠ¤íƒ¬í”„ë§Œ ì‚¬ìš©
    experiment_dir = os.path.join(log_dir, experiment_timestamp)
    os.makedirs(experiment_dir, exist_ok=True)
    
    log_filename = f"all_scenarios_{experiment_timestamp}.log"
    log_path = os.path.join(experiment_dir, log_filename)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"í†µí•© ì‹¤í—˜ ë¡œê¹… ì‹œì‘: {log_path}")
    logger.info(f"ì‹¤í—˜ ê²°ê³¼ í´ë”: {experiment_dir}")
    
    return logger, experiment_dir


class ScenarioConfig:
    """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„¤ì • ê´€ë¦¬"""
    
    UOS_CONFIG = {
        'name': 'UOS_Scenario1_VaryingSpeed',
        'data_dir': 'data_scenario1',
        'dataset_type': 'uos',
        'domain_order': [600, 800, 1000, 1200, 1400, 1600],
        'domain_names': ['600RPM', '800RPM', '1000RPM', '1200RPM', '1400RPM', '1600RPM'],
        'shift_type': 'Varying Speed',
        'first_domain_epochs': 25,  # 15 â†’ 25 (ì¶©ë¶„í•œ í•™ìŠµ)
        'remaining_epochs': 15,     # 10 â†’ 15 (ì•ˆì •ì  continual learning)
        'batch_size': 4,  # ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê°•í™”
        'replay_buffer_size': 1000,
        'patience': 8  # 5 â†’ 8 (ë” ê´€ëŒ€í•œ early stopping)
    }
    
    CWRU_CONFIG = {
        'name': 'CWRU_Scenario2_VaryingLoad',
        'data_dir': 'data_scenario2',
        'dataset_type': 'cwru',
        'domain_order': [0, 1, 2, 3],
        'domain_names': ['0HP', '1HP', '2HP', '3HP'],
        'shift_type': 'Varying Load',
        'first_domain_epochs': 30,  # 20 â†’ 30 (CWRU ë°ì´í„° ë¶€ì¡± ë³´ìƒ)
        'remaining_epochs': 15,     # 10 â†’ 15
        'batch_size': 8,
        'replay_buffer_size': 200,
        'patience': 15
    }


class ExperimentResults:
    """ì‹¤í—˜ ê²°ê³¼ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scenario_results = {}
        self.all_results = []  
    
    def add_scenario_result(self, scenario_name: str, results: Dict):
        """ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ì¶”ê°€"""
        self.scenario_results[scenario_name] = results
        
        # ìƒì„¸ ê²°ê³¼ ì¶”ê°€ (ë„ë©”ì¸ë³„)
        for domain_idx, domain_name in enumerate(results['domain_names']):
            if domain_idx < len(results['final_accuracies']):
                row = {
                    'Scenario': scenario_name,
                    'Domain_Index': domain_idx + 1,
                    'Domain_Name': domain_name,
                    'Shift_Type': results['shift_type'],
                    'Accuracy': results['final_accuracies'][domain_idx],
                    'Top1_Retrieval': results.get('final_top1_retrievals', [0] * len(results['domain_names']))[domain_idx],
                    'Top5_Retrieval': results.get('final_top5_retrievals', [0] * len(results['domain_names']))[domain_idx],
                    # ê´€ì‹¬ ì—†ëŠ” í•„ë“œ ì œê±°: Samples_Per_Domain, Total_Training_Time
                }
                # validation ë©”íŠ¸ë¦­ ì¶”ê°€ ì €ì¥(ìˆì„ ê²½ìš°)
                val_accs = results.get('val_accuracies', [])
                val_top1 = results.get('val_top1_retrievals', [])
                val_top5 = results.get('val_top5_retrievals', [])
                if domain_idx < len(val_accs):
                    row['Val_Accuracy'] = val_accs[domain_idx]
                    row['Val_Top1_Retrieval'] = val_top1[domain_idx] if domain_idx < len(val_top1) else 0
                    row['Val_Top5_Retrieval'] = val_top5[domain_idx] if domain_idx < len(val_top5) else 0
                # best epoch(í•´ë‹¹ ë„ë©”ì¸ì˜ ì¸ë±ìŠ¤ê°€ 1ë¶€í„° ì‹œì‘í•˜ëŠ” remainingì— ëŒ€ì‘)
                be_list = results.get('best_epochs', [])
                if domain_idx >= 1 and (domain_idx - 1) < len(be_list):
                    row['Best_Epoch'] = be_list[domain_idx - 1]
                # í”„ë¡œí† íƒ€ì… ì •ë ¬ í†µê³„ì˜ ì£¼ìš” ì§€í‘œë§Œ ìƒì„¸ CSVì—ë„ ìš”ì•½ ì €ì¥
                proto_stats = results.get('proto_alignment_stats', {}).get(
                    results['domain_names'][domain_idx].replace('RPM','').replace('HP',''), {})
                # í‚¤ê°€ ì •ìˆ˜ ë„ë©”ì¸ì¼ ìˆ˜ ìˆì–´ ë³´ì¡° ë§¤í•‘
                if not proto_stats:
                    try:
                        # domain_names -> domain_order ë§¤í•‘ì„ í†µí•´ í‚¤ ì°¾ê¸°
                        pass
                    except Exception:
                        proto_stats = {}
                for k in ['v_within_var_mean', 'proto_between_mean_cosdist']:
                    if k in proto_stats:
                        row[k] = proto_stats[k]
                self.all_results.append(row)
        
        # ìš”ì•½ ê²°ê³¼ë¥¼ ë™ì¼ CSVì˜ ë§ˆì§€ë§‰ í–‰ìœ¼ë¡œ ì¶”ê°€ (ë„ë©”ì¸=ALL)
        summary_row = {
            'Scenario': scenario_name,
            'Domain_Index': 0,
            'Domain_Name': 'ALL',
            'Shift_Type': results['shift_type'],
            'Accuracy': None,
            'Top1_Retrieval': None,
            'Top5_Retrieval': None,
            'Val_Accuracy': None,
            'Val_Top1_Retrieval': None,
            'Val_Top5_Retrieval': None,
            'Best_Epoch': None,
            'Num_Domains': len(results['domain_names']),
            'Avg_Accuracy': results.get('average_accuracy', 0),
            'Avg_Forgetting': results.get('average_forgetting', 0),
            'Avg_Top1_Retrieval': np.mean(results.get('final_top1_retrievals', [0])) if len(results.get('final_top1_retrievals', [])) else 0,
            'Avg_Top5_Retrieval': np.mean(results.get('final_top5_retrievals', [0])) if len(results.get('final_top5_retrievals', [])) else 0,
            'First_Domain_Epochs': results.get('first_domain_epochs', 0),
            'Remaining_Epochs': results.get('remaining_epochs', 0),
            'Batch_Size': results.get('batch_size', 0)
        }
        self.all_results.append(summary_row)
    
    def save_to_csv(self, output_dir: str):
        """ê²°ê³¼ë¥¼ ë‹¨ì¼ CSV íŒŒì¼ë¡œ ì €ì¥ (detailed + summary í†µí•©, ë¹„êµ íŒŒì¼ ìƒì„± ì•ˆ í•¨)"""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if not PANDAS_AVAILABLE:
            # pandasê°€ ì—†ìœ¼ë©´ JSONìœ¼ë¡œ ì €ì¥
            all_path = os.path.join(output_dir, f'results_{timestamp}.json')
            with open(all_path, 'w') as f:
                json.dump(self.all_results, f, indent=2)
            return all_path
        
        # ë‹¨ì¼ CSVë¡œ ì €ì¥
        all_df = pd.DataFrame(self.all_results)
        all_path = os.path.join(output_dir, f'results_{timestamp}.csv')
        all_df.to_csv(all_path, index=False)
        return all_path


def run_single_scenario(config: Dict, logger: logging.Logger, device: torch.device, args, experiment_dir: str) -> Dict:
    """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰"""
    logger.info(f"ğŸš€ {config['name']} ì‹œì‘!")
    logger.info(f"   Domain Shift: {config['shift_type']}")
    logger.info(f"   Domains: {' â†’ '.join(config['domain_names'])}")
    
    start_time = time.time()
    
    try:
        # Trainer ìƒì„±
        trainer = ContinualTrainer(
            device=device,
            save_dir=f"checkpoints/{config['name']}",
            use_amp=False,  # ğŸ¯ AMP ë¹„í™œì„±í™” (ìˆ˜ì¹˜ ì•ˆì •ì„± í™•ë³´)
            max_grad_norm=0.1,
            domain_order=config['domain_order'],
            data_dir=config['data_dir'],
            dataset_type=config['dataset_type'],
            patience=config.get('patience', TRAINING_CONFIG.get('patience', 10))
        )
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        trainer.batch_size = config['batch_size']
        trainer.learning_rate = 3e-4  # configì™€ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •
        trainer.replay_buffer.buffer_size_per_domain = config['replay_buffer_size']
        
        # ğŸš€ ìºì‹œëœ ë°ì´í„°ì…‹ ì •ë³´ ìˆ˜ì§‘ (ê³ ì†í™”)
        from src.data_cache import CachedBearingDataset
        sample_dataset = CachedBearingDataset(
            data_dir=config['data_dir'],
            dataset_type=config['dataset_type'],
            domain_value=config['domain_order'][0],
            subset='train'
        )
        samples_per_domain = len(sample_dataset)
        total_samples = samples_per_domain * len(config['domain_order'])
        
        logger.info(f"   ë°ì´í„°: {samples_per_domain:,} ìƒ˜í”Œ/ë„ë©”ì¸, ì´ {total_samples:,} ìƒ˜í”Œ")
        
        # First Domain Training
        logger.info("ğŸ“š First Domain Training...")
        
        # ğŸš€ ìºì‹œëœ DataLoader ì‚¬ìš© (ê³ ì†í™”)
        if args.no_cache:
            from src.data_loader import create_first_domain_dataloader
            first_loader = create_first_domain_dataloader(
                data_dir=config['data_dir'],
                domain_order=config['domain_order'],
                dataset_type=config['dataset_type'],
                subset='train',
                batch_size=config['batch_size']
            )
        else:
            first_loader = create_cached_first_domain_dataloader(
                data_dir=config['data_dir'],
                domain_order=config['domain_order'],
                dataset_type=config['dataset_type'],
                subset='train',
                batch_size=config['batch_size']
            )
        
        # ğŸ¯ FIXED: ì¡°ê¸° ì¢…ë£Œ ì˜ˆì™¸ ì²˜ë¦¬ ì œê±° (ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸)
        first_results = trainer.train_first_domain(
            first_domain_dataloader=first_loader,
            num_epochs=config['first_domain_epochs']
        )
        
        # SANITY ëª¨ë“œ: Remaining domains ì™„ì „ ìŠ¤í‚µí•˜ê³  ì²« ë„ë©”ì¸ë§Œ ê²°ê³¼ ë°˜í™˜
        if config.get('remaining_epochs', 0) == 0:
            logger.info("ğŸ§ª SANITY ëª¨ë“œ ê°ì§€: Remaining domains í•™ìŠµ/í‰ê°€ ìŠ¤í‚µ")
            first_domain_value = config['domain_order'][0]
            first_domain_name = config['domain_names'][0]
            perf = first_results.get('domain_performances', {})
            metrics = perf.get(first_domain_value, {'accuracy': 0.0, 'top1_retrieval': 0.0, 'top5_retrieval': 0.0})

            total_time = time.time() - start_time

            results = {
                'domain_names': [first_domain_name],
                'shift_type': config['shift_type'],
                'final_accuracies': [metrics.get('accuracy', 0.0)],
                'final_top1_retrievals': [metrics.get('top1_retrieval', 0.0)],
                'final_top5_retrievals': [metrics.get('top5_retrieval', 0.0)],
                'average_accuracy': metrics.get('accuracy', 0.0),
                'average_forgetting': 0.0,
                'samples_per_domain': samples_per_domain,
                'total_samples': samples_per_domain,
                'total_time': total_time,
                'first_domain_epochs': config['first_domain_epochs'],
                'remaining_epochs': 0,
                'batch_size': config['batch_size']
            }

            # ì²« ë„ë©”ì¸ë§Œ ì„ë² ë”© ìˆ˜ì§‘ (ì‹œê°í™”ìš©)
            logger.info("ğŸ“Š SANITY - ì²« ë„ë©”ì¸ ì„ë² ë”© ìˆ˜ì§‘ ì¤‘...")
            domain = first_domain_value
            test_dataset = BearingDataset(
                data_dir=config['data_dir'],
                dataset_type=config['dataset_type'],
                domain_value=domain,
                subset='test'
            )
            domain_embeddings = {}
            if len(test_dataset) > 0:
                max_viz_samples = min(config.get('sanity_samples', 100), len(test_dataset))
                indices = torch.randperm(len(test_dataset))[:max_viz_samples]
                text_embeddings = []
                vib_embeddings = []
                metadata_list = []
                trainer.model.eval()
                # ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ íš¨ìœ¨í™”
                from torch.utils.data import DataLoader, Subset
                subset = Subset(test_dataset, indices.tolist())
                # ê¸°ë³¸ ì½œë ˆì´íŠ¸ëŠ” dict-of-listsë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ list-of-dictsë¡œ ë³µì›
                def _collate_identity(samples):
                    return samples  # list of dicts ìœ ì§€
                dl = DataLoader(subset, batch_size=16, shuffle=False, collate_fn=_collate_identity)
                with torch.no_grad():
                    for samples in dl:
                        # samples: list of dicts
                        vib_batch = torch.stack([s['vibration'] for s in samples], dim=0).to(device)
                        text_batch = [s['text'] for s in samples]
                        meta_batch = [s.get('metadata', {}) for s in samples]
                        batch = {
                            'vibration': vib_batch,
                            'text': text_batch
                        }
                        model_results = trainer.model(batch, return_embeddings=True)
                        text_embeddings.append(model_results['text_embeddings'])
                        vib_embeddings.append(model_results['vib_embeddings'])
                        metadata_list.extend(meta_batch)
                if text_embeddings:
                    domain_embeddings[domain] = {
                        'text': torch.cat(text_embeddings, dim=0),
                        'vib': torch.cat(vib_embeddings, dim=0),
                        'metadata': metadata_list
                    }
            results['domain_embeddings'] = domain_embeddings

            logger.info("âœ… SANITY - ì²« ë„ë©”ì¸ ê²°ê³¼ ë°˜í™˜ ì™„ë£Œ")
            return results

        # Remaining Domains Training (SANITYê°€ ì•„ë‹ˆë©´ ì§„í–‰)
        logger.info("ğŸ”„ Remaining Domains Training...")
        trainer.num_epochs = config['remaining_epochs']
        
        # ğŸš€ ìºì‹œëœ ë„ë©”ì¸ë³„ ë°ì´í„°ë¡œë” ìƒì„± (ê³ ì†í™”)
        if args.no_cache:
            domain_loaders = create_domain_dataloaders(
                data_dir=config['data_dir'],
                domain_order=config['domain_order'],
                dataset_type=config['dataset_type'],
                batch_size=config['batch_size']
            )
        else:
            domain_loaders = create_cached_domain_dataloaders(
                data_dir=config['data_dir'],
                domain_order=config['domain_order'],
                dataset_type=config['dataset_type'],
                batch_size=config['batch_size']
            )
        
        remaining_results = trainer.train_remaining_domains(domain_loaders)
        
        # ê²°ê³¼ ì •ë¦¬
        final_metrics = remaining_results['final_metrics']
        # best epoch ë¡œê·¸(ë„ë©”ì¸ë³„)
        try:
            best_epochs = []
            for d in config['domain_order'][1:]:
                if d in remaining_results:
                    be = remaining_results[d]['training_results'].get('best_epoch', -1)
                    best_epochs.append(be)
                else:
                    best_epochs.append(-1)
        except Exception:
            best_epochs = []
        # Validation ë©”íŠ¸ë¦­ ìˆ˜ì§‘: ë„ë©”ì¸ë³„ val í‰ê°€ ìˆ˜í–‰
        try:
            val_eval = trainer._evaluate_all_domains_val(domain_loaders)
            val_domain_order = config['domain_order']
            val_accs = [val_eval[d]['accuracy'] for d in val_domain_order if d in val_eval]
            val_top1 = [val_eval[d]['top1_retrieval'] for d in val_domain_order if d in val_eval]
            val_top5 = [val_eval[d]['top5_retrieval'] for d in val_domain_order if d in val_eval]
        except Exception:
            val_accs, val_top1, val_top5 = [], [], []

        # í”„ë¡œí† íƒ€ì… ì •ë ¬ í†µê³„ ìˆ˜ì§‘(Validation ê¸°ì¤€)
        proto_stats_per_domain = {}
        try:
            proto_stats_per_domain = trainer.compute_prototype_alignment_stats_for_domains(domain_loaders)
            # CSVë¡œ ì €ì¥
            if PANDAS_AVAILABLE and proto_stats_per_domain:
                rows = []
                for d, st in proto_stats_per_domain.items():
                    base = {'Domain': d}
                    base.update(st)
                    rows.append(base)
                if rows:
                    import pandas as pd
                    df_proto = pd.DataFrame(rows)
                    path_proto = os.path.join(experiment_dir, f'prototype_alignment_stats_{config["name"]}.csv')
                    df_proto.to_csv(path_proto, index=False)
                    logger.info(f"   âœ… Prototype ì •ë ¬ í†µê³„ ì €ì¥: {os.path.basename(path_proto)}")
        except Exception as e:
            logger.info(f"   â„¹ï¸ Prototype ì •ë ¬ í†µê³„ ìˆ˜ì§‘ ìŠ¤í‚µ: {e}")

        # RKD/Validation ì¶”ì´ ì €ì¥ (ë„ë©”ì¸ë³„)
        try:
            if PANDAS_AVAILABLE:
                import pandas as pd
                rows = []
                for d in config['domain_order'][1:]:
                    if d in remaining_results:
                        tr = remaining_results[d].get('training_results', {})
                        rkd_hist = tr.get('rkd_history', []) or []
                        val_hist = tr.get('val_acc_history', []) or []
                        for i, v in enumerate(rkd_hist):
                            rows.append({'Domain': d, 'Epoch': i+1, 'RKD_Loss': v, 'Val_Accuracy': val_hist[i] if i < len(val_hist) else None})
                if rows:
                    df_hist = pd.DataFrame(rows)
                    path_hist = os.path.join(experiment_dir, f'rkd_val_history_{config["name"]}.csv')
                    df_hist.to_csv(path_hist, index=False)
                    logger.info(f"   âœ… RKD/Val ì¶”ì´ ì €ì¥: {os.path.basename(path_hist)}")
        except Exception as e:
            logger.info(f"   â„¹ï¸ RKD/Val ì¶”ì´ ì €ì¥ ìŠ¤í‚µ: {e}")
        total_time = time.time() - start_time
        
        results = {
            'domain_names': config['domain_names'],
            'domain_values': config['domain_order'],
            'shift_type': config['shift_type'],
            'final_accuracies': final_metrics['final_accuracies'],
            'final_top1_retrievals': final_metrics.get('final_top1_retrievals', []),
            'final_top5_retrievals': final_metrics.get('final_top5_retrievals', []),
            'val_accuracies': val_accs,
            'val_top1_retrievals': val_top1,
            'val_top5_retrievals': val_top5,
            'best_epochs': best_epochs,
        'proto_alignment_stats': proto_stats_per_domain,
            'average_accuracy': final_metrics['average_accuracy'],
            'average_forgetting': final_metrics['average_forgetting'],
            'samples_per_domain': samples_per_domain,
            'total_samples': total_samples,
            'total_time': total_time,
            'first_domain_epochs': config['first_domain_epochs'],
            'remaining_epochs': config['remaining_epochs'],
            'batch_size': config['batch_size']
        }
        
        logger.info(f"âœ… {config['name']} ì™„ë£Œ!")
        logger.info(f"   í‰ê·  ì •í™•ë„: {final_metrics['average_accuracy']:.4f}")
        logger.info(f"   í‰ê·  ë§ê°ë„: {final_metrics['average_forgetting']:.4f}")
        logger.info(f"   ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        
        # ë„ë©”ì¸ë³„ ì„ë² ë”© ìˆ˜ì§‘ (ì‹œê°í™”ìš© + ì„ í˜• í”„ë¡œë¸Œ ì§„ë‹¨)
        logger.info("ğŸ“Š ì‹œê°í™”ìš© ì„ë² ë”© ìˆ˜ì§‘ ì¤‘...")
        domain_embeddings = {}
        diag_rows: List[Dict[str, Any]] = []
        
        for domain in config['domain_order']:
            # ğŸš€ ìºì‹œëœ ë°ì´í„°ì…‹ ì‚¬ìš© (ê³ ì†í™”)
            test_dataset = CachedBearingDataset(
                data_dir=config['data_dir'],
                dataset_type=config['dataset_type'],
                domain_value=domain,
                subset='test'
            )
            # ë¶„í•  ë°ì´í„°ì…‹ ì°¸ì¡° ìƒì„±(ì¤‘ë³µ ê·¼ì‚¬ ê³„ì‚°ìš©)
            train_dataset = CachedBearingDataset(
                data_dir=config['data_dir'],
                dataset_type=config['dataset_type'],
                domain_value=domain,
                subset='train'
            )
            val_dataset = CachedBearingDataset(
                data_dir=config['data_dir'],
                dataset_type=config['dataset_type'],
                domain_value=domain,
                subset='val'
            )
            
            if len(test_dataset) > 0:
                # ìƒ˜í”Œë§ (ì‹œê°í™”ìš©ìœ¼ë¡œ ì ë‹¹í•œ ìˆ˜ë§Œ)
                max_viz_samples = min(100, len(test_dataset))
                indices = torch.randperm(len(test_dataset))[:max_viz_samples]
                
                text_embeddings = []
                vib_embeddings = []
                metadata_list = []
                
                trainer.model.eval()
                with torch.no_grad():
                    for idx in indices:
                        sample = test_dataset[idx]
                        batch = {
                            'vibration': sample['vibration'].unsqueeze(0).to(device),
                            'text': [sample['text']]
                        }
                        
                        model_results = trainer.model(batch, return_embeddings=True)
                        text_embeddings.append(model_results['text_embeddings'])
                        vib_embeddings.append(model_results['vib_embeddings'])
                        metadata_list.append(sample['metadata'])
                
                if text_embeddings:
                    # ì‹œê°í™” ëª¨ë“ˆì˜ ê¸°ëŒ€ í‚¤ ì´ë¦„ì— ë§ì¶° ì €ì¥ ('text', 'vib')
                    domain_embeddings[domain] = {
                        'text': torch.cat(text_embeddings, dim=0),
                        'vib': torch.cat(vib_embeddings, dim=0),
                        'metadata': metadata_list
                    }
                    logger.info(f"   ğŸ“Š ë„ë©”ì¸ {domain} ì„ë² ë”© ìˆ˜ì§‘: {len(text_embeddings)}ê°œ ìƒ˜í”Œ")
                else:
                    logger.warning(f"   âš ï¸ ë„ë©”ì¸ {domain}: ì„ë² ë”© ìˆ˜ì§‘ ì‹¤íŒ¨")
            # ì„ í˜• í”„ë¡œë¸Œ ì§„ë‹¨(ë„ë©”ì¸ë³„ test ë¶„ë¦¬ ê°€ëŠ¥ì„±)
            try:
                from torch.utils.data import DataLoader
                test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)
                v_emb, v_lab = collect_embeddings(trainer.model, test_loader, device)
                if v_emb.numel() > 0 and v_lab.numel() > 0:
                    probe_acc = evaluate_linear_probe(v_emb, v_lab)
                else:
                    probe_acc = float('nan')
            except Exception:
                probe_acc = float('nan')
            # ë¶„í•  ê²½ê³„ ì¤‘ë³µ ê·¼ì‚¬(Train-Val, Val-Test)
            overlap_tv = compute_subset_overlap_ratio(train_dataset, (0.0, 0.6), (0.6, 0.8)) if len(train_dataset) > 0 else float('nan')
            overlap_vt = compute_subset_overlap_ratio(val_dataset, (0.6, 0.8), (0.8, 1.0)) if len(val_dataset) > 0 else float('nan')
            diag_rows.append({
                'Domain': domain,
                'LinearProbe_Vib_TestAcc': probe_acc,
                'Overlap_TrainVal_Approx': overlap_tv,
                'Overlap_ValTest_Approx': overlap_vt,
                'Num_Test_Samples': len(test_dataset)
            })
        
        results['domain_embeddings'] = domain_embeddings
        # ì§„ë‹¨ ë¦¬í¬íŠ¸ ì €ì¥
        try:
            diag_path = save_diagnostics_report(experiment_dir, config['name'], diag_rows)
            logger.info(f"   âœ… ë°ì´í„° ì§„ë‹¨ ë¦¬í¬íŠ¸: {os.path.basename(diag_path)}")
        except Exception as e:
            logger.info(f"   â„¹ï¸ ë°ì´í„° ì§„ë‹¨ ë¦¬í¬íŠ¸ ìŠ¤í‚µ: {e}")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ {config['name']} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        logger.exception("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
        return None


def print_final_summary(results: ExperimentResults, logger: logging.Logger):
    """ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (ë‹¨ì¼ CSV ì €ì¥ ì²´ê³„ì— ë§ì¶° ê°„ë‹¨ ìš”ì•½)"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ‰ ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í—˜ ì™„ë£Œ!")
    logger.info("="*80)
    # ë‹¨ì¼ CSV ì €ì¥ìœ¼ë¡œ ì„¸ë¶€ ë¹„êµëŠ” CSVì—ì„œ í™•ì¸í•˜ë„ë¡ ê°„ì†Œí™”


def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='TextVibCLIP ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ í†µí•© ì‹¤í—˜')
    
    parser.add_argument('--output_dir', type=str, default='results',
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--quick_test', action='store_true',
                       help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì—í¬í¬ ìˆ˜ ê°ì†Œ)')
    parser.add_argument('--epochs', type=int, default=None,
                       help='ì—í¬í¬ ìˆ˜ (quick_test ëª¨ë“œìš©)')
    parser.add_argument('--device', type=str, default='auto',
                       choices=['auto', 'cpu', 'cuda'], help='í•™ìŠµ ë””ë°”ì´ìŠ¤')
    parser.add_argument('--seed', type=int, default=42,
                       help='ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œê°’')
    parser.add_argument('--skip_uos', action='store_true',
                       help='UOS ì‹œë‚˜ë¦¬ì˜¤ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip_cwru', action='store_true',
                       help='CWRU ì‹œë‚˜ë¦¬ì˜¤ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--sanity_check', action='store_true',
                       help='First domain sanity checkë§Œ ìˆ˜í–‰ (í•™ìŠµ/ì •í™•ë„/ì„ë² ë”© ì •ë ¬ í™•ì¸)')
    parser.add_argument('--sanity_samples', type=int, default=100,
                       help='sanity checkì—ì„œ ìˆ˜ì§‘í•  ìƒ˜í”Œ ìˆ˜(ë„ë©”ì¸ ë‚´)')
    parser.add_argument('--clear_cache', action='store_true',
                       help='ì‹¤í—˜ ì‹œì‘ ì „ ëª¨ë“  ìºì‹œ ì‚­ì œ')
    parser.add_argument('--no_cache', action='store_true',
                       help='ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš© ì•ˆí•¨ (ë””ë²„ê¹…ìš©)')
    
    return parser.parse_args()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_arguments()
    
    # ì¬í˜„ì„± ì„¤ì •
    set_seed(args.seed)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ë¡œê¹… ì„¤ì • ë° ì‹¤í—˜ í´ë” ìƒì„±
    logger, experiment_dir = setup_logging(args.output_dir)
    
    # ìºì‹œ ê´€ë¦¬ (ë¡œê¹… ì´ˆê¸°í™” í›„)
    if args.clear_cache:
        logger.info("ğŸ—‘ï¸ ìºì‹œ ì‚­ì œ ì¤‘...")
        clear_all_caches()
    logger.info("ğŸ¯ TextVibCLIP ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ í†µí•© ì‹¤í—˜ ì‹œì‘!")
    logger.info(f"ğŸ“ ê¸°ë³¸ ì €ì¥ ê²½ë¡œ: {args.output_dir}")
    logger.info(f"ğŸ“ ì‹¤í—˜ ê²°ê³¼ í´ë”: {experiment_dir}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    if device.type == 'cuda':
        logger.info(f"   GPU: {torch.cuda.get_device_name()}")
    
    # ì‹¤í—˜ ê²°ê³¼ ê´€ë¦¬ì
    results = ExperimentResults()
    
    # ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
    scenarios = []
    if not args.skip_uos:
        scenarios.append(ScenarioConfig.UOS_CONFIG)
    if not args.skip_cwru:
        scenarios.append(ScenarioConfig.CWRU_CONFIG)
    
    if not scenarios:
        logger.error("âŒ ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ì—í¬í¬ ì„¤ì • (quick_test ëª¨ë“œì™€ ì¼ë°˜ ëª¨ë“œ ëª¨ë‘ ì§€ì›)
    if args.quick_test:
        test_epochs = args.epochs if args.epochs else 5
        logger.info(f"âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì—í¬í¬ ìˆ˜ë¥¼ {test_epochs}ë¡œ ì¶•ì†Œ")
        for scenario in scenarios:
            scenario['first_domain_epochs'] = test_epochs
            scenario['remaining_epochs'] = test_epochs // 2
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ì—ì„œë„ ë©”ëª¨ë¦¬ ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
            scenario['batch_size'] = min(scenario.get('batch_size', 4), 4)
    elif args.epochs:
        # ğŸ¯ FIXED: ì¼ë°˜ ëª¨ë“œì—ì„œë„ --epochs ì˜µì…˜ ì ìš©
        logger.info(f"ğŸ”§ ì‚¬ìš©ì ì§€ì • ì—í¬í¬: {args.epochs}")
        for scenario in scenarios:
            scenario['first_domain_epochs'] = args.epochs
            scenario['remaining_epochs'] = max(args.epochs // 3, 5)  # 1/3 ë˜ëŠ” ìµœì†Œ 5
    
    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì‹¤í–‰
    total_start_time = time.time()
    
    for i, scenario in enumerate(scenarios, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"ì‹œë‚˜ë¦¬ì˜¤ {i}/{len(scenarios)}: {scenario['name']}")
        logger.info(f"{'='*60}")
        
        # Sanity check ëª¨ë“œ: ì²« ë„ë©”ì¸ë§Œ ë¹ ë¥´ê²Œ ê²€ì¦í•˜ê³  ë¦¬í¬íŠ¸/ì‹œê°í™”
        if args.sanity_check:
            scenario_result = run_single_scenario({
                **scenario,
                'remaining_epochs': 0  # ë‚˜ë¨¸ì§€ ë„ë©”ì¸ ìŠ¤í‚µ
            }, logger, device, args, experiment_dir)
            if scenario_result:
                # First-domain ì „ìš© ë¦¬í¬íŠ¸
                domain_names = scenario_result['domain_names']
                if len(domain_names) > 0 and 'final_accuracies' in scenario_result:
                    first_acc = scenario_result['final_accuracies'][0] if scenario_result['final_accuracies'] else 0.0
                    logger.info(f"ğŸ§ª Sanity - First domain accuracy: {first_acc:.4f}")
                # ì‹œê°í™”ê°€ ìˆìœ¼ë©´ ì €ì¥
                if 'domain_embeddings' in scenario_result and scenario_result['domain_embeddings']:
                    visualizer = create_visualizer(experiment_dir)
                    viz_path = visualizer.create_continual_learning_performance_plot(
                        domain_names=domain_names,
                        accuracies=scenario_result.get('final_accuracies', [0.0]*len(domain_names)),
                        forgetting_scores=[0.0]*len(domain_names),
                        scenario_name=f"SANITY_{scenario['name']}"
                    )
                    logger.info(f"ğŸ§ª Sanity - ì„±ëŠ¥ í”Œë¡¯: {os.path.basename(viz_path)}")
                # í•œ ì‹œë‚˜ë¦¬ì˜¤ë§Œ ê²€ì‚¬í•˜ê³  ì¢…ë£Œ
                results.add_scenario_result(scenario['name'], scenario_result)
            else:
                logger.error(f"âŒ {scenario['name']} ì‹¤í–‰ ì‹¤íŒ¨!")
            break
        else:
            scenario_result = run_single_scenario(scenario, logger, device, args, experiment_dir)
        
        if scenario_result:
            results.add_scenario_result(scenario['name'], scenario_result)
        else:
            logger.error(f"âŒ {scenario['name']} ì‹¤í–‰ ì‹¤íŒ¨!")
    
    # ê²°ê³¼ ì €ì¥ (ë‹¨ì¼ CSV)
    logger.info("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    try:
        all_path = results.save_to_csv(experiment_dir)
        logger.info(f"âœ… ê²°ê³¼: {all_path}")
    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    # ë…¼ë¬¸ìš© ê³ í’ˆì§ˆ ì‹œê°í™” ìƒì„± (ì‹¤í—˜ í´ë”ì—)
    logger.info("\nğŸ¨ ë…¼ë¬¸ìš© ê³ í’ˆì§ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
    try:
        visualizer = create_visualizer(experiment_dir)
        figure_count = 0
        
        # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ê°œë³„ ì‹œê°í™” ìƒì„±
        for scenario_name, scenario_result in results.scenario_results.items():
            logger.info(f"ğŸ“Š {scenario_name} ì‹œê°í™” ìƒì„± ì¤‘...")
            
            # 1. Continual Learning ì„±ëŠ¥ ì‹œê°í™” (ë„ë©”ì¸ë³„ ì •í™•ë„ + Forgetting)
            if 'domain_names' in scenario_result and 'final_accuracies' in scenario_result:
                domain_names = scenario_result['domain_names']
                accuracies = scenario_result['final_accuracies']
                forgetting_scores = scenario_result.get('forgetting_scores', [0.0] * len(domain_names))
                # Validationê³¼ í•¨ê»˜ double-bar ë„í‘œë¥¼ ì €ì¥(ì¶”ê°€ íŒŒì¼ëª…)
                val_accs = scenario_result.get('val_accuracies', [])
                
                perf_path = visualizer.create_continual_learning_performance_plot(
                    domain_names=domain_names,
                    accuracies=accuracies,
                    forgetting_scores=forgetting_scores,
                    scenario_name=scenario_name
                )
                if perf_path:
                    figure_count += 1
                    logger.info(f"   âœ… ì„±ëŠ¥ ì‹œê°í™”: {os.path.basename(perf_path)}")
                # ë³´ì¡°: validation ë§‰ëŒ€í‘œ CSV ì €ì¥(ê°„ë‹¨ ë¡œê·¸)
                try:
                    if len(val_accs) == len(domain_names) and PANDAS_AVAILABLE:
                        import pandas as pd
                        df_va = pd.DataFrame({'Domain': domain_names, 'Val_Accuracy': val_accs, 'Test_Accuracy': accuracies})
                        va_path = os.path.join(experiment_dir, f'val_vs_test_{scenario_name}.csv')
                        df_va.to_csv(va_path, index=False)
                        logger.info(f"   âœ… Val vs Test CSV: {os.path.basename(va_path)}")
                except Exception as e:
                    logger.info(f"   â„¹ï¸ Val/Test CSV ìƒì„± ìŠ¤í‚µ: {e}")
            
            # 2. Domain Shift Robustness ì‹œê°í™” (ë„ë©”ì¸ë³„ ì„ë² ë”© ë¶„í¬)
            if 'domain_embeddings' in scenario_result:
                domain_embeddings = scenario_result['domain_embeddings']
                
                robustness_path = visualizer.create_domain_shift_robustness_plot(
                    domain_embeddings=domain_embeddings,
                    scenario_name=scenario_name
                )
                if robustness_path:
                    figure_count += 1
                    logger.info(f"   âœ… ë„ë©”ì¸ ì‹œí”„íŠ¸ ì‹œê°í™”: {os.path.basename(robustness_path)}")
            
            # 3. Encoder Alignment ì‹œê°í™” (ì²« ë²ˆì§¸ ë„ë©”ì¸ë§Œ)
            if 'domain_embeddings' in scenario_result:
                domain_embeddings = scenario_result['domain_embeddings']
                
                logger.info(f"   ğŸ“Š Domain embeddings í™•ì¸: {len(domain_embeddings)}ê°œ ë„ë©”ì¸")
                for domain_key, domain_data in domain_embeddings.items():
                    logger.info(f"     - ë„ë©”ì¸ {domain_key}: text={len(domain_data.get('text', []))}, vib={len(domain_data.get('vib', []))}")
                
                first_domain = list(domain_embeddings.keys())[0] if domain_embeddings else None
                
                if first_domain and 'text' in domain_embeddings[first_domain] and 'vib' in domain_embeddings[first_domain]:
                    try:
                        # ì‹¤ì œ ë©”íƒ€ë°ì´í„°ì—ì„œ ë¼ë²¨ ì‚¬ìš©
                        text_emb = domain_embeddings[first_domain]['text'][:100]
                        vib_emb = domain_embeddings[first_domain]['vib'][:100]
                        meta = domain_embeddings[first_domain].get('metadata', [])[:len(text_emb)]
                        
                        # CWRUì™€ UOSì˜ ë©”íƒ€ë°ì´í„° êµ¬ì¡° ì°¨ì´ ê³ ë ¤
                        if scenario_name.startswith('CWRU'):
                            # CWRU: bearing_conditionì´ 'H', 'B', 'IR', 'OR' (H = Healthy)
                            labels = [m.get('bearing_condition', 'H') for m in meta]
                            types = [m.get('bearing_type', 'deep_groove_ball') for m in meta]
                        else:
                            # UOS: bearing_conditionì´ ìƒíƒœ (H, B, IR, OR, L, U, M)
                            labels = [m.get('bearing_condition', 'H') for m in meta]
                            types = [m.get('bearing_type', '6204') for m in meta]
                        
                        logger.info(f"   ğŸ“Š Alignment ì‹œê°í™” ë°ì´í„°: {len(text_emb)}ê°œ ìƒ˜í”Œ, ì²« ë„ë©”ì¸={first_domain}")
                        logger.info(f"      ë¼ë²¨ ë¶„í¬: {set(labels)}")
                        
                        # ë„ë©”ì¸ëª… í˜•ì‹ í†µì¼ (UOS: 600RPM -> 600, CWRU: 0HP -> 0)
                        domain_display = str(first_domain).replace('RPM', '').replace('HP', '')
                        
                        alignment_path = visualizer.create_encoder_alignment_plot(
                            text_embeddings=text_emb,
                            vib_embeddings=vib_emb,
                            labels=labels,
                            bearing_types=types,
                            domain_name=domain_display,
                            save_name=f"encoder_alignment_{scenario_name}_{domain_display}"
                        )
                        if alignment_path:
                            figure_count += 1
                            logger.info(f"   âœ… Encoder alignment ì‹œê°í™”: {os.path.basename(alignment_path)}")
                        else:
                            logger.warning(f"   âš ï¸ Encoder alignment ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {scenario_name}")
                            
                    except Exception as e:
                        logger.error(f"   âŒ Encoder alignment ì‹œê°í™” ì˜¤ë¥˜ ({scenario_name}): {e}")
                        logger.exception("ìƒì„¸ ì˜¤ë¥˜:")
                else:
                    if not first_domain:
                        logger.warning(f"   âš ï¸ {scenario_name}: ì²« ë²ˆì§¸ ë„ë©”ì¸ì´ ì—†ìŠµë‹ˆë‹¤")
                    elif first_domain not in domain_embeddings:
                        logger.warning(f"   âš ï¸ {scenario_name}: ë„ë©”ì¸ {first_domain} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                    elif 'text' not in domain_embeddings[first_domain]:
                        logger.warning(f"   âš ï¸ {scenario_name}: ë„ë©”ì¸ {first_domain}ì— text ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤")
                    elif 'vib' not in domain_embeddings[first_domain]:
                        logger.warning(f"   âš ï¸ {scenario_name}: ë„ë©”ì¸ {first_domain}ì— vib ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤")
            else:
                logger.warning(f"   âš ï¸ {scenario_name}: domain_embeddingsê°€ ì—†ìŠµë‹ˆë‹¤")
        
        logger.info(f"âœ… ë…¼ë¬¸ìš© Figure {figure_count}ê°œ ìƒì„± ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")
        logger.exception("ìƒì„¸ ì˜¤ë¥˜:")
    
    # ìºì‹œ í†µê³„ ì¶œë ¥
    cache = get_global_cache()
    cache_stats = cache.get_cache_stats()
    logger.info(f"\nğŸ“Š ìºì‹œ ì„±ëŠ¥:")
    logger.info(f"   ì ì¤‘ë¥ : {cache_stats['hit_rate']:.1f}%")
    logger.info(f"   ìºì‹œ íŒŒì¼: {cache_stats['cached_files']}ê°œ")
    logger.info(f"   ìºì‹œ í¬ê¸°: {cache_stats['total_size_mb']:.1f}MB")
    
    # ìµœì¢… ìš”ì•½
    total_time = time.time() - total_start_time
    logger.info(f"\nâ±ï¸ ì „ì²´ ì‹¤í—˜ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
    
    print_final_summary(results, logger)
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    logger.info("ğŸ‰ ëª¨ë“  ì‹¤í—˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
