{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TextVibCLIP Vibration Encoder 단독 성능 테스트\n",
        "\n",
        "UOS 데이터셋에서 Vibration Encoder가 진동 신호를 제대로 분류할 수 있는지 검증\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/home/kyj2024/anaconda3/envs/TVCLIP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Vibration Encoder 단독 성능 테스트 시작\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/home/kyj2024/anaconda3/envs/TVCLIP/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "/data/home/kyj2024/anaconda3/envs/TVCLIP/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from collections import Counter\n",
        "import logging\n",
        "import seaborn as sns\n",
        "\n",
        "# 프로젝트 루트 추가\n",
        "sys.path.append('/data/home/kyj2024/TextVibCLIP')\n",
        "\n",
        "from src.vibration_encoder import VibrationEncoder\n",
        "from src.data_loader import BearingDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from src.data_loader import create_collate_fn\n",
        "from configs.model_config import MODEL_CONFIG\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"🔧 Vibration Encoder 단독 성능 테스트 시작\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 데이터 준비\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data_loader:UOS 데이터 라벨 분포: {'B_6204': 1, 'H_6204': 4, 'IR_6204': 1, 'OR_6204': 1}\n",
            "INFO:src.data_loader:최소 샘플 수: 1\n",
            "INFO:src.data_loader:UOS Domain-Incremental 윈도우 레벨 분할:\n",
            "INFO:src.data_loader:  모든 subset에 모든 7개 파일 포함\n",
            "INFO:src.data_loader:  각 파일 내에서 윈도우 분할: Train 60%, Val 20%, Test 20%\n",
            "INFO:src.data_loader:  Deep Groove Ball 7-클래스 분포: {'B': 1, 'H': 1, 'IR': 1, 'OR': 1, 'L': 1, 'M': 1, 'U': 1}\n",
            "INFO:src.data_loader:  클래스 수: 7개 (균형 확인)\n",
            "INFO:src.data_loader:  ✅ 완벽한 클래스 균형 달성!\n",
            "INFO:src.data_loader:UOS train 분할 결과:\n",
            "INFO:src.data_loader:  Train: 7개 파일, Val: 7개 파일, Test: 7개 파일\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "디바이스: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data_loader:BearingDataset 초기화 완료 (UOS): 7개 파일, 1249개 윈도우/파일, 총 8743개 샘플, Domain: 600, Subset: train\n",
            "INFO:src.data_loader:UOS 데이터 라벨 분포: {'B_6204': 1, 'H_6204': 4, 'IR_6204': 1, 'OR_6204': 1}\n",
            "INFO:src.data_loader:최소 샘플 수: 1\n",
            "INFO:src.data_loader:UOS Domain-Incremental 윈도우 레벨 분할:\n",
            "INFO:src.data_loader:  모든 subset에 모든 7개 파일 포함\n",
            "INFO:src.data_loader:  각 파일 내에서 윈도우 분할: Train 60%, Val 20%, Test 20%\n",
            "INFO:src.data_loader:  Deep Groove Ball 7-클래스 분포: {'B': 1, 'H': 1, 'IR': 1, 'OR': 1, 'L': 1, 'M': 1, 'U': 1}\n",
            "INFO:src.data_loader:  클래스 수: 7개 (균형 확인)\n",
            "INFO:src.data_loader:  ✅ 완벽한 클래스 균형 달성!\n",
            "INFO:src.data_loader:UOS test 분할 결과:\n",
            "INFO:src.data_loader:  Train: 7개 파일, Val: 7개 파일, Test: 7개 파일\n",
            "INFO:src.data_loader:BearingDataset 초기화 완료 (UOS): 7개 파일, 1249개 윈도우/파일, 총 8743개 샘플, Domain: 600, Subset: test\n",
            "INFO:src.data_loader:인덱스 매핑 생성 완료: 8743개 (파일 7개 × 윈도우 1249개)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train 데이터: 8743개 샘플\n",
            "Test 데이터: 8743개 샘플\n",
            "\n",
            "샘플 구조:\n",
            "  라벨: tensor([6, 0])\n",
            "  진동 신호 shape: torch.Size([2048])\n",
            "  진동 신호 범위: [-1.5622, 1.7830]\n"
          ]
        }
      ],
      "source": [
        "# UOS 데이터셋 로드\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"디바이스: {device}\")\n",
        "\n",
        "# Train 데이터\n",
        "train_dataset = BearingDataset(\n",
        "    data_dir='data_scenario1',\n",
        "    dataset_type='uos',\n",
        "    domain_value=600,\n",
        "    subset='train'\n",
        ")\n",
        "\n",
        "# Test 데이터\n",
        "test_dataset = BearingDataset(\n",
        "    data_dir='data_scenario1',\n",
        "    dataset_type='uos',\n",
        "    domain_value=600,\n",
        "    subset='test'\n",
        ")\n",
        "\n",
        "print(f\"Train 데이터: {len(train_dataset)}개 샘플\")\n",
        "print(f\"Test 데이터: {len(test_dataset)}개 샘플\")\n",
        "\n",
        "# 샘플 확인\n",
        "sample = train_dataset[0]\n",
        "print(f\"\\n샘플 구조:\")\n",
        "print(f\"  라벨: {sample['labels']}\")\n",
        "print(f\"  진동 신호 shape: {sample['vibration'].shape}\")\n",
        "print(f\"  진동 신호 범위: [{sample['vibration'].min():.4f}, {sample['vibration'].max():.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Vibration Encoder 생성 및 진동 임베딩\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.vibration_encoder:1D-CNN VibrationEncoder 초기화: input_length=2048, embedding_dim=256\n",
            "INFO:src.vibration_encoder:   OPTIMIZED: 커널 크기: [16, 32, 64, 32] - 4-layer 베어링 최적화\n",
            "INFO:src.vibration_encoder:   OPTIMIZED: 채널 수: [64, 128, 256, 512] - 자연스러운 64→512 증가\n",
            "INFO:src.vibration_encoder:   총 파라미터: 6,985,288\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vibration Encoder 정보:\n",
            "  입력 길이: 2048\n",
            "  출력 차원: 256\n",
            "  총 파라미터: 6,985,288개\n",
            "  학습 파라미터: 6,985,288개\n",
            "  아키텍처: [16, 32, 64, 32] kernels, [64, 128, 256, 512] channels\n",
            "  실제 출력 shape: torch.Size([1, 256])\n",
            "  정규화 후 범위: [-0.0825, 0.0676]\n"
          ]
        }
      ],
      "source": [
        "# Vibration Encoder 생성 (올바른 파라미터 사용)\n",
        "vibration_config = MODEL_CONFIG['vibration_encoder']\n",
        "embedding_dim = MODEL_CONFIG['embedding_dim']\n",
        "\n",
        "vibration_encoder = VibrationEncoder(\n",
        "    input_length=vibration_config['input_length'],\n",
        "    embedding_dim=embedding_dim\n",
        ")\n",
        "\n",
        "vibration_encoder.to(device)\n",
        "vibration_encoder.eval()\n",
        "\n",
        "# 파라미터 수 계산\n",
        "total_params = sum(p.numel() for p in vibration_encoder.parameters())\n",
        "trainable_params = sum(p.numel() for p in vibration_encoder.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Vibration Encoder 정보:\")\n",
        "print(f\"  입력 길이: {vibration_config['input_length']}\")\n",
        "print(f\"  출력 차원: {embedding_dim}\")\n",
        "print(f\"  총 파라미터: {total_params:,}개\")\n",
        "print(f\"  학습 파라미터: {trainable_params:,}개\")\n",
        "print(f\"  아키텍처: {vibration_config['kernel_sizes']} kernels, {vibration_config['channels']} channels\")\n",
        "\n",
        "# 테스트 입력으로 출력 차원 확인\n",
        "with torch.no_grad():\n",
        "    test_input = torch.randn(1, vibration_config['input_length']).to(device)\n",
        "    test_output = vibration_encoder(test_input)\n",
        "    print(f\"  실제 출력 shape: {test_output.shape}\")\n",
        "    print(f\"  정규화 후 범위: [{test_output.min().item():.4f}, {test_output.max().item():.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 진동 데이터 샘플링 및 임베딩 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "클래스별 진동 데이터 수집 중...\n",
            "\n",
            "수집된 클래스별 진동 데이터 수: {0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100}\n",
            "전체 진동 데이터: torch.Size([700, 2048])\n",
            "클래스 분포: Counter({0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100})\n",
            "진동 신호 통계: mean=-0.0004, std=1.0121\n"
          ]
        }
      ],
      "source": [
        "# 클래스별 진동 데이터 수집 (각 클래스당 100개씩)\n",
        "class_vibrations = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: []}  # 7개 클래스\n",
        "class_labels = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: []}\n",
        "samples_per_class = 100\n",
        "class_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
        "\n",
        "print(\"클래스별 진동 데이터 수집 중...\")\n",
        "for i in range(len(train_dataset)):\n",
        "    if all(count >= samples_per_class for count in class_counts.values()):\n",
        "        break\n",
        "        \n",
        "    sample = train_dataset[i]\n",
        "    label = sample['labels'][0].item()  # 주 분류\n",
        "    vibration = sample['vibration']\n",
        "    \n",
        "    if class_counts[label] < samples_per_class:\n",
        "        class_vibrations[label].append(vibration)\n",
        "        class_labels[label].append(label)\n",
        "        class_counts[label] += 1\n",
        "\n",
        "print(f\"\\n수집된 클래스별 진동 데이터 수: {class_counts}\")\n",
        "\n",
        "# 모든 진동 데이터를 하나의 텐서로 결합\n",
        "all_vibrations = []\n",
        "all_labels = []\n",
        "\n",
        "for class_id, vibrations in class_vibrations.items():\n",
        "    all_vibrations.extend(vibrations)\n",
        "    all_labels.extend(class_labels[class_id])\n",
        "\n",
        "# 텐서로 변환\n",
        "vibration_tensor = torch.stack(all_vibrations)  # (700, 2048)\n",
        "labels_tensor = torch.tensor(all_labels)\n",
        "\n",
        "print(f\"전체 진동 데이터: {vibration_tensor.shape}\")\n",
        "print(f\"클래스 분포: {Counter(all_labels)}\")\n",
        "print(f\"진동 신호 통계: mean={vibration_tensor.mean():.4f}, std={vibration_tensor.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 진동 임베딩 생성 및 분석\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "진동 임베딩 생성 중...\n",
            "Vibration embeddings shape: torch.Size([700, 256])\n",
            "임베딩 통계: mean=-0.0034, std=0.0624\n",
            "임베딩 범위: min=-0.1840, max=0.1650\n",
            "고유 클래스: [0, 1, 2, 3, 4, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "# 진동 임베딩 생성\n",
        "print(\"진동 임베딩 생성 중...\")\n",
        "vibration_embeddings_list = []\n",
        "batch_size = 32\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(vibration_tensor), batch_size):\n",
        "        batch_vibrations = vibration_tensor[i:i+batch_size].to(device)\n",
        "        batch_embeddings = vibration_encoder(batch_vibrations)\n",
        "        batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
        "        vibration_embeddings_list.append(batch_embeddings.cpu())\n",
        "\n",
        "vibration_embeddings = torch.cat(vibration_embeddings_list, dim=0)\n",
        "\n",
        "print(f\"Vibration embeddings shape: {vibration_embeddings.shape}\")\n",
        "print(f\"임베딩 통계: mean={vibration_embeddings.mean().item():.4f}, std={vibration_embeddings.std().item():.4f}\")\n",
        "print(f\"임베딩 범위: min={vibration_embeddings.min().item():.4f}, max={vibration_embeddings.max().item():.4f}\")\n",
        "\n",
        "unique_classes = torch.unique(labels_tensor)\n",
        "print(f\"고유 클래스: {unique_classes.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 진동 신호 분류 성능 테스트 (핵심)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "클래스별 진동 prototype 생성:\n",
            "  클래스 0: 100개, 내부 유사도 0.9985±0.0004\n",
            "  클래스 1: 100개, 내부 유사도 0.9977±0.0007\n",
            "  클래스 2: 100개, 내부 유사도 0.9909±0.0031\n",
            "  클래스 3: 100개, 내부 유사도 0.9968±0.0008\n",
            "  클래스 4: 100개, 내부 유사도 0.9936±0.0025\n",
            "  클래스 5: 100개, 내부 유사도 0.9943±0.0022\n",
            "  클래스 6: 100개, 내부 유사도 0.9975±0.0010\n",
            "\n",
            "Prototype matrix shape: torch.Size([7, 256])\n",
            "Prototype labels: [0, 1, 2, 3, 4, 5, 6]\n",
            "\n",
            "🔧 Vibration Encoder 분류 정확도: 0.8857 (88.6%)\n",
            "이론적 랜덤 베이스라인: 0.1429 (14.3%)\n",
            "랜덤 대비 향상: 6.20배\n",
            "\n",
            "클래스별 정확도:\n",
            "  클래스 0 (H): 100/100 = 1.0000 (100.0%)\n",
            "  클래스 1 (B): 98/100 = 0.9800 (98.0%)\n",
            "  클래스 2 (IR): 66/100 = 0.6600 (66.0%)\n",
            "  클래스 3 (OR): 100/100 = 1.0000 (100.0%)\n",
            "  클래스 4 (L): 91/100 = 0.9100 (91.0%)\n",
            "  클래스 5 (U): 95/100 = 0.9500 (95.0%)\n",
            "  클래스 6 (M): 70/100 = 0.7000 (70.0%)\n",
            "\n",
            "혼동 행렬:\n",
            "[[100   0   0   0   0   0   0]\n",
            " [  2  98   0   0   0   0   0]\n",
            " [  0   0  66   5  29   0   0]\n",
            " [  0   0   0 100   0   0   0]\n",
            " [  0   0   1   8  91   0   0]\n",
            " [  0   5   0   0   0  95   0]\n",
            " [ 30   0   0   0   0   0  70]]\n"
          ]
        }
      ],
      "source": [
        "# 클래스별 prototype 계산\n",
        "class_prototypes = []\n",
        "prototype_labels = []\n",
        "\n",
        "print(\"클래스별 진동 prototype 생성:\")\n",
        "for cls in unique_classes:\n",
        "    cls_mask = (labels_tensor == cls)\n",
        "    cls_embeddings = vibration_embeddings[cls_mask]\n",
        "    cls_prototype = cls_embeddings.mean(dim=0, keepdim=True)\n",
        "    \n",
        "    class_prototypes.append(cls_prototype)\n",
        "    prototype_labels.append(cls)\n",
        "    \n",
        "    # 클래스 내 유사도 분석\n",
        "    if len(cls_embeddings) > 1:\n",
        "        intra_sim = torch.matmul(cls_embeddings, cls_embeddings.t())\n",
        "        # 대각선 제외한 평균 (자기 자신 제외)\n",
        "        mask = ~torch.eye(len(cls_embeddings), dtype=torch.bool)\n",
        "        intra_mean = intra_sim[mask].mean().item()\n",
        "        intra_std = intra_sim[mask].std().item()\n",
        "        print(f\"  클래스 {cls.item()}: {len(cls_embeddings)}개, 내부 유사도 {intra_mean:.4f}±{intra_std:.4f}\")\n",
        "\n",
        "# Prototype 결합\n",
        "prototype_matrix = torch.cat(class_prototypes, dim=0)  # (7, 256)\n",
        "prototype_labels = torch.stack(prototype_labels)  # (7,)\n",
        "\n",
        "print(f\"\\nPrototype matrix shape: {prototype_matrix.shape}\")\n",
        "print(f\"Prototype labels: {prototype_labels.tolist()}\")\n",
        "\n",
        "# 🎯 핵심: 진동 신호 분류 성능 테스트\n",
        "similarities = torch.matmul(vibration_embeddings, prototype_matrix.t())  # (700, 7)\n",
        "predicted_indices = torch.argmax(similarities, dim=1)  # (700,)\n",
        "\n",
        "# 디바이스 일치 확인\n",
        "if predicted_indices.device != prototype_labels.device:\n",
        "    predicted_indices = predicted_indices.to(prototype_labels.device)\n",
        "\n",
        "predicted_classes = prototype_labels[predicted_indices]  # (700,)\n",
        "\n",
        "# 정확도 계산\n",
        "accuracy = (predicted_classes == labels_tensor).float().mean().item()\n",
        "print(f\"\\n🔧 Vibration Encoder 분류 정확도: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
        "\n",
        "# 이론적 랜덤 베이스라인\n",
        "random_baseline = 1.0 / len(unique_classes)\n",
        "print(f\"이론적 랜덤 베이스라인: {random_baseline:.4f} ({random_baseline*100:.1f}%)\")\n",
        "print(f\"랜덤 대비 향상: {accuracy/random_baseline:.2f}배\")\n",
        "\n",
        "# 클래스별 정확도\n",
        "print(\"\\n클래스별 정확도:\")\n",
        "class_names = ['H', 'B', 'IR', 'OR', 'L', 'U', 'M']\n",
        "class_accuracies = []\n",
        "for cls in unique_classes:\n",
        "    cls_mask = (labels_tensor == cls)\n",
        "    cls_correct = (predicted_classes[cls_mask] == labels_tensor[cls_mask]).sum().item()\n",
        "    cls_total = cls_mask.sum().item()\n",
        "    cls_acc = cls_correct / cls_total if cls_total > 0 else 0\n",
        "    class_accuracies.append(cls_acc)\n",
        "    print(f\"  클래스 {cls.item()} ({class_names[cls.item()]}): {cls_correct}/{cls_total} = {cls_acc:.4f} ({cls_acc*100:.1f}%)\")\n",
        "\n",
        "# 혼동 행렬\n",
        "cm = confusion_matrix(labels_tensor.cpu(), predicted_classes.cpu())\n",
        "print(f\"\\n혼동 행렬:\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 결론 및 진단\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "🔧 Vibration Encoder 성능 진단 결과\n",
            "============================================================\n",
            "1. 분류 정확도: 88.6% (랜덤: 14.3%)\n",
            "2. 클래스 구분도: 0.0073\n",
            "3. 임베딩 품질: std=0.0624\n",
            "4. 최고 클래스 정확도: 100.0%\n",
            "5. 최저 클래스 정확도: 66.0%\n",
            "\n",
            "🔍 종합 진단:\n",
            "✅ Vibration Encoder 성능: 우수 (88.6%)\n",
            "📋 다음 단계: Text-Vibration 정렬 문제 집중 분석\n",
            "\n",
            "🎯 세부 권장사항:\n",
            "- Vibration Encoder는 정상 작동\n",
            "- Text-Vibration 정렬 문제에 집중\n",
            "\n",
            "📈 성능 비교:\n",
            "  Text Encoder 단독: 83.0%\n",
            "  Vibration Encoder 단독: 88.6%\n",
            "  TextVibCLIP 전체: 14.4%\n",
            "🚨 두 Encoder 모두 정상 - 정렬/통합 과정에 심각한 문제\n",
            "\n",
            "🔧 아키텍처 정보:\n",
            "  커널 크기: [16, 32, 64, 32]\n",
            "  채널 수: [64, 128, 256, 512]\n",
            "  드롭아웃: 0.1\n",
            "  입력→출력: 2048 → 256\n",
            "  총 파라미터: 6,985,288개\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🔧 Vibration Encoder 성능 진단 결과\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 클래스 구분도 계산\n",
        "inter_class_sim = torch.matmul(prototype_matrix, prototype_matrix.t())\n",
        "diag_sim = torch.diag(inter_class_sim).mean().item()\n",
        "off_diag_mask = ~torch.eye(len(unique_classes), dtype=torch.bool)\n",
        "off_diag_sim = inter_class_sim[off_diag_mask].mean().item()\n",
        "separation_score = diag_sim - off_diag_sim\n",
        "\n",
        "print(f\"1. 분류 정확도: {accuracy*100:.1f}% (랜덤: {random_baseline*100:.1f}%)\")\n",
        "print(f\"2. 클래스 구분도: {separation_score:.4f}\")\n",
        "print(f\"3. 임베딩 품질: std={vibration_embeddings.std().item():.4f}\")\n",
        "print(f\"4. 최고 클래스 정확도: {max(class_accuracies)*100:.1f}%\")\n",
        "print(f\"5. 최저 클래스 정확도: {min(class_accuracies)*100:.1f}%\")\n",
        "\n",
        "# 종합 진단\n",
        "print(f\"\\n🔍 종합 진단:\")\n",
        "if accuracy > 0.8:\n",
        "    diagnosis = \"우수\"\n",
        "    next_step = \"Text-Vibration 정렬 문제 집중 분석\"\n",
        "    color = \"✅\"\n",
        "elif accuracy > 0.6:\n",
        "    diagnosis = \"양호\"\n",
        "    next_step = \"Vibration Encoder 아키텍처 개선\"\n",
        "    color = \"⚠️\"\n",
        "elif accuracy > 0.4:\n",
        "    diagnosis = \"보통\"\n",
        "    next_step = \"데이터 전처리 및 정규화 개선\"\n",
        "    color = \"⚠️\"\n",
        "else:\n",
        "    diagnosis = \"불량\"\n",
        "    next_step = \"Vibration Encoder 아키텍처 재설계\"\n",
        "    color = \"❌\"\n",
        "\n",
        "print(f\"{color} Vibration Encoder 성능: {diagnosis} ({accuracy*100:.1f}%)\")\n",
        "print(f\"📋 다음 단계: {next_step}\")\n",
        "\n",
        "# 세부 권장사항\n",
        "print(f\"\\n🎯 세부 권장사항:\")\n",
        "if accuracy < 0.5:\n",
        "    print(\"- 1D-CNN 아키텍처 재설계 (더 깊은 네트워크)\")\n",
        "    print(\"- 데이터 정규화 방법 변경\")\n",
        "    print(\"- 윈도우 크기 조정 (2048 → 4096)\")\n",
        "elif accuracy < 0.7:\n",
        "    print(\"- Dropout 비율 조정\")\n",
        "    print(\"- 커널 크기 최적화\")\n",
        "    print(\"- 배치 정규화 추가\")\n",
        "else:\n",
        "    print(\"- Vibration Encoder는 정상 작동\")\n",
        "    print(\"- Text-Vibration 정렬 문제에 집중\")\n",
        "\n",
        "# Text Encoder vs Vibration Encoder 비교\n",
        "text_encoder_performance = 0.83  # Text Encoder 성능\n",
        "textvib_performance = 0.1443  # TextVibCLIP 전체 성능\n",
        "\n",
        "print(f\"\\n📈 성능 비교:\")\n",
        "print(f\"  Text Encoder 단독: {text_encoder_performance*100:.1f}%\")\n",
        "print(f\"  Vibration Encoder 단독: {accuracy*100:.1f}%\")\n",
        "print(f\"  TextVibCLIP 전체: {textvib_performance*100:.1f}%\")\n",
        "\n",
        "if accuracy > 0.7 and text_encoder_performance > 0.7:\n",
        "    print(\"🚨 두 Encoder 모두 정상 - 정렬/통합 과정에 심각한 문제\")\n",
        "elif accuracy < 0.5:\n",
        "    print(\"🔍 Vibration Encoder 개선 필요\")\n",
        "else:\n",
        "    print(\"⚠️ 부분적 문제 - 추가 분석 필요\")\n",
        "\n",
        "print(f\"\\n🔧 아키텍처 정보:\")\n",
        "print(f\"  커널 크기: {vibration_config['kernel_sizes']}\")\n",
        "print(f\"  채널 수: {vibration_config['channels']}\")\n",
        "print(f\"  드롭아웃: {vibration_config['dropout']}\")\n",
        "print(f\"  입력→출력: {vibration_config['input_length']} → {embedding_dim}\")\n",
        "print(f\"  총 파라미터: {total_params:,}개\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TVCLIP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
